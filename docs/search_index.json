[["index.html", "Building Skills in Quantitative Biology 1 Welcome! 1.1 Open science 1.2 Accessibility Statement 1.3 Feedback 1.4 About the authors 1.5 Acknowledgments 1.6 Funding 1.7 Citation 1.8 License", " Building Skills in Quantitative Biology Kim Cuddington, Andrew M. Edwards, Brian Ingalls 26 February, 2022 1 Welcome! Quantitative skills are essential for biological research. We consider such skills to include the computational, statistical and mathematical techniques used to study life and living organisms, including aspects of big data, transparency and reproducibility in science. However, the breadth of quantitative techniques now employed in biology make it very likely that there may be no suitable local expertise within a students home institution or a biologists workplace. This e-book consists of five independent chapters or modules designed to teach different quantitative skills to graduate students and biologists working in academia, government agencies and private organisations. A key theme is that while the techniques presented are from various disciplines (such as computer science, statistics and mathematics), they are presented in a way that is suitable for a biological audience. Our examples and approach reflect our personal use of these tools as researchers in biology. Each module is designed to quickly get you up and running on the topic of interest in 3-5 hours. As such, these materials should be thought of a basic introduction. We provide pointers to more advanced materials, but our aim is to jump-start your use of these tools. On the other hand, these modules are not designed for absolute beginners. With the exception of the module on Git and Github, the materials are written assuming that learners have some basic familiarity with R, and a standard undergraduate background in calculus and univariate statistics. Because the modules are designed to be independent (i.e., you dont have read the whole e-book!), you can select subtopics that are directly relevant to your research. Our hope is that this approach will simultaneously provide more targeted training and reduce the time commitment that would be involved in more generalized formal courses. Pick and choose what you need! The five modules are: Git and GitHub (Andrew Edwards)  covers the tools widely used to share code, collaborate with colleagues and create a transparent record of research. R Markdown (Andrew Edwards)  demonstrates how to create dynamic documents that are fully reproducible, with a clear link between the underlying R code and the resulting figures, tables and results. Multivariate analysis: Clustering and Ordination (Kim Cuddington)  discusses multivariate quantitative methods that are used to understand data when there is more than one response (or measurement) Machine learning and classification (Kim Cuddington)  continues explorations of multivariate data and in particular the topic of classification but using machine learning approaches. Optimization (Brian Ingalls)  optimization is the act of identifying the extreme (cheapest, tallest, fastest, ) over a collection of possibilities. Applications include the manipulation (e.g. optimal harvesting or optimal drug dosing) and construction (e.g. robust synthetic genetic circuits) of biological systems, and experimental design. 1.1 Open science In the spirit of open science (and using the tools introduced in the first two modules), this e-book was written collaboratively and openly in R Markdown, with files shared via GitHub here. 1.2 Accessibility Statement We developed Building Skills in Quantitative Biology with a commitment to accessibility and usability for all learners. The accessibility of these materials was assessed by the Centre for Extended Learning, University of Waterloo. This review was based on the WCAG 2.0 Guidelines at success criteria Level AA. The authors have addressed all known accessibility issues to the best of their abilities. The following known accessibility issues persist and may cause difficulties for some persons with disabilities: Code output is only distinguished from code input by 1. output has a black border box and 2. input is colour shaded Internal links to figures and tables are only linked by the numbering (e.g., 5.1) of the object, rather than the full text (e.g., Figure 5.1) Please let us know if you discover other issues on the issues tab for the github repository 1.3 Feedback Despite extensive feedback from our student guinea pigs, we anticipate further revisions based on feedback, since we can consider this work as a living document. If you use these materials please take some time to let us know how they work for you, using this survey. 1.4 About the authors Kim Cuddington is an Associate Professor at the University of Waterloo in the Department of Biology, with a cross-appointment to Applied Mathematics. She has several projects designed to improve quantitative education for biology students and has training in instructional design (University of Waterloo, Center for Teaching Excellence). She teaches courses that emphasize quantitative methods (such as Quantitative Ecology and Mathematical Modelling in Biology). Her research involves the use of mathematics, statistics, and computational approaches to answer questions regarding population and ecosystem dynamics, invasive species, and impacts of climate change  see www.ecotheory.ca. Kim has a PhD in biology yet was once told she was too mathematical for a position in a biology department. Andrew Edwards is a Research Scientist with the Department of Fisheries and Oceans Canada (DFO) at the Pacific Biological Station in Nanaimo, British Columbia, and holds an Adjunct Professor position in the Department of Biology at the University of Victoria. He has previously developed in-person workshops on Git, GitHub and R Markdown (tools that are widely used in DFO), and co-developed a quantitative biology course at Dalhousie University. Current work includes developing methods for fitting size spectra to data, conducting fisheries stock assessments (including for Pacific Hake, the largest groundfish stock off the west coast of North America), and recent guidelines to using mathematical notation in ecology. Andy has a PhD in applied mathematics yet was once told he was too biological for a position in a math department. Brian Ingalls is an Associate Professor in the Department of Applied Mathematics at the University of Waterloo, cross-appointed to the Departments of Biology and Chemical Engineering. He is the author of Mathematical Modeling in System Biology: An Introduction (2013), and has previously taught undergraduate and graduate courses on quantitative techniques in biology (including Computational Modelling of Cellular Systems and Mathematical Cell Biology). Brian has a PhD in mathematics and everyone has always been too in awe of him to make remarks about what positions he is qualified for. 1.5 Acknowledgments We acknowledge support from the Government of Ontario through a grant from eCampusOntario, and the support of DFO and the Faculty of Science, University of Waterloo. The grant and the Faculty of Science each funded a student to help with creating the project. We thank Luwen Chang and Matthew Zhou for their amazing learning curves, and subsequent help coding up the modules. The eCampusOntario grant also funded several students to evaluate an early version of the materials. Lina Aragon Baquero, Lauren Banks, Madison Brook, Jacob Burbank, Nicole Gauvreau and Aranksha Dilip Thakor provided valuable feedback. The Git and GitHub module builds upon workshop materials that were originally developed with Chris Grandin (DFO), who AME also thanks for assistance with the module. 1.6 Funding This project is made possible with funding by the Government of Ontario and through eCampusOntarios support of the Virtual Learning Strategy. To learn more about the Virtual Learning Strategy visit: https://vls.ecampusontario.ca. 1.7 Citation Please cite this work as: Cuddington, K, Edwards, A.M., and Ingalls, B. (2021). Building Skills in Quantitative Biology. https://www.quantitative-biology.ca 1.8 License Kim Cuddington, Andrew M. Edwards, and Brian Ingalls. Building Skills in Quantitative Biology (https://www.quantitative-biology.ca) is available under an Ontario Commons License and a CC BY-NC-SA 4.0. . Contact kcudding AT uwaterloo DOT ca for more information All materials licensed under the Ontario Commons License (Version 1.0) unless otherwise specified. The license deed is available to read at https://vls.ecampusontario.ca/wp-content/uploads/2021/01/Ontario-Commons-License-1.0.pdf. All materials are also licensed under the Creative Commons BY-NC-SA 4.0 license unless otherwise specified. The license deed is available to read at https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode. "],["git-and-github.html", "2 Git and GitHub 2.1 What are Git and GitHub and how might they be useful? 2.2 You just want do download someone elses code from their GitHub repository 2.3 Motivation for learning more 2.4 Getting set up for the first time 2.5 Using Git and GitHub to share your own code 2.6 Using Git and GitHub to collaborate with colleagues 2.7 Workflow tips when collaborating 2.8 Beyond the basics of Git and GitHub  getting more advanced", " 2 Git and GitHub This module consists of text, graphics, four videos, and three exercises to get you started using Git and GitHub. Note that the final exercise requires a colleague to practice collaborating with. 2.1 What are Git and GitHub and how might they be useful? As a biology graduate student or a professional biologist in a university or government setting, there are several reasons you might want to use Git and GitHub. Well discuss these shortly, but first give two definitions: GitHub refers to the website github.com where millions of people and companies build, ship, and maintain their software. This is organised by users having a unique username. For a particular project, a user creates a repository, which is basically a collection of code needed for that project. Repositories can be public (anyone can see them) or private (only accessible to users specified by the original owner). Hosting code in a public GitHub repository allows users to share their code with the world and collaborate with anyone. The term repository is generally used interchangeably to mean both the collection of code (which includes some files that Git uses) and the GitHub website that hosts the collection of code. Git is software that keeps track of the latest versions of your files on your local computer. Your files can include computer code, data, write-ups of your work, etc. Git allows you to interact with GitHub to fetch code that others have changed, and to push your own changes so that others can retrieve them. Crucially, it allows you to merge each others changes and easily keep track of who has done what. It is called a version control system, which means that you can go back to all earlier versions (instances) of your repository  this is occasionally very useful. The three main ways that you may want to use Git and GitHub are (starting with the simplest): You want do download someone elses code from their GitHub repository. You want to share your own code on GitHub with others. This will automatically include the version control aspect mentioned above. You want to collaborate with colleagues on a project. We will work through these in order, giving more motivation for each as we go along. 2.2 You just want do download someone elses code from their GitHub repository Each GitHub repository can be viewed on GitHub. We will first take a quick video tour of a GitHub repository, showing: the layout of the GitHub website of a repository the most useful parts to know about how to download the code if you just want to get it and dont intend to collaborate (click the green Code button and Download Zip); useful to get the code to a supervisor, say. just download one file by clicking on Raw and then save-as. So grab some popcorn and click on the screenshot below to watch Video 1, which is five minutes long: To access a private repository, and for the rest of this module, you will need to set up a GitHub account: Sign up for GitHub at github.com If possible, choose a user name that will make sense to colleagues, e.g. andrew-edwards or cgrandin, not pink-unicorn (such a name may make sense to your current colleagues, but you may want something more professional so that future colleagues know who you are) Desirable: attach a photo (headshot) to your profile. This makes it easy for collaborators to identify you. For accessing a private repository, your colleague has to invite you from their GitHub repository page (they would do Settings-Manage access). You will get an email invitation, and once accepted you will have access to the repository when you are logged into GitHub. 2.3 Motivation for learning more The above example is pretty basic. Motivation to learn more comes from various reasons: Scientists (including students) are working far more collaboratively than in the past This involves both sharing code and writing up results There is a push towards open science  including your code as part of a scientific paper We have called this a TTT approach: Transparent  a clear and open way to show data, code, and results, enabling reproducibility Traceable  a clear link from database queries and code to final results (numbers, tables, and graphs in a document) Transferable  it should be feasible for another person to reproduce work and build upon it with a minimal learning curve Using Git and GitHub in your workflow greatly enables this, both when working alone and in a team. We use them extensively: to collaborate on writing code and producing documents (such as this entire set of modules!). to easily share code and R packages publically for scientific papers, and update them as necessary. when working alone to retain a methodical workflow. 2.3.1 Example application  Pacific Hake stock assessment Under a formal Agreement between the Canadian and US governments, a team of four of us (two from each country) conduct an annual stock assessment for Pacific Hake (Merluccius productus) off the west coast of Canada and the US. The assessment is used to manage the stock, which is of important ecological and economic value ($100 million export value in Canada). Figure 2.1: Artist rendition of a Pacific Hake We fit complex population models to data to make projections about future health of the stock under different levels of catch. There is an extremely short turnaround (four to five weeks) between getting the final data, doing the analyses (model runs can take many hours of computer time), resolving new problems, and submitting the assessment document, which is typically &gt;200 pages and contains numerous figures and tables (2022 assessment available here). Prior to 2016, the document was assembled in Word, requiring lots of editing and amaglamating of files, often late at night. Now we share our code via GitHub and automate a lot of the document production using knitr (knitr is similar to R Markdown which is covered in the R Markdown module). So with four people constantly working on the same large document, we need to ensure we are keeping up-to-date with each other, can all produce the latest version, and have identical folder structures on each others computers. The alternative of emailing files back and forth is: very inefficient, prone to errors, just painful. 2.3.2 Examples of what we can avoid Here are some real-world examples from not using GitHub, showing what we can avoid when using GitHub. Using GitHub it is easy to see what text/code collaborators have changed, avoiding things like the following example of Track Changes in Word, for which it hard to see where to get started: Figure 2.2: Example of numerous Track Changes in Word, leaving the reader unsure of how to proceed. Often we may want to keep old versions of files (and email them back and forth), but without GitHub we can end up with a veritable gong show, with multiple saved versions of the same file: Figure 2.3: Having multiple versions of the same file can easily lead to not all edits being considered. We can avoid having to manually co-ordinate having only one person working on the latest version of a document. So we dont get things like this example, which tells me I shouldnt really do anything now and should wait until others are done merging edits: Figure 2.4: Example of having to wait for colleagues to merge edits. We can avoid having multiple versions of a file that then have to be carefully merged: Figure 2.5: Example of having to manually manage multiple version of the same document. While GoogleDocs, for example, is fine for collaborating on a short document, it isnt suitable for sharing code that needs to be run on your local computer (or complex code containing files that refer to each other), or complex documents that are somewhat automatically updated. 2.3.3 Example of advantages that arise from using GitHub Say youve off on a two-week hike while your collaborators have been diligently working away and they have edited 15 new files of code in five folders, added four data sets, and created five new pages of text towards a manuscript. With GitHub you can easily catch up with them (get all their changes onto your computer) with a few simple commands. You dont even have to pester them to ask what theyve done, as you can check it yourself. So rather than this conversation: You: Hey, Im back from my awesome trip and saw some bears. What have you been doing with the project? Likely reply: Glad you had fun. Im busy on something else right now. Er, where were we at when you left? You can have this one: You: Hey, Im back from my awesome trip and saw some bears. I went through your commits on GitHub and everything looks great. Shall I get on with those questions you asked in the GitHub Issues regarding methods? Likely reply: Glad you had fun, looking forward to hearing about it. Im busy on something else right now so, yes, resolving those Issues will be great, thanks. And the project keeps moving in an efficient way. Well cover GitHub Issues later. By having code shared publicly, it is easy to answer questions, such as this one I once received: {r, echo = FALSE, fig.cap = \"Part of an email someone once had about some code.\", fig.alt = \"Screenshot of part of an email from someone asking about some code I had written. They helpfully provided a directly link to the file on GitHub plus referred to explicit line numbers.\" out.width=\"90%\", fig.align = \"center\"} knitr::include_graphics(\"MEEcodeQuest.png\") Rather than go searching on my laptop for the code that I hadnt looked at for six months, I could immediately open the file (on GitHub) by clicking on the link the questioner sent. I could answer very quickly, with a simple link to the file I am referring to (there is no ambiguity): Figure 2.6: Reply showing link to code on GitHub. You can even work out who last edited a particular line of code/text (GitHub amusingly calls it Blame). Just open a file on GitHub and click Blame: Figure 2.7: Example of GitHub showing who wrote each last line of a file. Though youll often find that it was you all along: Figure 2.8: Amusing cartoon based on the storyline in every Scooby Doo episode. You can properly keep track of Issues on GitHub (discussed later) to be thought about or fixed, rather than having things in emails that get forgotten: Figure 2.9: Example of GitHub Issues. Important: You still have all your work locally on your computer. So if your internet access goes down or GitHub is unavailable (which of course will only happen when you have a deadline) you can still carry on with your work. 2.3.4 Why this course? Delving into the Git and GitHub world online it can feel like you need a computer science degree to get started. This may not be surprising as Git was written by the guy who wrote the operating system Linux, to help people collaborate on writing the operating system Linux. But it means that, for example, the second paragraph of the Wikipedia Git page says: As with most other distributed version control systems, and unlike most clientserver systems, every Git directory on every computer is a full-fledged repository with complete history and full version-tracking abilities, independent of network access or a central server. Say what??? That is fairly incomprehensible to those without strong computer science backgrounds. The aim of this module is to introduce biologists to the world of Git and GitHub, while avoiding a lot of the technical details. However, once you have mastered the basics then it should be easier to delve deeper. Our target audience is: graduate level biology students biology faculty government scientists scientists in non-governmental organisations in fact anyone wanting to learn these tools This work is extended from lectures and exercises developed by Chris Grandin and myself as part of a Fisheries and Oceans Canada workshop. (Luckily Chris does have a computer science degree, and so was able to get some of us going with Git and GitHub several years ago). These tools are now widely used within our organisation. 2.3.5 Does it matter which computer language my code is in? For sharing code, it doesnt matter what language your code is in (R, Matlab, Python, C, ), as we will just be sharing text files. There is a learning curve, but once you get going you only really need a few main commands. Unfortunately the hardest bit is actually getting everything set up. 2.4 Getting set up for the first time Before you start using Git you need to set up your computer to use it, and install a few other programs that are useful. This is a one-time setup and, although it can sometimes be tricky, once it is done you will be able to easily create new projects or join others in collaboration. We have tested the installations as much as feasible. If you have an issue then search the internet, as it may be due to some configuration on your particular computer, and installation approaches do get updated. This module is for any operating system: Windows, MacOS, Linux or Unix. 2.4.1 What you will end up having installed These are programs/things you will install. Obviously skip any that you already have working. A GitHub account (see earlier for instructions) A text editor that isnt Notepad Git on your computer Optional: Diffmerge or something similar for comparing changes to files (not completely necessary) Markdown Pad 2 or Chrome extension or something similar for viewing Markdown files (not completely necessary) 2.4.2 Text Editor You must have a text editor that is aware of outside changes in a file. This is necessary because if you have a file open in the editor and you download an updated version of the file, you want the editor to ask you if you want to use the updated version. We know that Emacs, Xemacs and maybe Vim are okay, as is RStudio for using R (and other) files. Notepad is not okay. But you can download and install Notepad++ which is fine: https://notepad-plus-plus.org 2.4.3 Install the Git application on your machine See https://git-scm.com/downloads for downloading instructions for Windows, MAC and Linux/Unix It seems best to accept the default options, except: default editor: as it says, you probably want to switch from Vim (unless you use Vim) as the default editor. This option seems to be for the editor that will be used if Git needs to open a text window for you to edit (which will be rare), not for editing your own files. Notepad seems fine here if you like. Adjusting the name of the initial branch in new repositories - click Override the default branch name for new repositories and leave it as main. 2.4.4 Git shell, GUIs and RStudio For this course we will use a simple Git shell to type commands (rather than a point-and-click Graphical User Interface). This is for several reasons: Commands are the same across operating systems. It is easier to demonstrate (and remember) a few simple commands, rather than follow a cursor moving across a screen. Learning the text commands will give you a good understanding of how Git and GitHub work. It is easier to Google for help when you get stuck or want to learn about more advanced options. Commands are quick, and you can usually the up arrow (or ctrl-up-arrow) to retrieve recent commands, or auto-complete commands using &lt;TAB&gt;. There are many Graphical User Interfaces that are available, as described at https://git-scm.com/downloads/guis. Many (but not all) biologists use R in RStudio for their analyses. There is Git functionality built into RStudio that we discuss briefly at the very end of this module. I use magit which works in the text editor Emacs (which for years I have used for pretty much everything, such as editing files, running R, Matlab, etc.). But I would not have been able to learn magit without first knowing the Git commands from using the shell, and occasionally need to use the shell to do something more advanced. For now we will stick with the Git shell for the aforementioned reasons. It will also give you a better understanding of Git and GitHub, and emphasise that you can use Git for any files, not just R code. 2.4.5 Powershell and posh-git Download a Powershell (a shell window in which you can type commands, presumably the power part means its more powerful than a basic version) and then posh-git (which adds some extra features). Do this by following the instructions at https://github.com/dahlbyk/posh-git . Note that that is a GitHub site  it is storing the code for `posh-git (so anyone can see the code), but when you scroll down you see instructions (like in the repository example in Video 1). Do the Installation and Using posh-git sections. If you dont understand some options (I dont!) just pick the simplest, usually the first. 2.4.6 Create a directory to keep all your Git-tracked work It is handy, but not essential, to keep directories that you are tracking with Git and GitHub all under a single directory; its a helpful way of reminding you that you are tracking such directories with Git. You can use something like C:\\github (Windows) or ~/github (Mac) or whatever you like, but make sure there are no spaces or special characters anywhere in the full path. So create such a directory. You will navigate to it later when using Git. 2.4.7 Install Diffmerge (optional) A differing tool can be used to examine differences between different versions of files. There are many programs that can be used but for consistency we will use Diffmerge. It is nice to have but not essential if you have trouble installing it. Install Diffmerge: https://sourcegear.com/diffmerge/downloads.php 2.4.8 Save our template .gitconfig file Git uses a configuration file (called .gitconfig) for your account info, name to use when committing, aliases (shortcuts) for commands, and other things. We are supplying a template .gitconfig file that you will then edit. For Windows, you will save it as C:\\Users\\YOUR-COMPUTER-USER-NAME\\.gitconfig, where YOUR-COMPUTER-USER-NAME is your username on your computer. For MAC and Linux (we think), you will save it as ~/.gitconfig. If there is already such a file there maybe save a copy of it under a different name so you dont completely overwrite it in the next step. The template file is on GitHub here. Do Save-As (right click or from a menu) and save the file as the filename mentioned above. Just a reminder for your understanding: that file on GitHub is shown in the normal way on the Github repository for this module: https://github.com/quantitative-biology/module-1-git/blob/main/misc/.gitconfig and the link we gave you is from clicking `Raw, which just gives you the raw code for the file  this works for any file on GitHub, as mentioned in Video 1. 2.4.9 Edit the .gitconfig file Use your text editor to open your saved .gitconfig file. Given it has a non-standard extension (i.e. it is not something like file.txt) you may have to specify to open it with your editor. Change the [user] name and email settings to reflect your information. Change the [difftool] and [diffmerge] directories so they point to the location where you have DiffMerge (if it installed okay). For Windows the location should be: C:\\Program Files\\SourceGear\\Common\\DiffMerge\\sgdm.exe For MAC the location should be: /usr/local/bin/diffmerge If you did not install Diffmerge then delete the nine lines from [diff] down to trustExitCode = true in your .gitconfig file. 2.4.10 GitHub authorisation You will use Git to communicate back and forth with your repository on the GitHub website. You dont want to have to enter a password every time. This is currently done by creating something called a Personal Access Token (PAT). If using R then do: install.packages(&quot;usethis&quot;) # Unless you already have the usethis pacakge install.packages(&quot;gitcreds&quot;) # Unless you already have the gitcreds pacakge usethis::create_github_token() # This opens up GitHub in your browser to set up # a PAT token. Copy and paste the next line into # R and then complete the setup on GitHub # (stick with defaults), copying the resulting # PAT token to your clipboard. gitcreds::gitcreds_set() # Paste the PAT token from the clipboard as # requested. This seems to be the simplest method. If you dont use R then either Install R from here and just copy the above lines into an R window. It obviously may seem overkill to install R just to set up something else, but the PAT stuff is fairly new, and the R route is fairly straightforward. Else, try these instructions. They mention that you may be asked for your PAT the first time you use Git (with the git clone command you will see soon), and that your PAT may get cached (meaning you dont have to keep entering it every time you interact with GitHub). If you repeatedly get asked for your PAT then see the instructions at the bottom of that page regarding caching your credentials. 2.4.11 Navigating in a shell You need to know how to change directories in the Git shell. This is like clicking on folders or going back in File Explorer (Windows) or Finder (Mac). Bascially you use cd &lt;dir.name&gt; to change directory (move into that directory), cd .. to go back up again, and dir (on Windows) or ls (on Mac) to list the contents (either command works for Linux). These are shown in the following screenshot, for which I opened up my Git shell in Windows and did the above commands, moving into the test directory, seeing whats there, and then moving back up: Figure 2.10: Screenshot of basic shell commands. You will create such a test directory during Video 2. 2.4.12 One-time authentication The first time you get set up or start using Git, there might be some one-time authentication to connect to your GitHub account. For example, in Windows you might be asked to type git config --global credential.helper wincred in the Git shell. So just follow any instructions if they come up. 2.4.13 MAC only: make your output pretty On the MAC, navigate to the ~/github directory in a shell and run the following command: git config --global color.ui.auto This will make your git output colored in a similar way to the Windows powershell version. 2.4.14 Something to view Markdown files (optional) Each project has an associated README.md file that appears on its GitHub homepage. The extension .md stands for Markdown and is just an ASCii text file that contains simple formatting (such as bold or italics). You can edit the file in any text editor. There are two options we have used to view the rendered versions of markdown files (rendered means it utilises the formatting you have coded). Choose one: The Markdown Pad 2 editor/viewer which is easy to use: http://markdownpad.com. Just get the free version. The Chrome extension for markdown viewing: https://chrome.google.com/webstore/detail/markdown-viewer/ckkdlimhmcjmikdlpkmbgfkaikojcbjk?hl=en. Here is an intuitive short introduction to Markdown, which is worth looking at once you start writing more in your README.md files: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet. With one of the above two options installed when you click on a README.md file you can edit the file and also see the rendered (formatted) version. I find that Markdown Pad 2 doesnt show (render) the README.md file exactly the same way as it appears on the GitHub website, with the GitHub version being better. 2.4.15 Bonus keyboard shortcut (optional) In the upcoming Video 2, I may occasionally type g instead of git as I have that shortcut set up. To enable this shortcut for yourself (this is not essential), create a file (on Windows at least) called g.bat somewhere on your PATH, and have it contain just the line git %*. 2.5 Using Git and GitHub to share your own code This section shows how to use Git to efficiently save ongoing versions of your files and GitHub to share them. This is the crux of this module. It consists of Video 2 where I demonstrate how to use Git and GitHub. You can pause the video at the suggested moments to repeat what was demonstrated. Two exercises are included. The notes below repeat the details from the video, so you can easily refer to them later (without having to search within the video). The slides that I show are also available here (though the notes below mostly replicate the slides). Okay, now for Video 2 (click on the screenshot below; video is 32 minutes): 2.5.1 Definitions Lets recall the main definitions: Repository  essentially a directory containing all your files for a project (plus some files that Git uses). Also used to mean the website on GitHub. Git  a program that allows you to efficiently save ongoing versions of your files (`version control) and link with GitHub. GitHub  a website that hosts your repositories so that you can easily share code and collaborate with colleagues. Basically, the idea is that you work on your files in a repository on your computer, use Git on your computer when you are happy to keep your changes, and use GitHub to easily share the files. In the opening video you learnt how to navigate a repository on GitHub and how to download other peoples code Here you will learn the important steps for your own repositories: Creating  create a new repository on GitHub Cloning  copying it to your local computer Committing  the crux of working with Git Collaborating  efficiently work with colleagues Conflicts  fixing conflicting changes when collaborating (happens rarely) 2.5.2 Creating a new repository Sign into your GitHub account, click on the Repositories tab, and press the New button. Give your repository a name. Lets call it test. Check Initialize this repository with a README. Leave Add .gitignore and Add a license set to None Click Create repository. You now have a new repository on the GitHub website. Next we will clone it onto your computer. 2.5.3 Cloning your new repository Copy the full URL (web address) of your test repository. Open the Git shell and navigate to your C:/github directory (or whatever you called it when you created it in the setup instructions  its the place you are going to save all your Git repositories). See the Navigating in a shell section earlier. Run the following command to clone your repository: git clone URL where URL is the url of your newly created repository (paste should work). You should now have a subdirectory called github/test on your computer. In Git shell, change to that directory (with cd test). The command cd just stands for change directory, to go back to where you were just do cd ... You probably want to also open the directory using your usual graphical interface (e.g. File Explorer in Windows) to check what is happening. So clone is Git speak for copying something from GitHub onto your local computer. This example has just one file (README.md). But the process is the same for a repository with multiple files and multiple directories, and the complate file sturcture is fully preserved. Windows only: Storing your credentials When you are using the Git shell for the very first time on Windows, issue the following command: git config --global credential.helper wincred This means that you dont have to repeatedly enter you GitHub password (just do it when you are first prompted). 2.5.4 Committing Create a new file, newFile.txt, in the github/test directory. Open it, add a line of text at the start of the file and save it. Check the status of your (test) repository: git status It should say that you have an Untracked file called newFile.txt. You want to tell Git to start tracking it, by using: git add. gitignore Type git status again. You should see that the file is listed as a new file under Changes to be commited. Lets now commit it: git commit -a -m \"Add newFile.txt.\" The commit message (in the quotes) should be a useful message saying what the commit encapsulates (more on that later). Push the commit to GitHub: git push Check (refresh) the GitHub webpage and see your commit and the uploaded file. What just happened? We just used three of the main Git commands: git add &lt;filename&gt;  tell Git to start keeping track of changes to this file. You only need to tell Git this once. git commit -a -m \"Message.\"  committing your changes, which means tell Git you are happy with your edits and want to save them. git push  this sends your commit to the GitHub website. You always have your files stored locally on your computer (as usual), even if you dont add them or commit changes. When you push to GitHub then your colleagues can easily fetch (retrieve) them. Keyboard aliases (shortcuts) Now, git commit -a -m \"Message.\" is a bit much to type, so we have an alias for it: git com \"Message.\" This is defined in the .gitconfig file you installed in the git-setup instructions into C:\\Users\\YOUR-USER-NAME\\.gitconfig (for Windows). You can also add your own commands to that file. The -a means commit all changes of files that Git is tracking, and -m is to include a message. Since we usually want to do both of these, git com \"Message.\" is a useful shortcut. But it is important to realise it is an alias if searching online for help. Similarly: git s  for git status git p  for git push git d  for git diff git f  for git fetch From now on we will mostly use the aliases. Use the full commands if the .gitconfig file didnt work for you. Edit Readme.md Edit the Readme.md file. Add some simple comments describing the project such as: A test repository for learning Git. Look over the changes, commit them, and push them to your GitHub repository: git s git d (or git diff)  this gives a simple look at the differences between the last committed version and your current version (of all files; only one in this case). git com Initial edit of Readme.md git p (or git push) Refresh your GitHub web page and you should see your text (the Readme.md file is what is shown on the main page of your repo). If you got Diffmerge installed okay, then instead of git diff you can do git difftool. This opens up, in turn, each file that changed since your last commit and shows you the differences. On the right is the new text (actually the code for this paragraph): Figure 2.11: Screenshot of using git difftool. This is useful for changes that are more complex than can be easily see in the quick git d. 2.5.5 Exercise 1: create, edit and commit simpleText.txt Create a text file simpleText.txt in your local test repository. Add a line of text at the start and save it. Predict what git s will tell you, then type it in the Git shell to check. Add the file to the repository using the git commands: git add simpleText.txt git s  not necessary but useful to check you understand what is changing before you commit git com \"Adding simpleText.txt\" git p Add some more test to simpleText.txt then git com \"Message.\" and git p. Repeat this a few times to get the hang of it. git com frequently and git p occasionally (you do not have to push every commit), while intermittently doing git s and git d to understand whats changing. Keep an eye on your commits by refreshing the GitHub page. In reality when writing code/text you wont be committing quite so frequently, as your focus will be on the writing the actual code/text. Adding multiple files at once Often you add multiple files in a new directory. When you run git s, you will see a large list of Untracked files. They can be all added at once by simply adding the whole directory. 2.5.6 Exercise 2: multiple files Do the following, to get the idea of creating multiple files in a folder and committing that folder. Create a new directory called new-stuff in your test repository, using your normal method (e.g. in File Explorer, or just mkdir new-stuff in the shell). Navigate to it in your shell (cd new-stuff). Add a few new test files to that directory called test1.txt, test2.txt, etc. Put some example text in one or more of them if you want. On the command line, check the status: git s You will see a listing showing the new-stuff/ directory in Untracked files. To add all the new files in preparation for a commit, issue the command: git add new-stuff/ Check the status of the repository again with git s It will now show all files in Changes to be committed Commit the changes: git com \"Added new-stuff directory.\" Push the changes to GitHub: git p Check your GitHub webpage and see your commit and that the files have been uploaded. That works no matter how many files are in your new-stuff directory. There could be a hundred and its the same command. Wildcard symbol * This is useful to know (no need to do it as part of the exercise): To add multiple files with similar names you can use the wildcard * symbol. You just added (told Git to keep track of) the new files in your new-stuff/ directory. If you add more new files to that directory, you will have to tell Git to track those also. This is because they are new  you havent told Git about them yet. Say you have 10 new files called idea1.txt, idea2.txt, , idea 10.txt. Instead of typing git add new-stuff/idea1.txt git add new-stuff/idea2.txt etc. you can just use the wildcard symbol * which stands for any piece of text: git add new-stuff/idea*.txt or even just git add new-stuff/*.txt or git add new-stuff/*.*. The .gitignore file But what if you dont want to add all the files that you create? Each repository can have a .gitignore file, in the root directory of the repository. Such a file has names of files (such as my-secret-notes.txt) or wildcard names (such as *.pdf or *.doc) that will be completely ignored by Git. For an example, see https://github.com/pacific-hake/hake-assessment/blob/master/.gitignore, noting that the # can be used for comments. When sharing a repository with others, you want to share your code (for example, R, Python or Matlab code) and maybe data, but generally not share the output (such as figures that the code generates; more on this later). For reproducible research your colleague (or anyone) should be able to run your code to generate the results. Some programs you run may make temporary files that dont need to be tracked by Git, the names of which should also be included in your .gitignore. When sharing code or collaborating you want to keep your repository as clean as possible and not clutter it up with files that other people dont need. So when you run git s and see untracked files that you dont want to be tracked, add them (or a suitable wildcard expression) to your .gitignore file so that they are not added inadvertently. This will also simplify your workflow (you dont need to keep being reminded that you have untracked files). If you are on MacOS and you find that folders have a .DS_Store file in them, then include .DS_Store as a line in your .gitignore file. Generally, when you create a new repository you probably want to copy an existing .gitignore file over from an existing repository, as you will generally want to ignore the same types of files. You can also choose the private repository option when creating a repository, so that you can control who can see it. On your repository page on GitHub, go to Settings--Manage Access to add collaborators. 2.5.7 What to write in commit messages You dont want to agonise over what you write in your commit messages, when doing git com \"Message\", but it is worthwhile making them useful. Ideally You want to describe what (and sometimes why) you did something. The how is not needed since that will be explained by the actual changes in the code. If someone wants to see how something was done, they can see what was changed in detail in the commit. The message should be informative for collaborators (including your future self). Not useful: git com \"Tweaked function.\" Useful: git com \"Allow plot.biomass() to use extra colours.\" A good rule of thumb is to just complete the sentence This commit will . This is helpful for your collaborators and your future self. You have now learnt the basics of using Git. By creating a public repository on GitHub you can now release your code to the world! 2.6 Using Git and GitHub to collaborate with colleagues 2.6.1 Demonstration of collaborating Now we will show how to collaborate with colleagues, which is where the usefulness of GitHub will become more apparent. There are a few different ways to collaborate using Git and GitHub. We will focus on the following one since it is the simplest, and is what you need to collaborate with colleagues. Concept: there is a project where people contribute to a main repository that is considered the master copy. Everyone clones directly from the creators repository. All collaborators push their commits to the repository (the creator has to add them as collaborators once on GitHub). Since the creator has to grant permission, you wont have just anyone contributing to (and maybe messing up your work), just your trusted collaborators. But you have to trust your team to not mess things up (more on that later). In Video 3 we demonstrate the following (skip these bullet points if you dont like spoilers!): Kim creates new repo called collaborate (and clones it to her computer). Andy clones it also. On GitHub, Kim gives Andy push access to her collaborate repo. Both do some edits (create some new simple text files). For Andy to get Kims updates (and vice versa), he just uses: git fetch (or just git f)  fetches the latest version of the repository from GitHub onto your computer. Your local files have not yet changed (check them), but Git has the changes stored on your computer (?!?). git rebase  updates your local repository (the committed files on your computer) with the changes you have just fetched, merging both peoples work together. git p  pushes the merged changes back up to GitHub so that the other person can get them. That is the basic workflow. Here is Video 3 (click on the screenshot; video is 21 minutes): In the video you saw that we also showed an example of git p not being allowed for Person A because there are recent commits on GitHub (by Person B) that Person A has not yet merged into their local version of the repository. Here is an example of the error message you get: Figure 2.12: Example of a lengthy error message when trying to push; essentially its just telling you to fetch before trying to push. While a bit lengthy, the error message is useful. It forces you to get the other persons work before you push yours. You do this by: git f git rebase. So to be allowed to push, just fetch to get the new commits onto your computer, and then rebase to combine the commits into your local version. Then you can git push. Here is a full screenshot (g is just a shortcut for git). The green up arrow number 8 tells me I have 8 commits to push to GitHub. Its generally good to skim the white output text to check that Git did not give an error. The yellow arrows I think of as just implying I need to do a rebase (before doing that I would usually browse through the other persons commits on GitHub to check what they did): {r, echo = FALSE, fig.cap = \"Example of output when having 8 commits to push but then having to fetch and then rebase.\", fig.alt = \"Screenshot showing yellow text with up and down arrows, which I think of as meaning I have fetched commits and just need to rebase.\", width=\"100%\", fig.align = \"center\"} knitr::include_graphics(\"fix-the-pull.png\") After the rebase I was allowed to push and then everything is up to date. 2.6.2 A bit more about git rebase Andy commits local changes, tries to git push but is told to first git fetch (to get Kims changes from GitHub). Andy does git fetch and then git rebase. What git rebase does is basically rewind to the last common commit that both people had, and then add one persons commits and the others. Andy then does git push to push his commits to GitHub (from where Kim will fetch them when shes ready). Providing there are no conflicts, this will work fine. Another option you hear about is to do a git merge, which basically creates a new commit that merges both peoples work together. In our teams we used to use git merge and now use git rebase; some people dont like git merge because it adds extra commits. For a more in-depth understanding see here for one of the clearer explanations out there concerning rebase versus merge. Note that the error in the above screenshot (when I could not git push) told me that I might want to do git pull. This is basically git fetch git merge in one command, but it seems preferable to do git fetch git rebase. 2.6.3 Fixing a conflict A conflict happens when two people have edited the same line(s) of the same file. Conflicts happen relatively rarely and can be generally avoided by co-ordinating with collaborators so that you are working on different files. But, they will happen and you need to know how to resolve them. Git forces you to explicitly decide whose changes to keep  this is a good thing, since you want a human to make such a decision. In Video 4 we demonstrate a conflict, and fix it using the following (which is the best approach we have found): Trying git rebase will tell you there is a conflict. git rebase --abort  do this to abort the rebase attempt. git merge  this will tell you there is a conflict and which files are conflicting. Open the file(s) with the conflict and edit the text. You will see the conflicted section bracketed like this: &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Line(s) of text/code which are currently in your file. ======= Line(s) of text/code which are trying to merge in, but conflict. &gt;&gt;&gt;&gt;&gt;&gt; origin/main Note that origin/main refers to the version you have fetched from GitHub. Remove the line(s) of text that you do not want to keep (or edit the line(s) to be something else entirely), and remove the bracketing lines &lt;&lt;&lt;... and &gt;&gt;&gt;..., and the ====== line (this is all shown in the video). Save the file(s). git add &lt;filename(s)&gt;  you have to then add the file(s) that had the conflict (I am not sure why this is necessary, I just do it). git com \"&lt;message&gt;\"  in your commit message you can explain why you fixed the conflict. For example: git com \"Kept Kim's edits as more consistent with remaining text.\" This is useful so that your collaborators know you have resolved a conflict (they can look at the commit to see if they are happy with it). Here is Video 4 (click on the screenshot; video is 15 minutes): 2.6.4 Exercise 3: collaborating on a single repository If you have a colleague available, try what we just did: Person 1 creates a new repository on GitHub and clone to their computer. Give the Person 2 push access to the repository (on the repo page on GitHub: Settings  Manage access  Invite a collaborator) Person 2 clones to their computer Both create a simple text file (use different filenames), add some text and, as usual, add, commit, and push. git fetch and git rebase to get the other persons file. Continue editing either file, committing, and pushing. If you get the push error (shown earlier), refresh GitHub repository site to see recent commits (click on the X commits link, where X shows the total number of commits to the repository). You can easily spot the other persons recent commits. Click on one (the bold message) to see details. Purposefully create a conflict (both edit the same line of the same file). Resolve it as described earlier. In practice you wont commit so frequently when working, but this is good to get the hang of it. 2.6.5 Collaborating summary Congratulations, you now know the few basic commands and functionality needed to collaborate with Git and GitHub. It takes a bit of practice, but it is very powerful. 95% of the time, this is all you are doing: Change some code. git s git d git com &quot;My commit message&quot;` git p (the git s and git d are useful to check you have changed only what you think you have changed). If GitHub does not allow you to push then on GitHub check your colleagues latest commits that you havent yet seen, and if they look fine then: git fetch git rebase (If you dont agree with your colleagues latest commits, then still rebase them but then manually edit the files to be what you want. Though if you want to completely cancel their commits see git revert in the Advanced section below). If conflicts, then git rebase --abort git merge fix the conflicts manually and then git add &lt;conflicted file(s)&gt; git com &quot;Message to explain what you did&quot; git p Change some code and repeat! 2.7 Workflow tips when collaborating Here are some tips that weve developed based on our own experiences. Overall, remember that you still edit and save your files in the usual way on your local computer. If you dont do Git commits you will still have the latest versions of your files on your computer, as you would if you werent using Git at all. So if you do get stuck with Git you can carry on working as normal (though you probably do want to try and fix it at some point). When collaborating: If working closely with others, when you start each day (or after a break) then make sure you are up to date and have all their commits. Refresh the GitHub page for you repository, and git fetch (or just git f) and git clone if needed. (To be safe you can git f and git s to check). We find it helpful to co-ordinate our work (Slack is useful for this, or use GitHub Issues for complex discussion  see below), so that if multiple people are working at the same time, you are at least not working on exactly the same parts, just to reduce conflicts. Commit fairly frequently and write helpful commit messages (so your colleagues get an idea of what youve done in each commit). Push less frequently, and dont push code that doesnt work  that will annoy your colleagues. And then they (and probably you) may both spend time fixing it. To see who last edited a particular piece of code, when viewing the file on GitHub click Blame, as mentioned earlier. GitHub Issues GitHub Issues are very useful for discussing issues with your repo. For our annual Pacific Hake assessment we have used them extensively over the years: Figure 2.13: Example of GitHub Issues. The Issues tab lists our current Open issues  we have 20, of which five (the most recently posted) are shown here. We are currently in-between assessments (and not working on it), so we have created Issues that we want to think about or deal with for next year. This avoids forgetting about ideas or losing them in old emails. Issues are intuitive to use. There is a bright green New Issues button to create new ones, you give a title and then write some details, people can reply, you can assign people to look at them, and you can close them. In the above screenshot you can see that we have closed 815 issues (this was over several years). Useful tip: when doing a commit that refers to an Issue, if you refer to the Issue number (with #&lt;number&gt;) in your commit message, then after pushing that commit the Issue on GitHub will automatically mention and link to the commit: git com \"Add more options to fancy_function(), #21.\" will mention the commit when you look at the issue. You can even automatically close the issue by saying closes #21 in your commit message: git com \"Add more options to fancy_function(), closes #21.\" Issues are particularly useful to avoid cluttering up code with commented notes or ideas that you may easily not come back to, or avoiding endless emails that end up getting overlooked. You dont have to fix an Issue to close it, you can decide not to pursue, but at least you have made a decision. (We also use Slack a lot to communicate, but moreso for quick questions or bouncing ideas around  Issues are better for stuff that you want to come back to at some point). You may receive emails regarding Issues. If you use GitHub a lot you will spot Notifications (the blue dot on the bell in the top-right corner when signed in on GitHub) that will show you new Issues of repositories you are involved with, or if anyone has updated an Issue. You can then turn off the email notifications. GitHub organizations If you will frequently collaborate with colleagues, you can create an Organization on GitHub and invite collaborators to it (click on your GitHub photo in the top-right corner, Settings, Organizations). Then they will automatically have access to all repositories created under the Organization. You can choose the security settings. So, congratulations for getting this far  we have covered the basics of Git and GitHub to get you going. Occasionally you might get messed up, but it is generally hard to actually lose any work. If you get stuck then see The power to go back part of the next section, that contains more advanced material. Good luck and happy committing! 2.8 Beyond the basics of Git and GitHub  getting more advanced This section gives slightly more advanced background that should further improve your understanding (including why Git is useful even when not collaborating or sharing your code). If youre doing this module for the first time maybe read this section now so that you know what is here, but dont worry about understanding it all until you have become more familiar with using Git and GitHub. 2.8.1 So Ive made some changes but dont really want to keep them  git stash If youve changed some code but have not committed it, and then maybe got in a mess and just want to go back to your last commit, you can stash your changes git stash and to include a message (for your future self): git stash save &quot;Message&quot; This stashes them away such that they can be retrieved later if necessary. This is handy. You may think you dont want to keep those changes, but sometimes you may later wish you had kept them somehwere. Note this is only for files that Git is tracking (i.e. files that have been added at some point). To retrieve the last stash, you pop them back into your working copy with: git stash pop You can have multiple stashes, seen by doing: git stash list To deal with these, and other aspects of stash, see this tutorial. If you are really really sure that you do not want to keep your recent changes, see the Undoing changes not yet committed section near the end of this module. 2.8.2 The power to go back With Git you can go back to any previous state of your repository. This is very powerful, though slightly scary at first. Do this with your test repository, that should have some files in it from the earlier excercise: git s to make sure you are all up-to-date (commit and/or push if necessary). In File Explorer (or whatever you use) look at your repository, you should see all your files, including the new-stuff\\ directory. Look at the commit tab on GitHub for your test repo and click on the clipboard icon to copy the HASH number thingy to the clipboard . In Git shell: git checkout HASH (where HASH is the pasted HASH, or git co HASH using our Alias) Look at File Explorer again  your new-stuff directory should have  disappeared!! (If it hasnt disappeared then open it  the test files, i.e. test1.r, test2.r, etc. should be gone, but your text editor may have saved backup versions; manually delete them plus the new-stuff/ directory.) You are now back to the very first version of your repo! Powerful and scary. Now, to get your files back to the most recent version you had committed: git checkout main (it used to be git checkout master, the names have recently changed). Thats it! Check that your files are back. All this means that you can go back to any previous commit in your repository. This is very reassuring. For example you have some complex code that you realise is now a complete mess and you want to go back to yesterdays version of everything. In practice you rarely actually do this, but its very comforting to know that you can. Consequently, your workflow is less cluttered and more tractable than having to save multiple versions of the same files with dates in the filename, such as this nightmare shown earlier: Figure 2.14: Having multiple versions of the same file can be confusing and easily lead to not all edits being considered. Retrieving older work in practice I think there are fancy ways that Git can replace a current file with a version from an earlier commit. But, in practice (especially since you rarely want to do this) it is a bit safer to do the following: Say you are up-to-date (git s says all is good), but your program my_code.R just isnt working and you want to go back to the version you had yesterday at commit number abc123. git co abc123 (or git checkout abc123) to checkout the earlier commit, which includes the old version of my_code.R that you want get. Copy my_code.R to a new file my_code_old.R. In the shell you can just do this with cp my_code.R my_code_old.R. Do NOT edit my_code.R or make any changes, as you may end up with a scary DETACHED HEAD warning. git co main to checkout the latest version again. Since you have NOT done git add my_code_old.R, Git is not tracking my_code_old.R and so it is just sitting in your folder as normal. Now you can manually copy what you want from my_code_old.R into my_code.R to fix your problem. It could be the full file, or just some part of it. Then commit as normal. At some point you can delete my_code_old.R so it is not hanging around, but you dont have to. (Though maybe make a note in it as to which commit it was from, in case you do need it again). 2.8.3 So how does Git do all this? By now youre probably wondering how Git keeps track of everything. Git does not keep versions of code, it keeps commits. The commits are kept track of using a HASH key which is a generated 40-digit key in hexadecimal (base 16). The hashes are what you see on GitHub and in various places when you use Git shell. By stitching all the commits back together again, Git can recreate all your code. There is a hidden .git/ directory in each repository. Look at the .git/objects/ subdirectory. Each subdirectory name is the first two digits of a HASH. The rest of the digits of the HASH are the filenames in the subdirectory. You can basically think of the hashes as representing commits (apparently they can also be blobs and trees, whatever they might be). I think of the files in the subdirectories containing the differences between each commit. Because of these structures, Git can go back and rebuild any or all files at any commit, and even have different directory structures at each commit. Since Git is keeping track of differences between files, this all works best for plain ASCii (text) files, such as .R, .txt, .Rmd, etc. Git does work for binary files, such as .xls, .docx, .RData, but since changes to the files are not easily saved (Git essentially has to resave the whole file at each commit), this is not very efficient and may make your repository large. Such files will be fully resaved every time they are changed. Think of a binary file as something that you cannot open in a text editor and read (it does not contain simple ASCii letters and numbers). Exceptions: often you may have an image or photo or other type file that you need to share for a document, but it isnt going to keep changing. So thats fine to commit. An example of why you should not commit binary files: A collaborator was running some R code (and correctly committed the .R files so that I could run it), but also committed the results, which included .pdf, .png and .RData files, which can get quite large. But, these latter files got updated every time the code was run. So changing one line of the .R code (which Git deals with very efficiently), and running that code and committing, resulted in the new .pdf etc. files being fully saved (since Git cannot just save the difference from the last commit because they are binary files). Even if, say, one point changes on a figure in a graph in a .pdf file, Git has to save the whole new version. This ended up with .git/objects/pack (whatever that might be!) being 2.8Gb. I needed space quickly on my computer so just deleted four files in .git/objects/pack, which freed up 1.6Gb. Note that I still had the actual final versions of files (as you would if not using Git), but just not the full repository history. However, when I tried to later do some work and then commit I got lots of fatal errors with scary messages like bad object HEAD and the awesomely titled You are on a branch yet to be born: Figure 2.15: Example of why you shouldnt mess with the .git directory, as you get scary error messages and have to start again. I just had to start again from scratch (clone again I think). Take-home message: Dont mess with the .git directory!! 2.8.4 Git terminology At some point you will likely need to search online for some help (often questions are posted and answered on the excellent stackoverflow website). A bit more understanding of terminology will help you. Remember that Git keeps commits. Several of these commits have pointers to them that have special names: HEAD points to the commit you are currently on in the Git shell. main or master is the default branch when you set up a repository on GitHub (it is usually main now because of recent changes on GitHub). 2.8.5 Branching So far we have only worked on the main branch. Sometimes you want to create a new branch that branches off from the main branch. Its bit like a tree branching, except that at some point you want your new branch to be merged back into main. For example, you may want to try adding some new code to your project, but dont want to break what is already there. You may do this even if working alone, but its especially useful if you are collaborating, or if, say, you have an R packages hosted on GitHub that anyone may be downloading  you dont want to annoy them by pushing experimental code that doesnt work. So you would create a new branch, work on that new branch (i.e. commit changes to the new branch), and when you are happy with your new changes you can easily merge it all back into main. Working on a new branch When creating a new branch, your starting point is identical to the branch you were when you created the new one. In the Git shell navigate into your test repository: cd test Depending on your set up, you should see main indicated somewhere (if not do git s and it should say On branch main. Make sure you are up-to-date and have committed all changes (git s, and commit if necessary). Create a new branch called temp, this will be based off the latest commit of the main branch you are currently on: git checkout -b temp (We have an alias for that: git cb temp`). You will be automatically placed in the new branch called temp, and commits you make will now occur in that branch only. Make and commit some changes (e.g. add a new file)  these will now be on your temp branch. You can push to GitHub. The first time you try git p, the Git shell will tell you that you need to type the following so that future pushes go to the new branch: git --set-upstream origin BRANCH-NAME Check the GitHub webpage to see that your branch was pushed. You repository page (that will still be looking at your main branch) may tell you that there is a temp branch with more recent commits than main. If not then if you click on the main drop-down menu: it should give you the option to look at your new temp branch. (The 1 branch in the above image should also say 2 branches). You can now view your new file in your new temp branch on GitHub. A graphical way to see and understand branching is to click on InsightsNetwork to see the Network graph. The Network Graph is a useful visualization tool, where each commit is shown as a point on the graph (the numbers along the top are the dates). You can hover your mouse over a commit to see who committed it and the commit message. You can click to see full details of the commit. The Network Graph is particularly useful if you or others are working on multiple branches, or to check details about merges. Okay, back in your Git shell you can easily switch back to your original main branch: git checkout main (or the alias git co main). You will see that the file you just added is gone, because it only exists in the temp branch at this moment. Imagine that in your temp branch you did several commits to create a new function in your code, or have added some new text to a report. Now you are happy with what youve done you want to merge it back into the main branch. To view all local branches: git branch There is an asterisk next to the branch you are currently in. To switch to another branch (main in our case): git checkout main To combine the changes from the temp branch: git rebase temp or git merge temp Now the file you created in the temp branch now appears in the main branch. All commits done in the temp branch will now be in the main branch as well. If there was a merge conflict, you must fix it at this point (see earleir). Once youve merged your temp branch into main, you dont really need temp any more and so it is good protocol to delete to keep things tidy: git branch -d temp If you have unmerged changes in a branch, you will not be allowed to delete it, but Git shell will tell you the command to forcibly delete it: git branch -D temp Warning  you wont be able to get any of those changes back once you do this. To remove a branch entirely from GitHub: git p origin --delete BRANCH-NAME 2.8.6 Pull requests You often hear about Pull requests, but in my workflow Ive rarely used them. A pull request is when a contributor asks a maintainer of a Git repository to review code the contributor wants merged into the repository. Its not really used when collaborating with your own team, but more when you have started with someone elses repository and either made a branch or forked it, made some improvements (you hope!), and then you request that they pull your improvements into their main repository. So fairly advanced. For more details see here. 2.8.7 Undoing stuff Undoing committed changes If you want to undo your previous commit, then use revert: git revert HEAD This actually creates a new commit with the automatic message Revert \"&lt;previous commit message&gt;\". (So you can always revert your revert if you have to! Nothing committed is lost). If you make a commit followed by other commits, then realize you want to undo the earlier commit, you use: git revert HASH where HASH is the hash for the commit you want to undo. Remember that Git shell is smart enough that you only need the first five digits: git revert 1ef1d Obviously, you have to be careful with this if youre changing something that was a few commits back, as you might mess up your code. Note that the word revert is slightly confusing here  to me its really an undo of your earlier commit. It is not reverting back to the earlier state of everything at that commit, for that you use git checkout as discussed above. Undoing changes not yet committed If youve made a mess in your working directory and you want to change everything back to the way it was on the last commit: git reset --hard HEAD If youve messed up a single file and just want that one file to go back to the way it was on the last commit: git checkout HEAD &lt;filename_to_restore&gt; Warning  running these commands will delete the changes you have made. Since you have not committed any changes, they will be lost. Make sure you are certain you dont need the changes before running these commands. If you arent sure if you need the changes again in the future, use git stash instead. Changing the commit message in the last commit If you make a commit then realize you want to change it (add more information, fix something that will confuse your colleagues, fix something that will confuse you tomorrow), you can change the commit message: git commit --amend -m \"Correct message.\" This only works on the last commit. If you already pushed the commit before realizing that the message needs modification, do this: git p --force after making the amendment to the commit message. 2.8.8 Using R and GitHub within RStudio If you use RStudio to work in R, then you may want to use RStudio to use Git. Having already learnt the basic Git commands in a shell should help you. One way of using Git in RStudio is just to use the commands we have learnt within the Terminal: Figure 2.16: Example screenshot of using Git in the RStudio terminal. Alternatively, the RStudio interface has buttons with words that you are now familiar with, such as Diff and Commit: Figure 2.17: Screenshot of the Git buttons in RStudio. The blue arrow has options for fetching from GitHub, while the green arrow is for pushing to GitHub. When you commit, instead of including the message in the Git command like we did, with git com \"&lt;Message&gt;\", RStudio opens a window: Figure 2.18: Screenshot of the Git commit message window in RStudio. As you can see, having learnt what these various Git commands mean, it may be fairly intuitive how to proceed. To delve further, Jenny Bryans Happy Git and GitHub for the useR course introduces using Git in RStudio. It was originally developed for advanced-level statistics courses and is aimed at intermediate to advanced R users. It does include some use of a shell (like our module); for details on pre-requisites see here. Having worked through our Git and GitHub module, you should easily be able to pick up the necessary components that you need. In this modules videos that had R code, I was using the Emacs text editor. Ive used it for years for many reasons: it automatically highlights code in any language, is extremely versatile, has keyboard shortcuts which you can learn once and use forever, and as such it is fast to use when programming and avoids lots of mouse use (which can help avoid repetitive strain injury). I run R in Emacs with Emacs Speaks Statistics, and often use magit which is a way of using Git in Emacs. Also, enhancements to Emacs can improve accessibility for those that need it; for example, Emacs customisability can greatly improve productivity for visually impaired programmers. Finally, one colleague use RStudio to run R, Emacs to edit R files, and magit (in Emacs) to run Git. You will likely similarly figure out what setup works best for you. 2.8.9 Feedback We value your input! Please take the time to let us know how we might improve these materials using this survey. "],["r-markdown.html", "3 R Markdown 3.1 Motivation 3.2 Basic idea 3.3 Simple example 3.4 Output format 3.5 Caching (advanced) 3.6 Further reading 3.7 Using RStudio 3.8 Using Emacs for R (and everything else) 3.9 Feedback", " 3 R Markdown This module consists of text, example code, and two exercises to gently introduce to the world of R Markdown. 3.1 Motivation Say you were sent some tree data from a colleague, consisting of lengths and diameters of some trees repeatedly measured over 30 years. You then spent a month writing lots of R code to analyse the data. Your code also produces beautiful figures and tables, that you then manually copied into a Word document. You also had to write lots of text, including calculations such as the simple The average tree height was 10.1 m. Then your colleague sheepishly tells you that someone introduced an error when extracing the data from their tree-size database, and so you need to redo everything. Your heart sinks with the prospect of re-running all your code and making sure you manually copy the correct new figures into your document. Oh, and you need to redo the tables and check all the numbers in your text. The alternative modern approach is to use R Markdown. The idea is that you can generate a dynamic report. You write code that contains a mixture of your R code and your write up all in one place. You can use this for short analyses, scientific manuscripts, or even a complete thesis. For shorter documents you will have a single R Markdown file, and for more complex documents its better to break your work up into individual files that are linked together, such as chapters for a thesis. This module is aimed to get biologists started with the basics without too much extraneous information. Having gone through this module you will then be in a good position to learn more details from the online Definitive Guide to R Markdown and the RStudio introduction, which are aimed at a general statistical audience (rather than specifically biologists). A key concept is that everything is written as code. You do not have to manually point and click anything, or copy and paste figures between directories. So once you understand how something works or have figured out some formatting that you like, you can just copy that code and use it elsewhere. Example application A recent 328-page document we wrote in R Markdown is A reproducible data synopsis for over 100 species of British Columbia groundfish. For each of 113 species, we produced two pages of figures: Figure 3.1: First example page showing multiple figures of data. and Figure 3.2: Second example page showing multiple figures of data. Do not worry about the details  the point here is that for each species the layout of the figures is identical (even to show no data when none are available). Producing each figure and manually inserting them into a Word document would be extremely tedious and time-consuming. Instead, the production of the document is automated using R Markdown. Furthermore, the work is transparent and traceable. Because the code produces the figures (they are not pasted in from somewhere), we can trace back from the R Markdown code to see the R code that: pulled the data from databases fit models generated plots. In particular, we intend to periodically update the document as new data become available. While still a lot of work, it is less daunting knowing that the code is already available. On a practical level, the report has allowed anyone to see the data that are available, and has consequently increased data transparency between Fisheries and Oceans Canada, the fishing industry, non-governmental organizations, and the public. This is admittedly a very advanced example with a ton of code (several new R packages) and work behind it, but the idea is to show you what is possible and what you can work towards. 3.2 Basic idea In the above tree example, if using Word, for example, you would have a sentence that says The average tree height was 10.1 m The 10.1 is hard-wired into your text, and you typed it on from the value 10.1 that your R code calculated (in a variable you calculated, lets say you called it avge_height). In your R Markdown document you have your R code and your text write up. You would equivalently have: Instead of 10.1 you refer directly to the variable avge_height that you have already calculated. When you render your R Markdown document (convert it from code into .pdf, .html or other formats), it automatically fills in the avge_height value as 10.1. The means that the next bit of code (until the next backtick) should be evaluated using R, and the result inserted. This is the basic idea. Then, when your colleague mentions the error (or, say, provides you with extra data) you can just re-run your code and the 10.1 will be automatically updated in your document. This concept extends to your tables and figures  they can all be automatically updated. 3.3 Simple example Here we will generate some data, show some of it in a table, plot it, and show the results of fitting a simple linear regression. Read through this and then you will download and run the code in the exercise. Generate data First well need some libraries: Now generate some data: [1] 50 So we are showing our R code here (we can choose to hide it if we like), and it has been executed, yielding the printed output of the value of n (because of the final line of the code). We can also embed results from R within sentences, for example: We have a sample size of 50. This is done (as mentioned above) by the code: Note that you need the space straight after the r. We can also automatically say that the maximum value of the data is 506.5564788, or round it to a whole number: the maximum value of the data is 507. These were done by: Show some of the data Lets combine the data in a tibble (think of it as a data frame if you dont know what that is): # A tibble: 50 x 2 x y &lt;int&gt; &lt;dbl&gt; 1 1 23.7 2 2 14.4 3 3 33.6 4 4 46.3 5 5 54.0 6 6 58.9 7 7 85.1 8 8 79.1 9 9 110. 10 10 99.4 # ... with 40 more rows (only the first 10 rows get printed here thanks to dplyr). To have a proper table, we can do Table 3.1: The first rows of my data. x y 1 23.70958 2 14.35302 3 33.63128 4 46.32863 5 54.04268 6 58.93875 7 85.11522 8 79.05341 9 110.18424 10 99.37286 (If youre running the code separately the exact style may look different because of settings we have, but pretty much everything is tweakable with the kable and kableExtra packages). Plot then fit a regression data Now to plot the data: Figure 3.3: Simple plot of the 50 simulated data points. To fit and then print the summary regression output from R: Call: lm(formula = y ~ x) Residuals: Min 1Q Median 3Q Max -27.403 -4.366 -1.193 8.319 21.072 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.7071 3.2719 1.133 0.263 x 9.8406 0.1117 88.124 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 11.39 on 48 degrees of freedom Multiple R-squared: 0.9939, Adjusted R-squared: 0.9937 F-statistic: 7766 on 1 and 48 DF, p-value: &lt; 2.2e-16 And for a report we can produce a simple table (including a caption) of output and the regression fit: Table 3.2: Linear regression fit. Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 3.707115 3.2719100 1.133012 0.2628368 x 9.840634 0.1116686 88.123530 0.0000000 And create a plot: Figure 3.4: Simple plot of the 50 simulated data points with the fitted regression added in red. Now, lets go back and change the data The big feature of dynamically generating reports is when you go back and change or update the input data. For example, changing the data in the above example and then re-running it to redo the report. The best way to demonstrate this is for you to do it in Exercise 1. 3.3.1 Exercise 1 In R do library(rmarkdown) to make sure you have the rmarkdown package. If not then install it with install.packages(\"rmarkdown\"). Download rmark-exercise.Rmd onto your computer and put it where you want to work on this exercise. Make sure it is saved as rmark-exercise.Rmd and not rmark-exercise.txt. The file is an R Markdown (.Rmd) file that you can run by either doing rmarkdown::render(\"rmark-exercise.Rmd\") in R, or clicking the knitr button in RStudio and selecting the .html option. See the RStudio section at the end of this module for help using RStudio. Check that this has produced (rendered) an .html file document that looks similar to what you see above (dont worry if the styling is not identical, but the important content should be). Note that the .Rmd file is not repeating all the explanations that we gave above, but is a simplified version containing the commands, which is why the output wont be identical. (The exact R Markdown file, module-2-rmarkdown.Rmd, used to create what you are reading here is available on the GitHub repository for this module). Carefully read through the .Rmd file and compare it with what you see in the resulting .html. There are some comments in there, denoted by &lt;!-- comment --&gt; to help you, but you should be able to get the idea of how commands in the .Rmd file translate into output in the .html file. Copy the resulting rmark-exercise.html to rmark-exercise-orig.html, change n to 30 in rmark-exercise.Rmd, and re-render it. Compare the two .html files. You have done the same analyses but on different data. You changed one value, n, and consequently all the resulting calculations changed, and these are all updated in your new .html document! You have performed exactly the same analysis on your new data, including creating tables and figures. No copy-and-pasteing of tables, or manually keeping track of which figure corresponds to which analysis. That last bit is the crux of R Markdown. Once you understand that then you should incorporate it into your workflow. You can easily run identical analyses on two data sets, or do 10 runs of a model with different parameters, and easily compare the output. You can even get a bit clever with your writing by including an R ifelse statement to somewhat automate the text. Here is the results of some code (that will be included in the file): So the maximum value of \\(y\\) is 507, which is greater than the special value of 400. The greater than or less than part is given by But you have to be careful and think about all possibilities  what if \\(y=399.9\\)? 3.4 Output format A feature of R Markdown is that you can create output in various different formats, such as .html, .pdf, and .docx (if you are so inclined). This is often touted as a great feature  you can simply switch an argument in the start of your code to switch output formats. However, in practice, doing biological research and analyses, we have not found the need to have the same piece of code switch from .html to .pdf output. If you are writing a manuscript you are going to want it as .pdf, whereas if you are writing a vignette for an R package (these are done in R Markdown) you will probably want it as .html. The trouble with switching is that subtle formatting tweaks may render differently in the different formats, and you may end up spending a lot of time trying to make things work perfectly for two outputs  but in practice you may only really need, say, a .pdf. So we would recommend choosing which output you are going to use (depending on what you are writing your particular piece of code for), and then sticking with that. For example, I prefer to do exploratory analyses as .pdf while others may prefer .html. To keep it simple, in Exercise 1 we set the output to be .html. For scientific documents such as manuscripts and theses, a .pdf is preferable. We will now demonstrate this, for this you will need LaTeX installed. For decades, LaTeX has been the standard typesetting system for writing mathematical papers. In 2002, Friedrich Leisch created the R package Sweave, to weave together R code for calculations and LaTeX code for writing up results into a .pdf file. (It was called Sweave because S was the precursor language to R, plus I expect Sweave sounded better than Rweave). Sweave motivated the R package knitr (to knit together R code with text write ups) by Yihui Xie, which also allows html and other output formats. R Markdown was then created to allow simpler use of knitr, without learning lots of LaTeX commands, though you need LaTeX installed to use R Markdown. 3.4.1 Exercise 2 Now download rmark-pdf-example.Rmd onto your computer and put it where you want to run it. As before, make sure it is saved as rmark-pdf-example.Rmd and not rmark-pdf-example.txt. Try running it as earlier, either using rmarkdown::render(\"rmark-pdf-example.Rmd\") in R, or clicking the knitr button in RStudio but selecing the knit-as-pdf option. If it doesnt run then you probably need to install LaTeX (you should get a helpful error message). See the simple instructions in another book by Yhiui Xie et al., the R Markdown cookbook. Go through rmark-pdf-example.Rmd carefully, reading the notes in the comments, and comparing the .Rmd code to the rendered .pdf. I get three warnings (not errors) like Package xcolor Warning: Incompatible color definition on input line 283. It seems to work so dont worry if you get them also. Technial aside: Using those warnings as a teachable moment, this is why I set keep_tex = TRUE in the YAML header of rmark-pdf-example.Rmd. When you render the document, the rmarkdown::render() R function uses knitr::knit() to run the R code in your .Rmd file, and then build a LaTeX .tex file, which is then run by Latex to build a .pdf. Its good to be aware that there are several steps going on here, especially when something doesnt work. The warning in the previous paragraph is referring to line 283 of the rmark-pdf-example.tex file (the .tex file, not the original .Rmd file). The default R Markdown setting seems to be to delete the .tex file when everything builds okay, and so the .tex file was getting deleted. So keeping the .tex file can be helpful to try and debug warnings or errors that need fixing. Generally you want to fix warnings when you first see them occurring, but sometimes you just have to leave them in as everything seems to be working as intended (as is the case here). Also, if you are using Git and GitHub then you would track the .Rmd file using Git, but add the .tex file to your .gitignore file since it is an output from the .Rmd file (there is no need for Git to track each version of it). 3.5 Caching (advanced) The two example files ran fairly quickly. So making a small edit to the text and then re-running the code is not onerous. But if you are producing a large complex document and you make some minor text edits, then you do not want to have to wait for it to build (and calculate results that have not changed) to just check your edits in the resulting .pdf or .html. You can add the cache=TRUE option to a chunk. The results will then be saved (cached) and the code in that chunk not rerun when you render the document again, unless any part of the code in that chunk has changed (e.g. you changed a + to a -). This is very useful, but you have to be careful and understand how it works. To do so, see the Cache section of the R Markdown Cookbook. While the authors do not recommend that you set the chunk option cache = TRUE globally in a document, in our own work we have sometimes done this (as often once the results are finalised we are just editing text). But often we will clear the cache (remove the directory where all the cached results are saved) to re-run the document afresh, to guarantee that all results are reproducible. Certainly practice on individual chunks first and play around with caching if you wish to use it in your own work. To get started, in rmark-pdf-example.Rmd change the cache = FALSE option in the knitr::opts_chunk$set() call to cache = TRUE. Re-running the code will create two directories (as named in knitr::opts_chunk$set()) that will saved the cached versions of chunks and of figures. Re-running the code a second time will then use those cached values (because of the cache=TRUE setting). Then change a piece of code and see which cached files change (by seeing when they were saved). Another workflow is to have one exploratory R Markdown file that includes the analyses and calculations (and saves the results). Then have a write-up R Markdown file that loads those results (but does no time-consuming calculations) and uses them to quickly build tables and figures. If you do have to re-run the analyses then you can do so using the first file, and the results will get updated when you then re-run the second file. This would be a good approach for writing a manuscript or a thesis. 3.6 Further reading As we said, the idea of this module was to give you a simple introduction to using R Markdown. The Definitive Guide to R Markdown is highly recommended (and often where you end up when Googling for how to do something), but you do not need to understand all of it. For example, I know that Pandoc is doing stuff behind the scenes, but have managed to not need to understand much more than that. Chapter 2 goes through the basics, including many things that we glossed over above, such as more details about YAML, the code between the --- at the top of rmark-exercise.Rmd and rmark-pdf-example.Rmd. It also explains how R Markdown, despite the R in its name, can also be used for computer languages other than R, such as Python, SQL, Julia, C, and Fortran. Also see the related R Markdown Cookbook, which provides practical and relatively short examples to show the interesting and useful usage of R Markdown, and supplements the Definitive Guide by making it less daunting to figure out how to achieve certain tasks by providing a range of examples. Also see RStudio introduction if you use RStudio. Finally, there is an R package rticles that includes a suite of custom R Markdown LaTeX formats for various journals and publishers. We havent tried it, but it looks like it provides some useful templates. At Fisheries and Oceans Canada we have built our own R package csasdown to conform to stylistic requirements for certain documents. In practice, once you have something working in the style you like, you can just copy those settings for each new project. It can be handy to keep a readme file of containing commands/options/tips that you use a lot. 3.7 Using RStudio Here are some screenshots for using RStudio (thanks to Joe Watson). To knit a document: Figure 3.5: Screenshot highlighting the knit button in RStudio. and then select the format with (#fig:Screenshot showing knit options in RStudio.)Screenshot showing how to select knit output in RStudio. Be sure to click the correct format, though RStudio does not seem to over-ride the output option set in the .Rmd file (in the YAML at the top). You need to view the .html output in Exercise 1 in a browser, either clicking on the resulting file or doing: Figure 3.6: Screenshot highlighting the open in browser button in RStudio. To run a line of code you can click Ctrl-Enter, and to run a chunk you can click on the green triangle: Figure 3.7: Screenshot showing how to run a chunk in RStudio. and click the green rectangle to run all the preceding chunks also (useful if variables have changed): Figure 3.8: Screenshot showing how to run all code preceding a chunk. 3.8 Using Emacs for R (and everything else) If you have issues with using a mouse too much, or would rather just type commands (especially if you have vision issues) and prefer a less cluttered interface, then Id recommend Emacs Speaks Statistics, which lets you run R from the text editor Emacs. You can also use Emacs for editing all types of files (it highlights code very well), so you only need to learn one editor for all your needs. 3.9 Feedback We value your input! Please take the time to let us know how we might improve these materials in this survey. "],["multi.html", "4 Multivariate analysis: Clustering and Ordination 4.1 Multivariate resemblance 4.2 Cluster Analysis 4.3 Ordination 4.4 Constrained Ordination 4.5 References 4.6 Answer Key", " 4 Multivariate analysis: Clustering and Ordination Imagine you are a public health researcher and you wish to determine how the health of individuals in a population is related to their living and working conditions, and access to health services. You will, of course, have to define health in terms of factors that can be measured for each individual such as: body mass index, mobility, sick days per year, blood pressure, self-reported well-being on a scale of 1 to 5, and so on. One could create a single synthetic health metric to which standard univariate approaches, such as multiple linear regression, could be applied. However, it is certainly plausible that different components of this metric could be related to different aspects of the potential predictors. In addition, some of the components of health may not not responsive to the potential predictors. It may be better to explore how all the different indicator and predictor variables are related. Similarly, we may wish to understand how precipitation, fertilization application rates, slope, soil composition and compaction are related to plant community composition. In this question, we may be tracking the abundance of over a dozen different species and many different sites with different types of soil, precipitation and other environmental factors. Again, we could possibly convert this multiplicity of information so that we have a single metric to describe the plant community (e.g., biodiversity), but such conversion may obscure important differences between species. Again, you can easily see that this is not a situation that ordinary univariate approaches are designed to handle! In this module well be discussing multivariate quantitative methods. Analyses such as linear regression, where we relate a response, y, to a predictor variable, x, are univariate techniques. If we have multiple responses, \\(y_1...y_n\\), and multiple predictors, \\(x_1...x_n\\), we need multivariate approaches. There are many types of multivariate analysis, and we will only describe some of the most common ones. We can think of these different types of analysis as laying at different ends of a spectrum of treating the data as discrete vs continuous, and relying on identifying a reponse variable a priori versus letting the data tell us about explanatory features, i.e., latent variables (Fig. 4.1). Figure 4.1: Types of multivariate analysis 4.1 Multivariate resemblance The starting point for a lot of the classic multivariate methods is to find metrics that describe how similar two individuals, samples, sites or species might be. A natural way to quantify similarity is to list those characters that are shared. For example, what genetic or morphological features are the same or different between two species? A resemblance measure quantifies similarity by adding up in some way the similarities and differences between two things. We can express the shared characters of objects as either: similarity (S), which quantifies the degree of resemblance or dissimilarity (D), which quantifies the degree of difference. 4.1.1 Binary Similarity metrics The simplest similarity metric just tallys the number of shared features. This is called a binary similarity metric, since we are just indicating a yes or no for each characteristic of the two things we wish to compare (Table 4.1). Table 4.1: List of shared attributes for two things Attribute Object 1 Object 2 Similar Attribute 1 1 0 no Attribute 2 0 1 no Attribute 3 0 0 yes Attribute 4 1 1 yes Attribute 5 1 1 yes Attribute 6 0 0 yes Attribute 7 0 1 no Attribute 8 0 0 yes Attribute 9 1 1 yes Attribute 10 1 0 no We could also use a shared lack of features as an indicator of similarity. The simple matching coefficient uses both shared features, and shared absent features, to quantify similarity as \\(S_m=\\frac{a+d}{a+b+c+d}\\), where a refers to the number of shared characteristics of object 1 and object 2, b is the number characteristics that object 1 possesses but object 2 does not and so on (see Table 4.2). Table 4.2: Summary of shared and absent attributes Object 1 Object 2 Present Absent Object 1 Present a b Object 2 Absent c d We can further categorize similarity metrics as symmetric, where we regard both shared presence and shared absence as evidence of similarity, the simple matching coefficient, \\(S_m\\) would be an example of this, or asymmetric, where we regard only shared presence as evidence of similarity (that is, we ignore shared absences). For example, asymmetric measures are most useful in analyzing ecological community data, since it is unlikely to be informative that two temperature zone communities lack tropical data, or that aquatic environments lack terrestrial species. The Jaccard index is an asymmetric binary similarity coefficient calculated as \\(S_J=\\frac{a}{a+b+c}\\), while the quite similar Sørenson index is given as \\(S_S=\\frac{2a}{2a+b+c}\\), and so gives greater weight to shared similarities. Both metrics range from 0 to 1, where a value of 1 indicates complete similarity. Notice that both metrics exclude cell \\(d\\) - the shared absences. Lets try an example. In the 70s, Watson &amp; Carpenter (1974) compared the zooplankton species present in Lake Erie and Lake Ontario. We can use this information to compare how similar the communities in the two lakes were at this time. We can see that they shared a lot of species (Table 4.3)! Table 4.3: Species presence and absence in lake Erie and lake Ontario (data from from Watson &amp; Carpenter 1974) species erie ontario 1 1 1 2 1 1 3 1 1 4 1 1 5 1 1 6 1 1 7 1 1 8 1 1 9 1 1 10 1 1 11 1 1 12 1 1 13 1 1 14 1 1 15 1 1 16 1 1 17 1 1 18 1 1 19 1 0 20 0 1 21 0 0 22 0 0 23 0 0 24 0 0 We can calculate the similarity metrics quite easily using the table() function, where 1 indicates presence and 0 indicates absence. I have stored the information from Table 4.3 in the the dataframe lksp. Im just going grab the presences and absences, since I dont need the species identifiers for my calculation. tlake = table(lksp[, c(&quot;erie&quot;, &quot;ontario&quot;)]) tlake ontario erie 1 0 1 18 1 0 1 4 a = tlake[1, 1] b = tlake[1, 2] c = tlake[2, 1] d = tlake[2, 2] S_j = a/(a + b + c) S_j [1] 0.9 S_s = 2 * a/(2 * a + b + c) S_s [1] 0.9473684 A final note: when a dissimilarity or similarity metric has a finite range, we can simply convert from one to the other. For example, for similarities that range from 1 (identical) to 0 (completely different), dissimilarity would simply be 1-similarity. 4.1.2 Quantitative similarity &amp; dissimilarity metrics While binary similarity metrics are easy to understand, there are a few problems. These metrics work best when we have a small number of characteristics and we have sampled very well (e.g., the zooplankton in Lake Erie and Ontario). However, these metrics are biased against maximum similarity values when we have lots of characteristics (or species) and poor sampling. In addition, we sometimes have more information than just a yes or no which we could use to further characterize similarity. Quantitative similarity and dissimilarity metrics make use of this information. Some examples of quantitative similarity metrics are: Percentage similarity (Renkonen index), Morisitas index of similarity (not dispersion) and Horns index. However, quantitative dissimilarity metrics are perhaps more commonly used. In this case, we often talk about the distance between two things. Distances are of two types, either dissimilarity, converted from analogous similarity indices, or specific distance measures, such as Euclidean distance, which doesnt have a counterpart in any similarity index. There are many, many such metrics, and obviously, you should choose the most accurate and meaningful distance measure for a given application. Legendre &amp; Legendre (2012) offer a key on how to select an appropriate measure for given data and problem (check their Tables 7.4-7.6). If you are uncertain, then choose several distance measures and compare the results. 4.1.2.1 Euclidean Distance Perhaps the mostly commonly used, and easiest to understand distance measure is Euclidean distance. This metric is zero for identical sampling units and has no fixed upper bound. Euclidean distance in multivariate space is derived from our understanding of distance in a Cartesian plane. If we had two species abundances measured in two different samples, we could then plot the abundance of species 1 and species 2 for each sample on a 2D plane, and draw a line between them. This would be our Euclidean distance: the shortest path between the two points (Fig. 4.2). Figure 4.2: Example of a Euclidean distance calculation in a two dimensional space of species abundance We know that to calculate this distance we would just use the Pythagorean theorem as \\(c=\\sqrt{a^2+b^2}\\). To generalize to \\(n\\) species we can say \\(D^E_{jk}=\\sqrt{\\sum^n_{i=1}(X_{ij}-X_{ik})^2}\\), where Euclidean distance between samples j and k, \\(D^E_{jk}\\), is calculated by summing over the distance in abundance of each of n species in the two samples. Lets try an example. Given the species abundances in Table 4.4, we can calculate the squared difference in abundance for each species, and sum that quantity. Table 4.4: Species abundance and distance calculations for two samples sample j sample k \\((X_j-X_k)^2\\) Species 1 19 35 256 Species 2 35 10 625 Species 3 0 0 0 Species 4 35 5 900 Species 5 10 50 1600 Species 6 0 0 0 Species 7 0 3 9 Species 8 0 0 0 Species 9 30 10 400 Species 10 2 0 4 TOTAL 131 113 3794 Exercise 1 Finish the calculation of the Euclidean distance for this data. Of course, R makes this much easier, I can calculate Euclidean distance using the dist() function, after creating a matrix of the two rows of species abundance data from my original eu dataframe. dist(rbind(eu$j, eu$k), method = &quot;euclidean&quot;) There are many other quantitative dissimilarity metrics. For example, Bray Curtis dissimilarity is frequently used by ecologists to quantify differences between samples based on abundance or count data. This measure is usually applied to raw abundance data, but can be applied to relative abundances. It is calculated as: \\(BC_{ij}=1-\\frac{C_{ij}}{S_{i}+S_{j}}\\), where \\(C_{ij}\\) is the sum over the smallest values for only those species in common between both sites, \\(S_{i}\\) and \\(S_{j}\\) are the sum of abundances at the two sites. This metric is directly related to the Sørenson binary similarity metric, and ranges from 0 to 1, with 0 indicating complete similarity. This is not at distance metric, and so, is not appropriate for some types of analysis. 4.1.3 Comparing more than two communities/samples/sites/genes/species What about the situation where we want to compare more than two communities, species, samples or genes? We can simply generate a dissimilarity or similarity matrix, where each pairwise comparison is given. In the species composition matrix below (Table 4.5), sample A and B do not share any species, while sample A and C share all species but differ in abundances (e.g. species 3 = 1 in sample A and 8 in sample C). The calculation of Euclidean distance using the dist() function produces a lower triangular matrix with the pairwise comparisons (Ive included the distance with the sample itself on the diagonal). You might notice that the Euclidean distance values suggest that A and B are the most similar! Euclidean distance puts more weight on differences in species abundances than on difference in species presences. As a result, two samples not sharing any species could appear more similar (with lower Euclidean distance) than two samples which share species that largely differ in their abundances. Table 4.5: Species abundance sample A sample B sample C species 1 0 1 0 species 2 1 0 4 species 3 1 0 8 dist(t(spmatrix), method = &quot;euclidean&quot;, diag = TRUE) A B C A 0.000000 B 1.732051 0.000000 C 7.615773 9.000000 0.000000 There are other disadvantages as well, and in general, there is simply no perfect metric. For example, you may dislike the fact that Euclidean distance also has no upper bound, and so it becomes difficult to understand how similar two things are (i.e., the metric can only be understood in a relative way when comparing many things, Sample A is more similar to sample B than sample C, for example). You could use a Bray-Curtis dissimilarity metric, which is quite easy to interpret, but this metric will also confound differences in species presences and differences in species counts (Greenacre 2017). The best policy is to be aware of the advantages and disadvantages of the metrics you choose, and interpret your analysis in light of this information. 4.1.4 R functions There are a number of functions in R that can be used to calculate similarity and dissimilarity metrics. Since we are usually not just comparing two objects, sites or samples, these functions can help make your calculations much quicker when you are comparing many units. dist() (base R, no package needed) offers a number of quantitative distance measures (e.g. Euclidean,Canberra and Manhattan). The result is the distance matrix which gives the dissimilarity of each pair of objects, sites or samples. the matrix is an object of the class dist in R. vegdist() (library vegan). The default distance used in this function is Bray-Curtis distance, which is considered more suitable for ecological data. dsvdis() (library labdsv) Offers some other indices than vegdist (e.g., ruzicka or Rika), a quantitative analogue of Jaccard, and Roberts. For full comparison of dist, vegdist and dsvdis, see http://ecology.msu.montana.edu/labdsv/R/labs/lab8/lab8.html. daisy() (library cluster) Offers Euclidean, Manhattan and Gower distance. designdist() (library vegan) Allows one to design virtually any distance measure using the formula for their calculation. dist.ldc() (library adespatial) Includes 21 dissimilarity indices described in Legendre &amp; De Cáceres (2013), twelve of which are not readily available in other packages. Note that Bray-Curtis dissimilarity is called percentage difference (method = percentdiff). distance() (library ecodist) Contains seven distance measures, but the function is more for demonstration (for larger matrices, the calculation takes rather long). 4.2 Cluster Analysis When we have a large number of things to compare, an examination of a matrix of similarity or dissimilarity metrics can be tedious or even impossible to do. One way to visualize the similarity among units is to use some form of cluster analysis. Clustering is the grouping of data objects into discrete similarity categories according to a defined similarity or dissimilarity measure. We can contrast clustering, which assumes that units (e.g., sites, communities, species or genes) can be grouped into discrete categories based on similarity, with ordination, which treats the similarity between units as a continuous gradient (well discuss ordination in section 4.3). We can use clustering to do things like discern whether there are one or two or three different communities in three or four or five sampling units. It is used in many fields, such as machine learning, data mining, pattern recognition, image analysis, genomics, systems biology, etc. Machine learning typically regards data clustering as a form of unsupervised learning, or from our figure above (Fig 4.1), as a technique that uses latent variables because we are not guided by a priori ideas of which variables or samples belong in which clusters. 4.2.1 Hierarchical clustering: groups are nested within other groups. Perhaps the most familiar type of clustering is hierarchical. There are two kinds of hierarchical clustering: divisive and agglomerative. In the divisive method, the entire set of units is divided into smaller and smaller groups. The agglomerative method starts with small groups of few units, and groups them into larger and larger clusters, until the entire data set is sampled (Pielou, 1984). Of course, once you have more than two units, you need some way to assess similarity between the clusters. There are a couple of different methods here. Single linkage assigns the similarity between clusters to the most similar units in each cluster. Complete linkage uses the similarity between the most dissimilar units in each cluster, while average linkage averages over all the units in each cluster (Fig. 4.3). Figure 4.3: Different methods of determining similarity between clusters 4.2.1.1 Single Linkage Cluster Analysis Single linkage cluster analysis is one of the easiest to explain. It is hierarchical, agglomerative technique. We start by creating a matrix of similarity (or dissimilarity) indices between the units we want to compare. Then we find the most similar pair of samples, and that will form the 1st cluster. Next, we find either: (a) the second most similar pair of samples or (b) highest similarity between a cluster and a sample, or (c) most similar pair of clusters, whichever is greatest. We then continue this process until until there is one big cluster. Remember that in single linkage, similarity between two clusters = similarity between the two nearest members of the clusters. Or if we are comparing a sample to a cluster, the similarity is defined as the similarity between sample and the nearest member of the cluster. Lets try this with simulated data where we have 5 data units (e.g., sites, species, genes), that each have 5 different quantitative characters (e.g., number of individuals of a given species, morphological features, functions). cls = data.frame(a = c(5, 6, 34, 1, 12), b = c(10, 5, 2, 3, 4), c = c(10, 59, 32, 3, 40), d = c(2, 63, 10, 29, 45), e = c(44, 35, 40, 12, 20)) clsd = dist(t(cls), method = &quot;euclidean&quot;) round(clsd, 0) a b c d b 33 c 60 71 d 76 76 36 e 51 62 48 66 We can see that we construct the cluster diagram by first grouping a and b, followed by c &amp; d, and so on (Fig. 4.4). Figure 4.4: Example of using a dissimilarity matrix to construct a single-linkage cluster diagram. Each panel labelled a), b) and c) shows each successive step in constructing the diagram for items a to e, where numbers indicate the similarity between two items, groups or a group and an item 4.2.1.2 How many clusters? These hierarchical methods just keep going until all objects are included (agglomerative methods), or are each in their own group (divisive methods). However, neither endpoint is very useful. How do we select the number of groups? There are metrics and techniques to make this decision more objective (see the NbClust package). In this brief introduction, well just mention that for hierarchical methods, you can determine the number of groups a given degree of similarity, or set the number of groups and find the degree of similarity that results in that number of groups. Lets try. Well use the cutree() function that works on cluster diagrams produced by the hclust() function (Fig. 4.5). If we set our dissimilarity threshold at 40, we find that there are three groups: a&amp;b, c&amp;d, and e in its own group. Figure 4.5: Cluster diagram produced by the function hclust() with cut-off line at euclidean distance=40 for group membership a b c d e 1 1 2 2 3 4.2.2 Partitional clustering and Fuzzy clustering There are other means of clustering data of course. Partitional clustering is the division of data objects into non-overlapping subsets, such that each data object is in exactly one subset. In one version of this, k-means clustering, each cluster is associated with a centroid (center point), and each data object is assigned to the cluster with the closest centroid. In this method, the number of clusters, K, must be specified in advance. Our method is: Choose the number of K clusters Select K points as the initial centroids Calculate the distance of all items to the K centroids Assign items to closest centroid Recompute the centroid of each cluster Repeat from (3) until clusters assignments are stable K-means has problems when clusters are of differing sizes and densities, or are non-globular shapes. It is also very sensitive to outliers. In contrast to strict (or hard) clustering approaches, fuzzy (or soft) clustering methods allow multiple cluster memberships of the clustered items. Fuzzy clustering is commonly achieved by assigning to each item a weight of belonging to each cluster. Thus, items at the edge of a cluster may be in a cluster to a lesser degree than items at the center of a cluster. Typically, each item has as many coefficients (weights) as there are clusters that sum up for each item to one. 4.2.3 R functions for clustering hclust() (base R, no library needed) calculates hierarchical, agglomerative clusters and has its own plot function. agnes() (library cluster) Contains six agglomerative algorithms, some not included in hclust. diana() divisive hierarchical clustering kmeans() kmeans clustering fanny()(cluster package) fuzzy clustering 4.2.4 Example: Cluster analysis of isotope data Lets try some of these methods on some ecological data. Try to work through the exercise semi-independently. Our first step is to download and import the dataset Dataset_S1.csv from Perkins et al. 2014 (see url below). This data contains 15N and 13C signatures for species from different food webs. Unfortunately, this data is saved in an .xlsx file. To read data into R one of the easiest options is to use the read.csv() function with the argument on a .csv file. These Comma Separated Files are one of your best options for reproducible research. They are human readable and easily handled by almost every type of software. In contrast Microsoft Excel uses a propriatory file format, is not fully backwards compatible, and although widely used, is not human readable. As a result, we need special tools to access this file outside of Microsoft software products Well download the data set using download.file(), and read it using the R library openxlsx (see example below). Once you have successfully read your data file into R, take a look at it! Type iso (or whatever you named your data object) to see if the data file was read in properly. Some datasets will be too large for this approach to be useful (the data will scroll right off the page). In that case, there are a number of commands to look at a portion of the dataset. For example, you could use a command like names(iso) or str(iso). One of the best things to do is plot the imported data. Of course, this is not always possible with very large datasets, but this set should work. library(openxlsx) urlj = &quot;https://doi.org/10.1371/journal.pone.0093281.s001&quot; download.file(urlj, &quot;p.xlsx&quot;, mode = &quot;wb&quot;) iso = read.xlsx(&quot;p.xlsx&quot;) plot(iso$N ~ iso$C, col = as.numeric(as.factor(iso$Food.Chain)), xlim = c(-35, 0), pch = as.numeric(as.factor(iso$Species)), xlab = expression(paste(delta, &quot;13C&quot;)), ylab = expression(paste(delta, &quot;15N&quot;))) legend(&quot;topright&quot;, legend = unique(as.factor(iso$Food.Chain)), pch = 1, col = as.numeric(unique(as.factor(iso$Food.Chain))), bty = &quot;n&quot;, title = &quot;Food chain&quot;) legend(&quot;bottomright&quot;, legend = as.character(unique(as.factor(iso$Species))), pch = as.numeric(unique(as.factor(iso$Species))), bty = &quot;n&quot;, title = &quot;Species&quot;) Figure 4.6: Isotope data from Perkins et al (2014), showing the both the food web membership as given by the dominant plants (colour) and species (symbol) of each sample vs the carbon (d13C) and nitrogen (d15N) isotope signature. Higher tropic levels are found closer to the top of the graph, and food webs with more C4 plant metabolism are found more to the right We are going to use this data set to see if a cluster analysis on 15N and 13C can identify the foodweb. That is we are going to see if the latent variables identified by our clustering method match up to what we think we know about the data. Our first step is to create a dissimilarity matrix, but even before this, we must select that part of the data that we wish to use, just the 15N and 13C data, not the other components of the dataframe for the downloaded data. In addition, our analysis will be affected by the missing data. So lets get remove those rows with missing data right now using the complete.cases() function. The function returns a value of TRUE for every row in a dataframe that no missing values in any column. So niso=iso[complete.cases(mydata),], will be a new data frame with only complete row entries. The function dist() will generate a matrix of the pairwise Euclidean distances between pairs of observations. Now that you have a dissimilarity matrix, you can complete a cluster analysis. The function hclust() will produce a data frame that can be sent to the plot() function to visualize the recommended clustering. The method used to complete the analysis is indicated below the graph. Please adjust the arguments of the function to complete a single linkage analysis (type ?hclust to call the help page for the function and determine the method to do this). str(iso) &#39;data.frame&#39;: 165 obs. of 7 variables: $ Replicate : num 1 2 3 4 5 6 7 8 9 10 ... $ Food.Chain : chr &quot;Wheat&quot; &quot;Wheat&quot; &quot;Wheat&quot; &quot;Wheat&quot; ... $ Species : chr &quot;Plant&quot; &quot;Plant&quot; &quot;Plant&quot; &quot;Plant&quot; ... $ Tissue : chr &quot;Leaf&quot; &quot;Leaf&quot; &quot;Leaf&quot; &quot;Leaf&quot; ... $ Lipid.Extracted: chr &quot;No&quot; &quot;No&quot; &quot;No&quot; &quot;No&quot; ... $ C : num -30.1 -31.7 -30.1 -30.9 -31 ... $ N : num -3.47 -2.68 3.42 1.27 6.2 ... diso &lt;- dist((iso[, c(&quot;C&quot;, &quot;N&quot;)]), method = &quot;euclidean&quot;) p = hclust(diso, method = &quot;single&quot;, ) plot(p, cex = 0.5, main = &quot;&quot;, xlab = &quot;&quot;) Figure 4.7: A dendrogram of isotope data from Perkins et al. (2014), where links between samples and groups at lower height indicate greater similarity. Note the distance between sample 5 and all other samples When you graph your cluster using plot(), you notice that there are many individual measurements, but there are only a few large groups. Does it look like there is an outlier? If so, you may want to remove this point from the data set, and then rerun the analysis. The row numbers are used as labels by default, so this is easy to do (niso=niso[-5,]). When you examine the data set, you noted that there are 4 Food.chain designations. We will use the cutree() function to cut our cluster tree to get the desired number of groups (4), and then save the group numbers to a new column in our original dataframe. For example, iso$clust&lt;- cutree(p,4).We can then plot the data using colours and symbols to see how well our cluster analysis matches the original data classification. niso = iso[complete.cases(iso), ] niso = niso[-5, ] diso &lt;- dist((niso[, c(&quot;C&quot;, &quot;N&quot;)]), method = &quot;euclidean&quot;) p = hclust(diso, method = &quot;single&quot;) niso$clust &lt;- cutree(p, k = 4) # plotting the data with 4 groups identified by the # single-linkage cluster analysis superimposed plot(niso$N ~ niso$C, col = as.numeric(as.factor(niso$clust)), xlim = c(-35, 0), pch = as.numeric(as.factor(niso$Species)), xlab = expression(paste(delta, &quot;13C&quot;)), ylab = expression(paste(delta, &quot;15N&quot;))) legend(&quot;topright&quot;, legend = unique(as.factor(niso$clust)), pch = 1, col = as.numeric(unique(as.factor(niso$clust))), bty = &quot;n&quot;, title = &quot;cluster&quot;) legend(&quot;bottomright&quot;, legend = as.character(unique(as.factor(niso$Species))), pch = as.numeric(unique(as.factor(niso$Species))), bty = &quot;n&quot;, title = &quot;Species&quot;) Figure 4.8: Data from Perkins et al (2014) with grouping from single linkage clustering superimposed with calculated clusters 1 to 4 as well as species categories of Plant, Aphid, Diplazon and Hoverfly with varying symbols It doesnt look like our cluster algorithm is matching up with our Food.chain data categories very well. Wheat- and Nettle-based food chains cannot be distinguished, which makes sense when you consider that both of these plants are terrestrial and use a C3 photosynthesis system. If you are not happy with the success of this clustering algorithm you could try other variants (e.g., complete linkage) and a different number of groups. Exercise 2 Try a non-hierarchical cluster analysis on the same data to see if it works better. Use the kmeans() function, which requires that we select the required number of clusters (4) ahead of time. Once you have your clustering, plot your results. 4.3 Ordination While cluster analysis lets us visualize multivariate data by grouping objects into discrete categories, ordination uses continuous axes to help us accomplish the same task. Physicists grumble if space exceeds four dimensions, while biologists typically grapple with dozens of dimensions. We can order this multivariate data to produce a low dimensional picture (i.e., a graph in 1-3 dimensions). Just like cluster analysis, we will use similarity metrics to accomplish this. Also like cluster analysis, simple ordination is not a statistical test: it is a method of visualizing data. Essentially, we find axes in the data that explain a lot of variation, and rotate so we can use the axes as our dimensions of visual representation (Fig. 4.9). Ive left the scatter about the line, but actually each point would have only a location on the synthetic xy axis. Figure 4.9: Synthetic axis rotation in ordination. We find the axis in 2D that explains most variation, and rotate to give a 1D representation Another way to think about it is that we are going to summarize the raw data, which has many variables, p, by a smaller set of synthetic variables, k (Fig. 4.10). If the ordination is informative, it reduces a large number of original correlated variables to a small number of new uncorrelated variables. But it really is a bit of a balancing act between clarity of representation, ease of understanding, and oversimplification. We will lose information in this data reduction, and if that information is important, then we can make the multivariate data harder to understand! Also note that if the original variables are not correlated, then we wont gain anything with ordination. Figure 4.10: Ordination as data reduction. We summarize data with many observations (n) and variables (p) by a smaller set of derived or synthetic variables (k) There are lots of different ways to perform an ordination, but most methods are based on extracting the eigenvalues of a similarity matrix. The four most commonly used methods are: Principle Component Analysis (PCA), which is the main eigenvector-based method, Correspondence Analysis (CA) which is used used on frequency data, Principle Coordinate Analysis (PCoA) which works on dissimilarity matrices, and Non Metric Multidimensional Scaling (nMDS) which is not an eigenvector method, instead it represents objects along a predetermined number of axes. Legendre &amp; Legendre (2012) provide a nice summary of when you can use each method (Table 4.6). I would like to reiterate that this is a very basic introduction to these methods, and once you are oriented, I suggest you go in search materials authored by experts like Legendre &amp; Legendre (2012), and related materials for R such as Borcard et al. (2011). Table 4.6: Domains of application of ordination methods (adapated from Legendre &amp; Legendre 2012) Method Distance Variables Principal component analysis (PCA) Euclidean Quantitative data, but not species community data Correspondence analysis (CA) \\(\\chi^2\\) Non-negative, quantitiative or binary data (e.g., species frequencies or presence/absence data) Principal coordinate analysis (PCoA) Any Quantitative, semiquantitative, qualitative, or mixed data Nonmetric multidimensional scaling (nMDS) Any Quantitative, semiquantitative, qualitative, or mixed data 4.3.1 Principal Components Analysis (PCA) Principal Components Analysis (PCA) is probably the most widely-used and well-known of the standard multivariate methods. It was invented by Pearson (1901) and Hotelling (1933), and first applied in ecology by Goodall (1954) under the name factor analysis (NB principal factor analysis is also a synonym of PCA). Like most ordination methods, PCA takes a data matrix of n objects by p variables, which may be correlated, and summarizes it by uncorrelated axes (principal components or principal axes) that are linear combinations of the original p variables. The first k components display as much as possible of the variation among objects. PCA uses Euclidean distance calculated from the p variables as the measure of dissimilarity among the n objects, and derives the best possible k-dimensional representation of the Euclidean distances among objects, where \\(k &lt; p\\) . We can think about this spatially. Objects are represented as a cloud of n points in a multidimensional space with an axis for each of the p variables. So the centroid of the points is defined by the mean of each variable, and the variance of each variable is the average squared deviation of its n values around the mean of that variable (i.e., \\(V_i= \\frac{1}{n-1}\\sum_{m=1}^{n}{(X_{im}-\\bar{X_i)}^2}\\)). The degree to which the variables are linearly correlated is given by their covariances \\(C_{ij}=\\frac{1}{n-1}\\sum_{m=1}^n{(X_{im}-\\bar{X_i})(X_{jm}-\\bar{X_j})}\\). The objective of PCA is to rigidly rotate the axes of the p-dimensional space to new positions (principal axes) that have the following properties: they are ordered such that principal axis 1 (or the principal component has the highest variance, axis 2 has the next highest variance etc, and the covariance among each pair of principal axes is zero (the principal axes are uncorrelated) (Fig. 4.11). Figure 4.11: Selecting the synthetic axes in ordination. We have two overlaid plots where those axes labelled X1 and X2 are the original data, and those labelled Y1 and Y2 are our synthetic axis. The two guassian functions suggest how much variation is present on each of the two y synthetic axes. So our steps are to compute the variance-covariance matrix of the data, calculate the eigenvalues of this matrix and then calculate the associated eigenvectors. Then, the jth eigenvalue is the variance of the jth principle component and the sum of all the eigenvalues is the total variance explained. The proportion of variance explained by each component, or synthetic axis, is the eigenvalue for the component divided by the total variance explained, while the rotations are the eigenvectors. Dimensionality reduction is the same as first rotating the data with the eigenvalues to be aligned with the principle components, then display using only the components with the greatest eigenvalues. 4.3.1.1 Example: PCA on the iris data Were going to use a sample dataset in R and the base R version of PCA to start exploring this data analysis technique. Get the iris dataset into memory by typing data(iris). Take a look at this dataset using the head(), str() or summary() functions. For a multivariate data set, you would also like to take a look at the pairwise correlations. Remember that PCA cant help us if the variables are not correlated. Lets use the pairs() function to do this data(&quot;iris&quot;) str(iris) &#39;data.frame&#39;: 150 obs. of 5 variables: $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... summary(iris[1:4]) Sepal.Length Sepal.Width Petal.Length Petal.Width Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 Median :5.800 Median :3.000 Median :4.350 Median :1.300 Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 pairs(iris[1:4], main = &quot;Iris Data&quot;, pch = as.numeric(iris$Species) + 14, col = as.numeric(iris$Species) + 1) Figure 4.12: Correlation matrix for characteristics of each individual the iris data. Colours give species identity The colours let us see the data for each species, the graph is all the pairwise plots of each pair of the 4 variables (Fig. 4.12). Do you see any correlations? If there seem to be some correlations we might use PCA to reduce the 4 dimensional variable space to 2 or 3 dimensions. Lets rush right in and use the prcomp() function to run a PCA on the numerical data in the iris dataframe. Save the output from the function to a new variable name so you can look at it when you type that name. The str() function will show you what the output object includes. If you use the summary() function, R will tell you what proportion of the total variance is explained by each synthetic axis. pca &lt;- prcomp(iris[, 1:4]) summary(pca) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 2.0563 0.49262 0.2797 0.15439 Proportion of Variance 0.9246 0.05307 0.0171 0.00521 Cumulative Proportion 0.9246 0.97769 0.9948 1.00000 4.3.1.2 Standardize your data There is a problem though, lets examine the variance in the raw data. Use the apply() function to quickly calculate the variance in each of the numeric columns of the data as apply(iris[,1:3], 1, var). What do you see? Are the variances of each the columns comparable? apply(iris[, 1:4], 2, var) Sepal.Length Sepal.Width Petal.Length Petal.Width 0.6856935 0.1899794 3.1162779 0.5810063 Using covariances among variables only makes sense if they are measured in the same units, and even then, variables with high variances will dominate the principal components. These problems are generally avoided by standardizing each variable to unit variance and zero mean as \\(X_{im}^{&#39;}=\\frac{x_{im}-\\bar{X_i}}{sd_i}\\) where sd is the standard deviation of variable i. After standardizaton, the variance of each variable is 1 and the covariances of the standardized variables are correlations. If you look at the help menu, the notes for the use of prcomp() STRONGLY recommend standardizing the data. To do this there is a built in option. We just need to set scale=TRUE. Lets try again with data standardization. Save your new PCA output to a different name. Take a look at the summary. p &lt;- prcomp(iris[, 1:4], scale = TRUE) summary(p) Importance of components: PC1 PC2 PC3 PC4 Standard deviation 1.7084 0.9560 0.38309 0.14393 Proportion of Variance 0.7296 0.2285 0.03669 0.00518 Cumulative Proportion 0.7296 0.9581 0.99482 1.00000 We now have less variance explained by axis 1. This makes sense, because, as we will see in a moment, axis 1 is strongly influenced by petal length, but in the unstandardized data, petal length had larger variance than anything else. 4.3.1.3 Choose your axes Now we need to determine how many axes to use to interpret our analysis. For 4 variables it is easy enough to just look that the amount of variance, as we just did. For larger numbers of variables a plot can be useful. The screeplot() function will output the variance (called inertia) explained by each of the principle component axes, and you can make a decision based on that. screeplot(p, type = (&quot;lines&quot;), main = &quot;&quot;, pch = 16, cex = 1) Figure 4.13: Screeplot: a display of the variance explained by each of the principal components (i.e., synthetic axes). The variance explained decreases for each subsequent axis An ideal curve should be steep, then bend at an elbow  this is your cutting-off point  and after that flattens out. To deal with a not-so-ideal scree plot curve you can apply the Kaiser rule: pick PCs with eigenvalues of at least 1. Or you can select using the proportion of variance where the PCs should be able to describe at least 80% of the variance. It looks like synthetic axes 1 &amp; 2 explain most of the variation. This is, of course, always true, but in this case only a very small proportion of the variance is explained by axes 3 &amp; 4, so we dont need to consider them any further. So lets plot axes 1 &amp; 2. 4.3.1.4 Plot your ordination A PCA plot displays our samples in terms of their position (or scores) on the new axes. We can add information about how much variation each axis explains, and colour our points to match species identity. In this 2D representation of 4 dimensional space, it looks like species I. versicolor and I. viriginica are the most similar (Fig. 4.14). pvar = round(summary(p)$importance[2, 1:2], 2) plot(p$x[, 1:2], col = as.numeric(iris$Species) + 1, ylim = c(-3, 3), cex = 1, pch = as.numeric(iris$Species) + 14, xlab = paste0(&quot;PC1 (&quot;, pvar[1] * 100, &quot;%)&quot;), ylab = paste0(&quot;PC2 (&quot;, pvar[2] * 100, &quot;%)&quot;)) legend(&quot;topright&quot;, legend = unique(iris$Species), pch = as.numeric(unique(iris$Species)) + 14, col = c(2, 3, 4), bty = &quot;n&quot;) Figure 4.14: PCA plot for the iris data where distance is related to the similarlity between each sample. Species identity is indicated by colour and symbol shape Ive plotted the amount of variance explained by each axis, but remember that the 1st axis always explains the most variance. Your interpretation of the ordination plot hould reflect this fact We can also plot information about influence the various characteristics are having on each of the axes. The eigenvectors used for the rotation give us this information. So lets just print that out. Table 4.7: Eigenvectors for each variable and synthetic axis PC1 PC2 PC3 PC4 Sepal.Length 0.52 -0.38 0.72 0.26 Sepal.Width -0.27 -0.92 -0.24 -0.12 Petal.Length 0.58 -0.02 -0.14 -0.80 Petal.Width 0.56 -0.07 -0.63 0.52 We can see that a lot of information is coming from the petal variables for PC1, but less from the sepal variables (Table 4.7). We can plot this out to show how strongly each variable affects each principle component (or synthetic axis). plot(NA, ylim = c(-5, 4), xlim = c(-5, 4), xlab = paste0(&quot;PC1 (&quot;, pvar[1] * 100, &quot;%)&quot;), ylab = paste0(&quot;PC2 (&quot;, pvar[2] * 100, &quot;%)&quot;)) abline(v = 0, col = &quot;grey90&quot;) abline(h = 0, col = &quot;grey90&quot;) # Get co-ordinates of variables (loadings), and multiply by # 10 l.x &lt;- p$rotation[, 1] * 4 l.y &lt;- p$rotation[, 2] * 4 # Draw arrows arrows(x0 = 0, x1 = l.x, y0 = 0, y1 = l.y, col = 5, length = 0.15, lwd = 1.5) # Label position l.pos &lt;- l.y # Create a vector of y axis coordinates lo &lt;- which(l.y &lt; 0) # Get the variables on the bottom half of the plot hi &lt;- which(l.y &gt; 0) # Get variables on the top half # Replace values in the vector l.pos &lt;- replace(l.pos, lo, &quot;1&quot;) l.pos &lt;- replace(l.pos, hi, &quot;3&quot;) l.pos[4] &lt;- &quot;3&quot; l.x[3:4] &lt;- l.x[3:4] + 0.75 # Variable labels text(l.x, l.y, labels = row.names(p$rotation), col = 5, pos = l.pos, cex = 1) Figure 4.15: Contribution of each of the characteristics of the samples to the first two principal component axes We can see that petal width and length are aligned along the PC1 axis, while PC2 explains more variation in sepal width (Fig. 4.15). That is, petal length and petal width variables are the most important contributors to the first PC. Sepal width variable is the most important contributor to the second PC. To interpret the variable plot remember that positively correlated variables are grouped close together (e.g., petal length and width). Variables with about a 90 angle are probably not correlated (sepal width is not correlated with the other variables), while negatively correlated variables are positioned on opposite sides of the plot origin (~180 angle; opposed quadrants). However, the direction of the axes is arbitrary! The distance between variables and the origin measures the contribution of the variables to the ordination. A shorter arrow indicates its less importance for the ordination. Variables that are away from the origin are well represented. Avoid the mistake of interpreting the relationships among variables based on the proximities of the apices (tips) of the vector arrows instead of their angles. Another way to portray this information is to create a biplot which, in addition to the coordinates of our samples on the synthetic axes PC1 and PC2, also provides information about how the variables align along the synthetic axes. (Fig. 4.16). According to the plot, I. versicolor and I. virginica have similar petal length and width, but note that the axis direction is arbitrary and coud not be interpreted as suggesting that these two species have longer petal widths than I.setosa. I have used an arbitrary scaling to display the variable loadings on each axis. Some of the R packages will use a specific scaling that will emphasize particular parts of the plot, either preserving the Euclidean distances between samples or the correlations/covariances between variables (e.g., the vegan package can do this for you, see section 4.4.5). plot(p$x[, 1:2], pch = as.numeric(iris$Species) + 14, col = as.numeric(iris$Species) + 1, ylim = c(-5, 4), xlim = c(-4, 4), cex = 1, xlab = paste(&quot;PC1 (&quot;, pvar[1] * 100, &quot;%)&quot;), ylab = paste(&quot;PC2 (&quot;, pvar[2] * 100, &quot;%)&quot;)) legend(&quot;topleft&quot;, legend = unique(iris$Species), pch = as.numeric(unique(iris$Species)) + 14, col = c(2, 3, 4), bty = &quot;n&quot;) # Get co-ordinates of variables (loadings), and multiply by # a constant l.x &lt;- p$rotation[, 1] * 4 l.y &lt;- p$rotation[, 2] * 4 # Draw arrows arrows(x0 = 0, x1 = l.x, y0 = 0, y1 = l.y, col = 5, length = 0.15, lwd = 1.5) # Label position l.pos &lt;- l.y # Create a vector of y axis coordinates lo &lt;- which(l.y &lt; 0) # Get the variables on the bottom half of the plot hi &lt;- which(l.y &gt; 0) # Get variables on the top half # Replace values in the vector l.pos &lt;- replace(l.pos, lo, &quot;1&quot;) l.pos &lt;- replace(l.pos, hi, &quot;3&quot;) l.pos[4] &lt;- &quot;3&quot; l.x[3:4] &lt;- l.x[3:4] + 0.75 # Variable labels text(l.x, l.y, labels = row.names(p$rotation), col = 5, pos = l.pos, cex = 1) Figure 4.16: PCA biplot for the iris data. This plot combines the vectors that should how the various characteristics of the samples are aligned with the synthetic axes, and the similarity of the the samples as given by their proximity in the ordination space There are some final points to note regarding interpretation. Principal components analysis assumes the relationships among variables are linear, so that the cloud of points in p-dimensional space has linear dimensions that can be effectively summarized by the principal axes. If the structure in the data is nonlinear (i.e., the cloud of points twists and curves its way through p-dimensional space), the principal axes will not be an efficient and informative summary of the data. For example, in community ecology, we might use PCA to summarize variables whose relationships are approximately linear or at least monotonic (e.g., soil properties might be used to extract a few components that summarize main dimensions of soil variation). However, in general PCA is generally not useful for ordinating community data because relationships among species and environmental factors are highly nonlinear. This nonlinearity can lead to characteristic artifacts, where, for example, community trends along environmental gradients appear as horseshoes in PCA ordinations because of low species density at opposite extremes of an environmental gradiant appear relatively close together in ordination space (i.e., arch or horseshoe effect). 4.3.1.5 R functions for PCA prcomp() (base R, no library needed) rda() (vegan) PCA() (FactoMineR library) dudi.pca() (ade4) acp() (amap) 4.3.2 Principle Coordinates Analysis (PCoA) The PCoA method may be used with all types of distance descriptors, and so might be able to avoid some problems of PCA. Although, a PCoA computed on a Euclidean distance matrix gives the same results as a PCA conducted on the original data 4.3.2.1 R functions for PCoA cmdscale() (base R, no package needed) smacofSym() (library smacof) pco()(ecodist) pco()(labdsv) pcoa()(ape) dudi.pco()(ade4) 4.3.3 Nonmetric Multidimensional Scaling (nMDS) Like PCoA, the method of nonmetric multidimensional scaling (nMDS), produces ordinations of objects from any resemblance matrix. However, nMDS compresses the distances in a non-linear way and its algorithm is computer-intensive, requiring more computing time than PCoA. PCoA is faster for large distance matrices. This ordination method does not to preserve the exact dissimilarities among objects in an ordination plot, instead it represents as well as possible the ordering relationships among objects in a small and specified number of axes. Like PCoA, nMDS can produce ordinations of objects from any dissimilarity matrix. The method can also cope with missing values, as long as there are enough measures left to position each object with respect to a few others. nMDS is not an eigenvalue technique, and it does not maximize the variability associated with individual axes of the ordination. In this computational method the steps are: Specify the desired number m of axes (dimensions) of the ordination. Construct an initial configuration of the objects in the m dimensions, to be used as a starting point of an iterative adjustment process. (tricky: the end result may depend on this. A PCoA ordination may be a good start. Otherwise, try many independent runs with random initial configurations. The package vegan has a function that does this for you) Try to position the objects in the requested number of dimensions in such a way as to minimize how far the dissimilarities in the reduced-space configuration are from being monotonic to the original dissimilarities in the association matrix The adjustment goes on until the difference between the observed and modelled dissimilarity matrices (called stress), can cannot be lowered any further, or until it reaches a predetermined low value (tolerated lack-of-fit). Most nMDS programs rotate the final solution using PCA, for easier interpretation. We can use a Shephard plot to get information about the distortion of representation. A Shepard diagram compares how far apart your data points are before and after you transform them (ie: goodness-of-fit) as a scatter plot. On the x-axis, we plot the original distances. On the y-axis, we plot the distances output by a dimension reduction algorithm. A really accurate dimension reduction will produce a straight line. However since information is almost always lost during data reduction, at least on real, high-dimension data, so Shepard diagrams rarely look this straight. Lets try this for the iris data. We can evaluate the quality of the nMDS solution by checking the Shephard plot. library(vegan) nMDS &lt;- metaMDS(iris[, -5], distance = &quot;bray&quot;, k = 2, trace = FALSE) par(mfrow = c(1, 2)) # Plot the stressplot stressplot(nMDS, pch = 16, l.col = NA, las = 1) # Plot the ordination plot(nMDS$points, pch = as.numeric(iris$Species) + 14, col = as.numeric(iris$Species) + 1, ylim = c(-0.3, 0.3), xlim = c(-0.6, 0.75), cex = 0.7, xlab = &quot;nMDS1&quot;, ylab = &quot;nMDS2&quot;) legend(&quot;bottomleft&quot;, legend = unique(iris$Species), pch = as.numeric(unique(iris$Species)) + 14, col = c(2, 3, 4), bty = &quot;n&quot;, cex = 1) # Get co-ordinates of variables, and multiply by scale l.x2 &lt;- nMDS$species[-1, 1] * 1.25 l.y2 &lt;- nMDS$species[-1, 2] * 1.25 # Draw arrows arrows(x0 = 0, x1 = l.x2, y0 = 0, y1 = l.y2, col = 5, length = 0.1, lwd = 2) # Variable labels l.x1 &lt;- nMDS$species[, 1] * 1.25 l.y1 &lt;- nMDS$species[, 2] * 1.25 text(l.x1, l.y1, labels = row.names(nMDS$species), col = 1, pos = 3, cex = 0.8) Figure 4.17: Shepards plot and ordination plot of an nMDS ordination using 2 axes to represent the varespec data in the vegan package In addition to the original dissimilarity and ordination distance, the plot displays two correlation-like statistics on the goodness of fit. The nonmetric fit is given by \\(R^2\\), while the linear fit is the squared correlation between fitted values and ordination distances (Fig. 4.17 ). There is some deformation here, but in general the representation is really quite good. nMDS often achieves a less deformed representation of the dissimilarity relationships among objects than a PCoA in the same number of dimensions. But nMDS is a computer-intensive iterative technique exposed to the risk of suboptimum solutions. In comparison, PCoA finds the optimal solution by eigenvalue decomposition. 4.3.3.1 R functions for nMDS metaMDS() (vegan package) isoMDS( ) (MASS) 4.3.4 Example: nMDS and PCoA We are going to use the vegan package, and some built-in data with it to run the nMDS and PcOA. Varespec is a data frame of observations of 44 species of lichen at 24 sites. Well calculate both an nMDS and a PCoA using the (cmdscale() function) on the Bray-Curtis distance matrix of these data. In each case, we will specify that we want 2 dimensions as our output. The vegan wrapper for nMDS ordination (metaMDS()) standardizes our community data by default, using a Wisconsin transform. This is a method of double standardization that avoids negative values in the transformed data, and is completed by first standardizing species data using the maxima, and then the site by totals. We will have to apply this standardization manually for the PCoA analysis, and then calculate the dissimilarity matrix. library(vegan) data(varespec) nMDS &lt;- metaMDS(varespec, trymax = 100, distance = &quot;bray&quot;, k = 2, trace = FALSE) svarespec = wisconsin(varespec) disimvar = vegdist(svarespec, method = &quot;bray&quot;) PCoA &lt;- cmdscale(disimvar, k = 2, eig = T, add = T) str(PCoA) List of 5 $ points: num [1:24, 1:2] -0.118 -0.103 0.182 0.486 0.106 ... ..- attr(*, &quot;dimnames&quot;)=List of 2 .. ..$ : chr [1:24] &quot;18&quot; &quot;15&quot; &quot;24&quot; &quot;27&quot; ... .. ..$ : NULL $ eig : num [1:24] 1.208 0.832 0.743 0.491 0.461 ... $ x : NULL $ ac : num 0.178 $ GOF : num [1:2] 0.298 0.298 Well plot the PCoA and the nMDS side by side to see if they differ, using the par(mfrow()) functions. In this case, our species are the variables and our sites are the objects of our attention. For the nMDS it does not make sense to plot the species as vectors, as that implies directionality or increasing abundance, and there is no reason to assume that the abundance will increase linearly in a given direction across the nMDS plot. For this package, the species score is calculated as the weighted average of the site scores, where the weights are the abundance of that species at each site. If we look at the object PCoA we see the new 2D coordinates for each site (points). These are raw scores, but we can weight them using the in the same way as the nMDS using the wascores() function. We can plot the results as plot(PCoA$points). But, since this will be a crowded plot, lets use a vegan package function ordipointlabel() for both instead, which will use an optimization routine to produce the best plot par(mfrow = c(1, 2)) ordipointlabel(nMDS, pch = c(NA, NA), cex = c(1.2, 0.6), xlim = c(-0.6, 1.2)) abline(h = 0, col = &quot;grey&quot;) abline(v = 0, col = &quot;grey&quot;) PCoA$species &lt;- wascores(PCoA$points, varespec, expand = TRUE) ordipointlabel(PCoA, pch = c(NA, NA), cex = c(1.2, 0.6), xlab = &quot;PCoA1&quot;, ylab = &quot;PCoA2&quot;, xlim = c(-0.6, 1), ylim = c(-0.5, 0.6)) abline(h = 0, col = &quot;grey&quot;) abline(v = 0, col = &quot;grey&quot;) Figure 4.18: Biplots of the lichen data for nMDS and PCoA ordinations Exercise 3 Provide an interpretation of these plots 4.4 Constrained Ordination The patterns we see in the previous exercise are created by differences among thie sites in the relative abundances in species aligned with the major direction of difference (e.g. Betupube). However, biologists often go further than this, and attempt to explain the differences in characteristics (in this case, species abundances) that drives data object differences (in this case, sampling sites) by superimposing relationships of environmental variation associated with the sites in a regression type exercise. This is constrained or canonical ordination Simple (or unconstrained) ordination is done on one data set, and we try to explain/understand the data by examining a graph constructed with a reduced set of orthogonal axes. The ordination is not influenced by external variables; these may only be considered after we construct the ordination. There is no hypotheses or hypothesis testing: this is an exploratory technique only. In unconstrained ordination axes correspond to the directions of the greatest variability within the data set. In contrast, constrained ordination associates two or more data sets in the ordination and explicitly explores the relationships between two matrices: a response matrix and an explanatory matrix (Fig 4.19). Both matrices are used in the production of the ordination. In this method, we can formally test statistical hypotheses about the significance of these relationships. The constrained ordination axes correspond to the directions of the greatest variability of the data set that can be explained by the environmental variables Figure 4.19: Diagram showing the differences between uncontrained ordination, multiple regression and constrained ordination (adapted from Legendre &amp; Legendre 2012) There are two major methods of constrained commonly used by ecologists. Both combine multiple regression with a standard ordination: Redundancy analysis (RDA) and Canonical correspondence analysis (CCA). RDA preserves the Euclidean distances among objects in matrix, which contains values of Y fitted by regression to the explanatory variables X. CCA preserves the \\(\\chi^2\\) distance (as in correspondence analysis), instead of the Euclidean distance. The calculations are a bit more complex since the matrix contains fitted values obtained by weighted linear regression of matrix of correspondence analysis on the explanatory variables X 4.4.1 Redundancy Analysis Redundancy analysis was created by Rao (1964) and also independently by Wollenberg (1977).The method seeks, in successive order, linear combinations of the explanatory variables that best explain the variation of the response data. The axes are defined in the space of the explanatory variables that are orthogonal to one another. RDA is therefore a constrained ordination procedure. The difference with unconstrained ordination is important: the matrix of explanatory variables conditions the weights (eigenvalues), and the directions of the ordination axes. In RDA, one can truly say that the axes explain or model (in the statistical sense) the variation of the dependent matrix. Furthermore, a global hypothesis (H0) of absence of linear relationship between Y and X can be tested in RDA; this is not the case in PCA. As in PCA, the variables in Y should be standardized if they are not dimensionally homogeneous (e.g., if they are a mixture of temperatures, concentrations, and pH values), or transformations applicable to community composition data applied if data is species abundance or presence/absence (Legendre &amp; Legendre 2012). As in multiple regression analysis, matrix X can contain explanatory variables of different mathematical types: quantitative, multistate qualitative (e.g. factors), or binary variables. If present, collinearity among the X variables should be reduced. In cases where several correlated explanatory variables are present, without clear a priori reasons to eliminate one or the other, one can examine the variance inflation factors (VIF) which measure how much the variance of the regression or canonical coefficients is inflated by the presence of correlations among explanatory variables. As a rule of thumb, ter Braak (1988) recommends that variables that have a VIF larger than 20 be removed from the analysis. (Note: always remove the variables one at a time and recompute the analysis, since the VIF of every variable depends on all the others!) So our steps for an RDA are a combination of the things we would do for a multiple linear regression, and the things we would do for an ordination: Multivariate linear regression of Y on X: equivalent to regressing each Y response variable on X to calculate vectors of fitted values followed by stacking these column vectors side by side into a new matrix Test regression for significance using a permutation test If significant, compute a PCA on matrix of fitted values to get the canonical eigenvalues and eigenvectors We may also compute the residual values of the multiple regressions and do a PCA on these values 4.4.1.1 R functions for RDA BEWARE: many things are called rda in R that have nothing to do with ordination!! rda (vegan package)- this function calculates RDA if a matrix of environmental variables is supplied (if not, it calculates PCA). Two types of syntax are available: matrix syntax - rda (Y, X, W), where Y is the response matrix (species composition), X is the explanatory matrix (environmental factors) and W is the matrix of covariables, or formula syntax (e.g., RDA = rda (Y ~ var1 + factorA + var2*var3 + Condition (var4), data = XW, where var1 is quantitative, factorA is categorical, there is an interaction term between var2 and var3, while var4 is used as covariable and hence partialled out). We should mention that there are several closely related forms of RDA analysis: tb-RDA (transformation-based RDA, Legendre &amp; Gallagher 2001): transform the species data with vegans decostand(), then use the transformed data matrix as input matrix Y in RDA. db-RDA (distance-based RDA, Legendre &amp; Anderson 1999): compute PCoA from a pre-computed dissimilarity matrix D, then use the principal coordinates as input matrix Y in RDA. db-RDA can also be computed directly by function dbrda() in vegan. For a Euclidean matrix, the result is the same as if PCoA had been computed, followed by regular RDA. Function dbrda() can directly handle non-Euclidean dissimilarity matrices, but beware of the results if the matrix is strongly non-Euclidean and make certain this is what you want. Vegan has three methods of constrained ordination: constrained or canonical correspondence analysis (cca()), redundancy analysis (rda()), and distance-based redundancy analysis (capscale()). All these functions can have a conditioning term that is partialled out. All functions accept similar commands and can be used in the same way. The preferred way is to use formula interface, where the left hand side gives the community data frame and the right hand side lists the constraining variables: 4.4.2 Example: Constrained ordination The rda approach is best for continuous data. Lets use the lichen data in the vegan package again, with the related environmental variables regarding soil chemistry (varechem). library(vegan) data(&quot;varespec&quot;) data(&quot;varechem&quot;) Remember that the environmental data can be inspected using commands str() which shows the structure of any object in a compact form, and by asking for a summary of a data frame (e.g., str(varechem) or summary(varechem)). We can also plot the data to get a sense of how correlated the various predictors might be but it is very large for this much data (Fig. 4.20) plot(varechem, gap = 0, panel = panel.smooth, cex.lab = 1.2, lwd = 2, pch = 16, cex = 0.75, col = rgb(0.5, 0.5, 0.7, 0.8)) Figure 4.20: Pairwise correlations between environmental variables in the varechem dataset, where variable name is listed on the diagonal. Chemical measurements are denoted by their chemical element names. Baresoil gives the estimated cover of bare soil, Humdepth provides the thickness of the humus layer and pH. We will use a formula interface to constrain the ordination. The shortcut ~. indicates that we want to use all the variables in the environmental dataset, and you can see this in the output. We will of course remember to standardize and scale our data. The output shows the eigenvalues for both the constrained and unconstrained ordination axes. In particular, see how the variance (inertia) and rank (number of axes) are decomposed in constrained ordination. stvarespec = as.data.frame(wisconsin(varespec)) scvarechem = as.data.frame(scale(varechem)) constr &lt;- vegan::rda(stvarespec ~ ., data = scvarechem) constr Call: rda(formula = stvarespec ~ N + P + K + Ca + Mg + S + Al + Fe + Mn + Zn + Mo + Baresoil + Humdepth + pH, data = scvarechem) Inertia Proportion Rank Total 0.05084 1.00000 Constrained 0.03612 0.71057 14 Unconstrained 0.01471 0.28943 9 Inertia is variance Eigenvalues for constrained axes: RDA1 RDA2 RDA3 RDA4 RDA5 RDA6 RDA7 0.008120 0.005642 0.004808 0.003525 0.002956 0.002592 0.001953 RDA8 RDA9 RDA10 RDA11 RDA12 RDA13 RDA14 0.001665 0.001265 0.001129 0.000964 0.000627 0.000507 0.000373 Eigenvalues for unconstrained axes: PC1 PC2 PC3 PC4 PC5 PC6 PC7 0.004051 0.002184 0.002003 0.001853 0.001247 0.001101 0.001022 PC8 PC9 0.000722 0.000532 However, it is not recommended to perform a constrained ordination with all the environmental variables you happen to have: adding a large number of constraints means eventually you end up with solution similar to the unconstrained ordination. Moreover, collinearity in the explanatory variables will cause instability in the estimation of regression coefficients (see Fig 4.20). Instead, lets use the formula interface to select particular constraining variables ord3 = vegan::rda(stvarespec ~ N + K + Al, data = scvarechem) ord3 Call: rda(formula = stvarespec ~ N + K + Al, data = scvarechem) Inertia Proportion Rank Total 0.05084 1.00000 Constrained 0.01171 0.23041 3 Unconstrained 0.03913 0.76959 20 Inertia is variance Eigenvalues for constrained axes: RDA1 RDA2 RDA3 0.006722 0.003232 0.001761 Eigenvalues for unconstrained axes: PC1 PC2 PC3 PC4 PC5 PC6 PC7 0.007246 0.004097 0.003737 0.003515 0.003305 0.002474 0.002369 PC8 0.001785 (Showing 8 of 20 unconstrained eigenvalues) Examine the output. How did we do? Unsurprisingly, we have not explained as much variance as the full model. In addition, the amount of variance (inertia) explained by the constrained axes is less than the unconstrained axes. Maybe we shouldnt be using this analysis at all. Of course, some of the variables in the full model may or may not be important. We need a significance test to try and sort this out. 4.4.3 Signficance tests for constrained ordination The vegan package provides permutation tests for the significance of constraints. The test mimics a standard ANOVA function, and the default test analyses all constraints simultaneously. anova(ord3) Permutation test for rda under reduced model Permutation: free Number of permutations: 999 Model: rda(formula = stvarespec ~ N + K + Al, data = scvarechem) Df Variance F Pr(&gt;F) Model 3 0.011714 1.996 0.001 *** Residual 20 0.039125 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So the results suggest that our model explains a significant amount of variation in the data. We can also perform significance tests for each variable: anova(ord3, by = &quot;term&quot;, permutations = 199) Permutation test for rda under reduced model Terms added sequentially (first to last) Permutation: free Number of permutations: 199 Model: rda(formula = stvarespec ~ N + K + Al, data = scvarechem) Df Variance F Pr(&gt;F) N 1 0.002922 1.4937 0.085 . K 1 0.003120 1.5948 0.045 * Al 1 0.005672 2.8994 0.005 ** Residual 20 0.039125 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The analysis suggests that maybe we havent chosen the best set of predictors. For example, N does not explain a significant amount of variation. This test is sequential: the terms are analyzed in the order they happen to be in the model, so you may get different results depending on how the model is specified. See if you can detect this, by rearranging your model terms in the call to ordination. NB It is also possible to analyze the significance of marginal effects (Type III effects) by specifying that option (by=mar) or analyse the significance of each axis, again by specifying that option in the anova call (i.e.,by=axis). 4.4.4 Forward Selection of explanatory variables You may wonder if you have selected the correct variables! We can use the ordiR2step() function to perform forward selection (and backwards selection or both directions) on a null model. In automatic model building we usually need two extreme models: the smallest and the largest model considered. First we specify a full model with all the environmental variables, and then a minimal model with intercept only, where both are defined using a formula so that terms can be added or removed from the model. Then we allow an automated model selection function to move between these extremes, trying to minimize a metric of model fit, such as \\(R^2\\). We will then use ordiR2step() to find an optimal model based on both permutation and adjusted \\(R^2\\) values. If you switch the trace=FALSE option to trace=TRUE you can watch the function doing its work. mfull &lt;- vegan::rda(stvarespec ~ ., data = scvarechem) m0 &lt;- vegan::rda(stvarespec ~ 1, data = scvarechem) optm &lt;- ordiR2step(m0, scope = formula(mfull), trace = FALSE) optm$anova R2.adj Df AIC F Pr(&gt;F) + Fe 0.063299 1 -71.156 2.5543 0.004 ** + P 0.103626 1 -71.329 1.9898 0.008 ** + Mn 0.136721 1 -71.402 1.8051 0.022 * &lt;All variables&gt; 0.260356 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Exercise 4 Examine the significance test. How does this automatically selected model differ from the full model and your manually selected version? Next check the variance (i.e., inertia). If the variance explained by the constrained model is not much higher than your unconstrained variance, then it may be that the rda is not really needed. This certainly seems to be the case, but lets press on regardless! Call: rda(formula = stvarespec ~ Fe + P + Mn, data = scvarechem) Inertia Proportion Rank Total 0.05084 1.00000 Constrained 0.01268 0.24932 3 Unconstrained 0.03816 0.75068 20 Inertia is variance Eigenvalues for constrained axes: RDA1 RDA2 RDA3 0.006070 0.003607 0.002998 Eigenvalues for unconstrained axes: PC1 PC2 PC3 PC4 PC5 PC6 PC7 0.006435 0.004626 0.003763 0.003301 0.003204 0.002592 0.002003 PC8 0.001852 (Showing 8 of 20 unconstrained eigenvalues) Contrary to what one sometimes reads, variables with high VIFs should generally not be manually removed before the application of a procedure of selection of variables. Indeed, two highly correlated variables that are both strong predictors of one or several of the response variables Y may both contribute significantly, in complementary manners, to the linear model of these response variables. A variable selection procedure is the appropriate way of determining if that is the case. However, lets see what happened with collinearity. VIFs can be computed in vegan after RDA or CCA. The algorithm in the vif.cca() function allows users to include factors in the RDA; the function will compute VIF after breaking down each factor into dummy variables. If X contains quantitative variables only, the vif.cca() function produces the same result as the normal calculation for quantitative variables. vif.cca(mfull) N P K Ca Mg S Al 1.884049 6.184742 11.746873 9.825650 9.595203 18.460717 20.331905 Fe Mn Zn Mo Baresoil Humdepth pH 8.849137 5.168278 7.755110 4.575108 2.213443 5.638807 6.910118 vif.cca(optm) Fe P Mn 1.259334 1.431022 1.738208 Some VIF values are above 10 or even 20 in the full model, so that a reduction of the number of explanatory variables is justified. The VIF values in the reduced model are quite low. Finally, lets examine the adjusted \\(R^2\\) of the full and selected models. round(RsquareAdj(mfull)$adj.r.squared, 2) [1] 0.26 round(RsquareAdj(optm)$adj.r.squared, 2) [1] 0.14 The adjusted \\(R^2\\) for the reduced model is pretty bad, but so is the full model! It will definitely be easier to interpret the reduced model Now! We are all set to produce some ordination plots 4.4.5 Triplots: Graphing a constrained ordination If the constrained ordination is significant, we go on to display the results graphically. All ordination results of vegan can be displayed with a plot command. For a constrained ordination we will want a triplot, which has three different things plotted: the data objects, the response variables and the explanatory variables. For the varespec and varechem data these will be sites, species, and environmental predictors respectively. Plotting an biplot or triplot is not as simple as it sounds, as I mentioned above, one can choose various scaling conventions that will dramatically change the appearance and interpretation of the plot. One of the editors of the vegan package, Gavin Simpson, has more to say about this here. Essentially, there can be no scaling (none), or either the data object (site) or characteristic (species) scores are scaled by eigenvalues, and the other set of scores is left unscaled, or both scores are scaled symmetrically by square root of eigenvalues (symmetric). The general advice is to use scaling for sites where you want a biplot focussed on the sites/samples and the (dis)similarity between them in terms of the species (or variables), and use scaling for species where you want to best represent the correlations between species (or variables). Ive plotted the site and species types below (Fig. 4.21). Were you able to produce the same ordinations? par(mfrow = c(1, 2)) # Triplots of the parsimonious RDA (scaling=1) Scaling 1 plot(optm, scaling = &quot;sites&quot;, main = &quot;Sites scaling (scaling=1)&quot;, correlation = TRUE) # Triplots of the parsimonious RDA (scaling=2) Scaling 2 plot(optm, scaling = &quot;species&quot;, main = &quot;Species scaling (scaling=2)&quot;) Figure 4.21: Triplots of an RDA ordination on the lichen data with either site or species scaling using a basic plot command Its pretty hard to see what is going on in these basic plots, so I am going to take the time to produce more curated versions. I am going to select which species data I display, and the change the appearance and axis limits somewhat. par(mfrow = c(1, 1)) sp.scores = scores(optm, display = &quot;species&quot;, scaling = 1) sp.scores = sp.scores[sp.scores[, 1] &gt; abs(0.1) | sp.scores[, 2] &gt; abs(0.1), ] plot(optm, scaling = 1, type = &quot;n&quot;, main = &quot;Sites scaling (scaling=1)&quot;, ylim = c(-0.2, 0.2), xlim = c(-0.3, 0.5)) arrows(x0 = 0, y0 = 0, sp.scores[, 1], sp.scores[, 2], length = 0.05) text(sp.scores, row.names(sp.scores), col = 2, cex = 0.6, pos = 3) text(optm, display = &quot;bp&quot;, scaling = 1, cex = 0.8, lwd = 1.5, row.names(scores(optm, display = &quot;bp&quot;)), col = 4) text(optm, display = c(&quot;sites&quot;), scaling = 1, cex = 1) Figure 4.22: Triplot of an RDA ordination on the lichen data with site scaling plot(optm, scaling = 2, type = &quot;n&quot;, main = &quot;Species scaling (scaling=2)&quot;, ylim = c(-0.4, 0.4)) arrows(x0 = 0, y0 = 0, sp.scores[, 1], sp.scores[, 2], length = 0.05) text(sp.scores, row.names(sp.scores), col = 2, cex = 0.7, pos = 3) text(optm, display = &quot;bp&quot;, scaling = 2, cex = 0.8, lwd = 1.5, row.names(scores(optm, display = &quot;bp&quot;)), col = 4) text(optm, display = c(&quot;sites&quot;), scaling = 2, cex = 1) Figure 4.23: Triplot of an RDA ordination on the lichen data with species scaling Okay, lets figure out how to interpret these plots. In the site scaling option (Fig 4.22), dissimilarity distances between data objects are preserved. So, sites which are closer together on the ordination are more similar. In addition, the projection of a data object (site) onto the line of a response variable (species) at right angle approximates the position of the corresponding object along the corresponding variable. Angles between lines of response variables (species) and lines of explanatory variables are a two-dimensional approximation of correlations. But other angles between lines are meaningless (e.g., angles between response vectors dont mean anything). In the species scaling plot (Fig 4.23), distances are now meaningless, and as a result the length of vectors are not important. However, the cosine of the angle between lines of the response variables or of explanatory variables is approximately equal to the correlation between the corresponding variables. So, looking at our first plot, sites 2, 28 and perhaps 3 seem quite different from others. For site 3, this is perhaps because of the abundance of Cladrang, which might be responding to iron levels (Fe). Site 28 might differ because of higher abundances of Hylosple, which may be related to Mn. Notice how we can see on the species scaling plot the strong correlation between P and species Dicrsp, which is somewhat different on the site scaling plot. 4.4.5.1 What next? In this short introduction we have not demonstrated how to complete and interpret other forms of constrained ordination such as Canonical Correspondence Analysis, partial RDA (which would allow you to control for well-known linear effects), or analysis designed for qualiticative variables, such as Multiple Correspondence Analysis (MCA). There are functions in the vegan and FactoMineR packages to do many of these things. 4.4.6 Feedback We value your input! Please take the time to let us know how we might improve these materials. Survey 4.5 References Borcard, D., Gillet, F., &amp; Legendre, P. (2011). Numerical ecology with R. New York: springer. Goodall, D. W. (1954). Objective methods for the classification of vegetation. III. An essay in the use of factor analysis. Australian Journal of Botany, 2(3), 304324. https://doi.org/10.1071/bt9540304 Greenacre, M. (2017). Ordination with any dissimilarity measure: A weighted Euclidean solution. Ecology, 98(9), 22932300. https://doi.org/10.1002/ecy.1937 Hotelling, H. (1933). Analysis of a complex of statistical variables into principal components. Journal of Educational Psychology, 24(6), 417441. https://doi.org/10.1037/h0071325 Legendre, P., &amp; Anderson, M. J. (1999). Distance-Based Redundancy Analysis: Testing Multispecies Responses in Multifactorial Ecological Experiments. Ecological Monographs, 69(1), 124. https://doi.org/10.1890/0012-9615(1999)069[0001:DBRATM]2.0.CO;2 Legendre, P., &amp; De Cáceres, M. (2013). Beta diversity as the variance of community data: Dissimilarity coefficients and partitioning. Ecology Letters, 16(8), 951963. https://doi.org/10.1111/ele.12141 Legendre, P., &amp; Gallagher, E. D. (2001). Ecologically meaningful transformations for ordination of species data. Oecologia, 129(2), 271280. https://doi.org/10.1007/s004420100716 Legendre, P., &amp; Legendre, L. (2012). Numerical ecology. Elsevier. Pearson, K. (1901). LIII. On lines and planes of closest fit to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11), 559572. https://doi.org/10.1080/14786440109462720 Perkins, M. J., McDonald, R. A., Veen, F. J. F. van, Kelly, S. D., Rees, G., &amp; Bearhop, S. (2014). Application of Nitrogen and Carbon Stable Isotopes (15N and 13C) to Quantify Food Chain Length and Trophic Structure. PLOS ONE, 9(3), e93281. https://doi.org/10.1371/journal.pone.0093281 Pielou, E. C. (1984). The interpretation of ecological data: A primer on classification and ordination. John Wiley &amp; Sons. Rao, C. R. (1964). The Use and Interpretation of Principal Component Analysis in Applied Research. Sankhy: The Indian Journal of Statistics, Series A (1961-2002), 26(4), 329358. https://www.jstor.org/stable/25049339 Ter Braak, C. J. (1988). CANOCO-a FORTRAN program for canonical community ordination by partial detrended canonical correspondence analysis, principal components analysis and redundancy analysis (version 2.1). MLV. Watson, N. H. F., &amp; Carpenter, G. F. (1974). Seasonal Abundance of Crustacean Zooplankton and Net Plankton Biomass of Lakes Huron, Erie, and Ontario. Journal of the Fisheries Research Board of Canada, 31(3), 309317. https://doi.org/10.1139/f74-050 van den Wollenberg, A. L. (1977). Redundancy analysis an alternative for canonical correlation analysis. Psychometrika, 42(2), 207219. https://doi.org/10.1007/BF02294050 4.6 Answer Key Exercise 1. All we need to do is to take the square root of the sum of the squared differences to obtain the Euclidean distance as round(sqrt(eu[11, 4]), 2) [1] 61.6 Exercise 2. We use the kmeans() function with 4 clusters to find groups in the isotope data. We can then save the assigned clusters to our dataframe and plot in a similar way # select 4 clusters and run the kmeans function kclust = kmeans(niso[, c(&quot;C&quot;, &quot;N&quot;)], 4) niso$kclust = kclust$cluster # superimpose the saved clusters on our plotted isotope # data plot(iso$N ~ iso$C, col = as.numeric(as.factor(niso$clust)), xlim = c(-35, 0), pch = as.numeric(as.factor(niso$Species)), xlab = expression(paste(delta, &quot;13C&quot;)), ylab = expression(paste(delta, &quot;15N&quot;))) legend(&quot;topright&quot;, legend = unique(as.factor(niso$kclust)), pch = 1, col = as.numeric(unique(as.factor(niso$kclust))), bty = &quot;n&quot;, title = &quot;kcluster&quot;) legend(&quot;bottomright&quot;, legend = as.character(unique(as.factor(niso$Species))), pch = as.numeric(unique(as.factor(niso$Species))), bty = &quot;n&quot;, title = &quot;Species&quot;) Figure 4.24: K means clustering on Perkins et al (2014) data for 4 clusters It looks like kmeans has the same problem with distinguishing C3 plant-based foodwebs. But we still get three groups that roughly map onto our information about the data. Exercise 3. Interpretation of the PCoA and the nMDS oridination plots of the varespec data is straightforward: sites ordinated closer to one another are more similar than those ordinated further away. We can interpret the nMDS, remembering that the first ordination axis corresponds to the most variance in our data and so on. Looking at the plot, it seems that sites 28, 27 and possibly 21 are pretty similar to each other, and different from other sites, possibly due to the species aligned in that direction on the x-axis such as Betupube. Sites 2 and 5 might be rather different from other sites. This might be due species like Cladcerv for site 2. Some sites (e.g., 6, 13, 20) are not well distinguished by the ordination. Others like 9, 10, 11, and 12 might group together. PCoA gives us broadly the same information and also suggests that sites 21, 28, 27 are similar to each other and different from other sites. This visualization also agrees that sites 2 and 5 might be different from the others, and that 9, 10, 11, and 12 might be similar. Exercise 4. The automatically selected rda model of the lichen data includes three significant predictors: Fe, P and Mn, while the full model has 14 potential predictors. Our manually selected model of three predictors N, K and Al dont even appear in the automatically selected model. "],["machine-learning-and-classification.html", "5 Machine learning and classification 5.1 Logistic regression 5.2 Cross-validation 5.3 Linear Discriminant Analysis (LDA) 5.4 Tree-based methods for classification 5.5 Artificial Neural Networks (ANN) 5.6 References 5.7 Answer Key", " 5 Machine learning and classification You have 2000 photos of zooplankton, or 500 recordings of bat vocalizations, or 20 000 pixels from a satellite image. How do you ensure that each one is placed in the right category of species or type? There are many methods that can be employed for classification tasks like this ranging from logistic regression to random forest techniques. While some of these methods are classic multivariate methods, others, like random forest classifiers, are machine learning tasks. Machine learning is an application of artificial intelligence, which allows algorithms to become more accurate at predicting outcomes without being explicitly programmed to do so. When we fit a regression in the ordinary way, we specify the model ahead of time and determine if the model has good or bad fit. We might then modify our model to reduce prediction error. In machine learning, the algorithm itself completes the tasks of model specification, evaluation and improvement. Spooky! In this module we will continue our exploration of techniques for multivariate data (see Module 3 on Clustering and ordination), but will pay more attention to machine learning approaches. Classification is the task of assigning data objects, such as sites, species or images to predetermined classes. Determining what class of data object you have is a question that usually turns on multiple predictors. For example, to classify leaf images to different species, predictors such as size, shape and colour may be used. If you have satellite data, you may need to classify the different pixels of the image as agricultural, forest, or urban. So for classification tasks our response variable, y, is qualitative or categorical (e.g., gender, species, land classification) while our predictors can be either quantitative or qualitative. The main thing to remember about machine learning is that these methods develop their own models for prediction. For example, in image recognition, they might learn to identify images that contain different species of zooplankton by analyzing example images that have been previously labelled by the researcher. However, generally there is no information provided about what rules the researcher used to identify the images (e.g., size, shape, etc). Instead the machine learning tool develops its own framework for identification, which is one reason why the solution from the machine learning routine can seem like a black box. Once trained, the algorithm can identify the species of unlabeled images, and may or may not be using the same kind of identifiers that a researcher would use to manually process the images. 5.1 Logistic regression One of the simplest classification methods, and one that does not involve machine learning, is logistic regression. Lets take a common example. A non-native species has been introduced to a region, and we would like to know what percentage of the region would be suitable habitat, in order to get an idea of risks of impact on the native ecosystem. We think that average temperature controls habitat suitability, and we have presence/absence data for the species across a range of different sites. Could we use simple regression to answer the question of whether a given area is suitable habitat? If we indicate absence as 0, and presence as 1, we can regress species occurrence again average annual temperatures at each location. # create some synthetic data with random variation x = rnorm(200, 12, 2.5) y = x y = ifelse(x &gt; 12, 1, 0) e = rnorm(200, 0, 0.1) y = ifelse(y + e &gt;= 1, 1, 0) # plot the data and the regression line plot(y ~ x, pch = 16, ylim = c(-0.1, 1.1), xlim = c(0, 20), col = rgb(0.1, 0.3, 0.4, 0.6), ylab = &quot;Species occurrence&quot;, xlab = &quot;Mean annual temperature (°C)&quot;, cex.lab = 1.5, las = 1) lreg = lm(y ~ x) abline(lreg, col = 2, lwd = 2) Figure 5.1: Species presence/absence and mean annual temperature with linear regression As we can see in Fig. 5.1, the linear regression does not make a lot of sense for a response variable that is restricted to the values of 0 and 1. The regression line \\(\\beta_0+\\beta_1x\\) can take on any value between negative and positive infinity, but we dont know how to interpret values greater than one or less than zero. The regression line almost always predicts wrong value for y in classification problems. Instead of trying to predict y, we can try to predict p(y = 1), i.e., the probability that the species will be found in the area. For invasive species, this probability is often interpreted as habitat suitability for the species. We need a function that gives outputs between 0 and 1: logistic regression is one solution. In this model, probability of y=1 for a given value of x is given as: \\[p(y=1|x)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x)}}.\\] Rearranging, we have: \\[\\frac{p(y=1|x)}{1-p(y=1|x)}=e^{\\beta_0+\\beta_1x}.\\] Taking the natural logarithm, we can see that the logistic regression is linear in x: \\[\\log\\left({\\frac{p(y=1|x)}{1-p(y=1|x)}}\\right )=\\beta_0+\\beta_1x,\\] where the lefthand side is called the log-odds or logit. The logistic function will always produce an S-shaped curve bounded at 0 and 1, so regardless of the value of x, we will obtain a sensible prediction. Lets apply this model to our non-native species data using the generalized linear model or glm() function. We use the glm() function to perform logistic regression by passing in the family=binomial argument. But if we use glm() to fit a model without passing in the family argument, then it performs linear regression, just like the linear model or lm() function. We will fit the model and then get the predictions. glm.fit.sp = glm(y ~ x, family = binomial) glm.probs &lt;- predict(glm.fit.sp, type = &quot;response&quot;) # plot the data plot(y ~ x, pch = 16, ylim = c(-0.1, 1.1), col = rgb(0.1, 0.3, 0.4, 0.6), ylab = &quot;Species occurrence&quot;, xlab = &quot;Mean annual temperature (°C)&quot;, cex.lab = 1.5, las = 1) # save the regression coefficients logcoefs = round(as.data.frame(summary(glm.fit.sp)$coefficients), 2) # plot the fitted curve using the saved coefficients curve(exp(logcoefs[1, 1] + logcoefs[2, 1] * x)/(1 + exp(logcoefs[1, 1] + logcoefs[2, 1] * x)), add = TRUE, col = 2, lwd = 2) Figure 5.2: Species presence/absence and mean annual temperature with logistic regression 5.1.1 Interpreting the logistic regression Lets take a closer look at the output for our logistic regression (Fig. 5.2). First off, we need to know if the intercept, \\(\\beta_0\\), and slope, \\(\\beta_1\\), are significantly different from zero. A z distribution is used for this test, and we find that both the intercept and slope are significantly different from zero (Table 5.1). Our p-values are very, very small, so our model is doing better than random chance. Table 5.1: Logistic regression coefficient estimates and hypothesis tests from species occurrence data Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -8.87 1.38 -6.41 0 x 0.62 0.10 5.97 0 The estimated intercept is typically not of interest. Its main purpose is to adjust the average fitted probability to the proportion of ones in the data. You may be confused by the slope value. Interpreting what \\(\\beta_1\\) means is not very easy with logistic regression, simply because we are predicting p(y=1) and not y, and that the function is sigmoid. If \\(\\beta_1\\) = 0, this means there is no relationship between p(y=1) and x. If \\(\\beta_1\\) &gt; 0, this means that when y gets larger so does the probability that y = 1. If \\(\\beta_1\\) &lt; 0, this means that when x gets larger, the probability that y = 1 gets smaller. Our \\(\\beta_1\\) is positive, so we are sure that as temperature increases, the probability of habitat suitability will increase as well. For example, suppose a region has an average annual temperature of 12°C. We know that the probability that the habitat is suitable (p(y=1)) for our invasive species is: \\[p(y=1|x)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x)}}.\\] Substituting in our fitted values from the logistic regression, we have: \\[p(y)=\\frac{1}{1+e^{-(-8.87 + 0.62)(12)}}=0.19.\\] The value changes with temperature, so for an average annual temperature of 16°C that probability is 0.74, and so on. Exercise 1 Try this for yourself, see if you can use similar code in order to estimate the probability that a site with an average annual temperature of 18°C will be occupied by the invasive species. 5.1.2 The confusion matrix Of course, the data used to fit this relationship is zeros (absence) and ones (present). In order to test model fit we need to decide on a threshold value for the probability prediction. For example, an easy one is 50%. If the model predicts a probability of greater than 0.5, then we will score that as presence (y=1), while if the predicted value is equal to or less than 0.5, we will score as an absence (y=0). Now we have model predictions in terms of predicted absences and presences (i.e., zeros and ones), and can compare directly to our data. When binary classifications are made by converting the probabilities using a threshold, there can be four cases for a certain observation: The response actually negative, the model predicts it to be negative. This is known as true negative (TN). In our case, the invasive species is not present at the location, and the model predicts that it should not be present. The response actually negative, but the model predicts it to be positive (i.e., false positive, FP). The response actually positive, and the model predicts it to be positive (i.e., true positive TP). The response actually positive, but the model predicts it to be negative (i.e., false negative FN). We can summarize this information in a confusion matrix which records the number of times the model correctly predicted the data, and the number of times the model makes incorrect predictions. # determine model occurrence predictions based on threshold # value logocc &lt;- ifelse(glm.probs &gt; 0.5, 1, 0) # Calculate a confusion matrix ctab = table(logocc, y) dimnames(ctab) &lt;- list(Actual = c(&quot;absence(0)&quot;, &quot;presence(1)&quot;), Predicted = c(&quot;absence(0)&quot;, &quot;presence(1)&quot;)) ctab Predicted Actual absence(0) presence(1) absence(0) 128 33 presence(1) 17 22 Elements on the diagonal from left to right are correct classifications, while off-diagonal elements are miss-classifications (i.e., predictions where the predict code of 0 or 1 does not match the actual code of 0 or 1). So in this case, we have 128 true negatives (TN), 33 false positives (FP), 17 false negatives (FN), and 22 true positives (TP). We can quantify these errors in a number of ways. The misclassification rate, or error rate is the most common metric used to quantify the performance of a binary classifier. This is the probability that the classifier makes a wrong prediction (given as \\(\\frac{FN+FP}{TN+FN+TP+FP}\\)). Overall 50 observations have been misclassified, so we have a total error rate of 25%. However, 44% of the presence data has been misclassified, compared to 20% of the absence data. So with respect to the predicting the potential presence of an invasive species, were not doing much better than random chance. The terms sensitivity and specificity characterize the performance of classifier for these specific types of errors. In this case, the sensitivity is the percentage of occupancy locations that are correctly identified (true positives), which is 56% or one minus the misclassification rate of positives (or 1-0.44), also calculated as TP/(TP+FN). Specificity is the percentage of non-occupancy sites that are correctly identified (true negatives). We can calculate this as one minus the misclassification of true negatives from the values above (1  0.2) = 0.8, or from the formula TN/(TN+FP). We can of course, change the decision threshold to see if we can get a better outcome. Lets try 0.65 instead of 0.5. Predicted Actual 0 1 0 136 44 1 9 11 This higher threshold gives us an error rate of 26%, sensitivity of 55% and specificity of 76%, so not much improvement. Exercise 2 Can you calculate the sensitivity and specificity for a threshold classification of 0.45? 5.1.3 ROC and AUC We can also examine the performance of the model across a range of thresholds. We often see this approach in species distribution modelling, where sensitivity (% of true positives) is plotted against 1-specificity (% of false positives) for threshold values from 0 to 1. This graph is called the Receiver Operator Curve (ROC), and the Area Under that Curve (AUC) is calculated. This is a fairly standard evaluation for binary classifiers, and there are a number of R packages that will complete this analysis for you. If the model is not performing better than random chance, the expected ROC curve is simply the y=x line. Where the model can perfectly separate the two classes, the ROC curve consists of a vertical line (x=0) and a horizontal line (y=1). For real and simulated data, usually the ROC stays in between these two extreme scenarios. Lets try with our simulated data. library(ROCit) ROCit_obj &lt;- rocit(score = glm.fit.sp$fitted.values, class = y) pauc = plot(ROCit_obj) Figure 5.3: Receiver Operator Curve (ROC) for the logistic regression binary classifier of species occurence data summary(ROCit_obj) Method used: empirical Number of positive(s): 55 Number of negative(s): 145 Area under curve: 0.8361 Overall, we have an area under the ROC curve (Fig.5.3) of 0.84, which is not bad, given the maximum value is one. The optimal threshold value is given by the Youden index as 0.66. The Youden index maximizes the difference between sensitivity and 1-specificity and is defined as sensitivity+specificity-1. Lets try this threshold directly: Predicted Actual 0 1 0 139 46 1 6 9 We can see that this error threshold, as summarized above, gives an error rate of 26%, sensitivity of 60% and specificity of 75%. Overall, an okay, but not fantastic model, at the best performing threshold. 5.2 Cross-validation So far, weve evaluated model performance with the data that we used to train the model, but the point of classification tools is to be able to use them on data where we dont already know the answer. In that sense, we are uninterested in in model performance on training data, what we really want to do is to test the model on data that was not used in model fitting. For example, we dont really care how well our method predicts habitat suitability where the invasive species is already located! What we need to know is how well it predicts the habitat suitability of locations where the species has not yet invaded. The model performance on this testing data will give us a better idea of the errors we might expect when we apply our classifier to novel data. While we might naively expect that model performance on the training data will be the same on the testing data, in practice the errors are usually larger, sometimes much larger. In more complex models, this error rate is often the result of overfitting the training data, so that pattern which is just noise is included in the model fit. Consequently, the model is not well fit to data with different sources of noise. Of course, in biology data is almost always limited! While you might have an extra independent dataset kicking around waiting to be used for model testing, if you dont, you can divide your single dataset into training and testing sets. One easy way to do this is just using random selection. Lets try on our data. Well divide a dataframe with our data into two parts using the sample() function, fit our logistic model on the training data, and evaluate its performance on the testing data. # simulate some temperature and occupancy data using a # random number generator xs = rnorm(200, 12, 2.5) ys = ifelse(xs &gt; 12, 1, 0) es = rnorm(200, 0, 0.1) # adding some randomness to simulated occupancy data ys = ifelse(ys + es &gt;= 1, 1, 0) # create a dataframe with our temperature and occupancy # data ivsp = data.frame(temp = xs, occ = ys) # randomly sample 75% of the data (by generating random # numbers based on the number of rows) samp = sample(nrow(ivsp), nrow(ivsp) * 0.75, replace = FALSE) # divide into training and testing sets train &lt;- ivsp[samp, ] test &lt;- ivsp[-samp, ] # fit the logistic model on the training data log.fit.inv = glm(occ ~ temp, family = binomial, data = train) # test the logistic model on the testing data log.predict &lt;- predict(log.fit.inv, newdata = test, type = &quot;response&quot;) # determine predicted occupancy based on threshold value of # 0.5 pred.occ &lt;- ifelse(log.predict &gt; 0.5, 1, 0) # Calculate a confusion matrix ctab = table(pred.occ, test$occ) dimnames(ctab) &lt;- list(Actual = c(0, 1), Predicted = c(0, 1)) ctab Predicted Actual 0 1 0 40 7 1 2 1 # Calculate error rate, sensitivity and specificity err = round((ctab[1, 2] + ctab[2, 1])/sum(ctab), 2) sens = round(ctab[2, 2]/(ctab[2, 1] + ctab[2, 2]), 2) spec = round(1 - ctab[1, 2]/(ctab[1, 1] + ctab[1, 2]), 2) print(paste0(&quot;error rate=&quot;, err, &quot;; sensitivity=&quot;, sens, &quot;; specificity=&quot;, spec)) [1] &quot;error rate=0.18; sensitivity=0.33; specificity=0.85&quot; You may want to verify for yourself that the sample function randomly selects rows out of a dataframe 5.2.1 k-Fold Cross-Validation Youll notice that dividing the data up into testing and training sets reduces the number of observations available to test the model. We can of course use different percentages to divide up our one dataset into training and testing sets, but models tend to have poorer performance when trained on fewer observations. On the other hand, the small testing dataset may tend to overestimate the test error rate for the model fit, as compared to error rates obtained on a larger amount of data. And what about the effects of that random sampling? If we repeat the process of randomly splitting the sample set into two parts, we will get a somewhat different estimate for the error rate on testing data each time, because different observations will be randomly included in the dataset each time. Sometimes the differences between error rates on different testing datasets can be rather large. For example, by random selection an observation which is a huge outlier could be included in one small testing dataset, but not in another, resulting is very different error rates. To guard against undue influence of single observations in our small test dataset we could do the routine of randomly sampling to obtain testing and training sets several times, and look at the average of our testing data performance. One way of implementing this type of resampling scheme is k-fold cross-validation. With this method, we randomly divide the set of observations into k groups, or folds, of approximately equal size. The first fold is treated as a testing set, and the model is fit on the remaining k  1 folds. This procedure is repeated k times, and each time, a different group of observations is treated as the testing set. See the pseudocode below for and idea of how to do this. We then average the error rates from each test fold. We can even repeat the entire procedure several times in repeated k-fold cross-validation. # example code for iterated cross-validation set up reps = 10 nfolds = 5 for (j in 1:reps) { # generate array containing fold-number for each sample # (row) foldsset &lt;- rep_len(1:nfolds, nrow(data)) # create subsetes of data based on the random fold # assignment folds &lt;- sample(foldsset, nrow(data)) # actual cross validation: we allow each fold to act as # the test data in turn for (k in 1:nfolds) { # split of the data fold &lt;- which(folds == k) data.train &lt;- data[-fold, ] data.test &lt;- data[fold, ] # train and test your model with data.train and # data.test } } # repeat for the desired number of interactions While we can program the k-fold cross-validation ourselves, there is a nice package in R, caret, that is a good wrapper for these kinds of tasks, and which is really great for machine learning stuff. We can use the caret package to run or k-fold cross validation and to generate our confusion matrix and other statistical info about our classifier. Going back to the species occurrence data, lets use the caret package to run a k-fold cross-validation, with 5 folds, rather than simply dividing the data into training and testing sets. library(caret) # cross-validation with 5 folds train_control &lt;- trainControl(method = &quot;cv&quot;, number = 5) # fit the logistic regression hab &lt;- train(as.factor(occ) ~ temp, data = train, trControl = train_control, method = &quot;glm&quot;, family = &quot;binomial&quot;) confusionMatrix(hab) Cross-Validated (5 fold) Confusion Matrix (entries are percentual average cell counts across resamples) Reference Prediction 0 1 0 71.3 18.0 1 6.7 4.0 Accuracy (average) : 0.7533 The caret package neatly does our data resampling, fits the model, gets an average model performance across the 5 testing sets, AND calculates the confusion matrix for us. Not bad! The first confusion matrix reports classification error averaged all the folds (assuming a 0.5 threshold). We can also see the regression output for the final model and its predictions. We can generate the confusion matrix and statistics for just the final model, for any given threshold. summary(hab$finalModel) Call: NULL Deviance Residuals: Min 1Q Median 3Q Max -1.8592 -0.6557 -0.4518 -0.1982 1.8227 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -6.56702 1.30310 -5.040 4.67e-07 *** temp 0.42048 0.09782 4.298 1.72e-05 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 158.07 on 149 degrees of freedom Residual deviance: 134.21 on 148 degrees of freedom AIC: 138.21 Number of Fisher Scoring iterations: 5 habprob &lt;- hab$finalModel$fitted.values # t is the threshold for which the confusion matrix shall # be computed habclass &lt;- function(t) ifelse(habprob &gt; t, 1, 0) # test with confusion matrix confusionMatrix(as.factor(habclass(0.5)), as.factor((train$occ))) Confusion Matrix and Statistics Reference Prediction 0 1 0 109 28 1 8 5 Accuracy : 0.76 95% CI : (0.6835, 0.8259) No Information Rate : 0.78 P-Value [Acc &gt; NIR] : 0.757854 Kappa : 0.1063 Mcnemar&#39;s Test P-Value : 0.001542 Sensitivity : 0.9316 Specificity : 0.1515 Pos Pred Value : 0.7956 Neg Pred Value : 0.3846 Prevalence : 0.7800 Detection Rate : 0.7267 Detection Prevalence : 0.9133 Balanced Accuracy : 0.5416 &#39;Positive&#39; Class : 0 Perhaps the most informative of these stats is the No Information Rate which tests whether our classifier does better than random assignment. We see that our accuracy rate is significantly greater than this no information rate, and so, this should be an okay classifier. Also, the Balanced Accuracy Statistic gives an accuracy value that weights both majority and minority classes evenly, and is useful if you have unbalanced class membership in your data. In our case, the accuracy and balanced accuracy rates are a bit different, so we may want to investigate further (see Unbalanced classes). 5.2.2 Multiple logistic regression If we have more than one predictor, we can fit a multiple logistic just like a regular regression, as: \\[p(y)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_nx_n)}}\\] and the \\(x_n\\) predictors can be both qualitative or quantitative. For example, we could add a land classification to our invasive species habitat suitability model so that we have both temperature (\\(x_1\\)) and urban and rural (\\(x_2\\)) land types. glm.fit.sp2 = glm(y ~ x + as.factor(x2), family = binomial) In this case, our model now has two responses, one for land categorized as urban, and one for land categorized as rural (Fig. 5.4). Table 5.2: Logistic regression coefficient estimates and hypothesis tests from species occurrence data with temperature and land category as predictors Estimate Std. Error z value Pr(&gt;|z|) (Intercept) -9.72 1.52 -6.39 0 x 0.65 0.11 5.87 0 as.factor(x2)urban 1.40 0.42 3.30 0 Both predictors are significantly different from zero (Table 5.2), and the values tell is that if the land is categorized as urban, we must increase our probability estimate upwards from that of a rural area as: \\[p(y)=\\frac{1}{1+e^{-(-9.72 + 0.65x_1+1.4 (1))}}\\] Figure 5.4: Species occurrence vs temperature and land classification as urban or rural Of course, we are still using logistic regression as a binary classifier. Logistic regression can be extended to multiple classification problems in different ways, but in practice these methods tend not to be used all that often. Instead other techniques such as linear discriminant analysis and random forest tend to be used for multiple-class classification problems (see below). Exercise 3: Logistic regression as a binary classifier Try to use logistic regression on your own, with a pre-existing dataset in the MASS package. Start by installing the MASS package, load the library and load the data. The dataset we will use has been previously divided into training (Pima.tr) and testing sets (Pima.te), so that you dont have to employ k-fold cross validation. The dataset describes several risk factors for diabetes. Type help(Pima.tr) or ?Pima.tr to get a description. Youll notice that the type variable is our classifier and determines whether the patient has diabetes or not. Next construct a multiple logistic regression to use as a classifier. Examine your output to determine if the regression is significant. We can use ~. in the formula argument to mean that we use all the remaining variables in the dataset as predictors. Next, use some testing data to test your classifier. The Prima.te dataset to be used as testing data has already been created for you in the MASS package. Use the predict function to get the predicted probabilities, and a threshold value to get classifications. Then construct a confusion matrix to determine how well your predictor did. 5.2.3 Examples of logistic regression used for classification While there is an emphasis in the literature on more complex classification methods, we should point out that we have spent a significant amount of time on this relatively simple method not just for pedagogical reasons. Logistic regression is sometimes just as effective, or even more effective, at classification problems, and further, has the merit of being easy to understand. Cuddington, K., Sobek-Swant, S., Drake, J., Lee, W., &amp; Brook, M. (2021). Risks of giant hogweed (Heracleum mantegazzianum) range increase in North America. Biological Invasions. https://doi.org/10.1007/s10530-021-02645-x Tuda, M., &amp; Luna-Maldonado, A. I. (2020). Image-based insect species and gender classification by trained supervised machine learning algorithms. Ecological Informatics, 60, 101135. https://doi.org/10.1016/j.ecoinf.2020.101135 5.3 Linear Discriminant Analysis (LDA) While there are ways to modify logistic regression to classify more that two types of objects, in practice there are better methods such as linear discriminant analysis (LDA) or various machine learning techniques (described below). LDA can be used to classify data based on categorical response variables. One implementation of LDA tries to find a linear combination of the predictors that gives maximum separation between the centers of the data while at the same time minimizing the variation within each group of data. This approach is implemented in many R packages, as in the lda() function of the MASS package, for example. You might wonder how finding a linear combination of predictors differs from linear regression, logistic regression, or principal components analysis (PCA). LDA is popular when we have more than two response classes (polytomous responses), which logistic regression can only handle with some modification. In addition, when classes are well-separated, parameter estimates for logistic regression are surprisingly unstable, but LDA does not suffer from this problem. Finally, unlike linear regression, LDA chooses parameters to maximize distance between means of different categories. This approach of using a linear combination of predictors to predict similarity also seems similar to PCA, although of course PCA is an unsupervised learning technique (that is we dont dont have classification which we provide), while LDA is a supervised learning technique (it uses a priori class information to train the model). Both PCA and LDA provide the possibility of dimensionality reduction, which is very useful for visualization, and is often used to prepare data for machine learning techniques. However, we would expect (by definition) LDA to provide better data separation when compared to PCA. Lets try out this method by attempting to classify observations in the iris dataset to species using the 4 metrics of sepal and petal length and width. You may want to take a look at the data first using the head(), str() or summary() functions. There are three species where quantitative morphological features are recorded for each individual. library(MASS) data(&quot;iris&quot;) train &lt;- sample(1:150, 75) irislda &lt;- lda(Species ~ ., iris, subset = train) irislda Call: lda(Species ~ ., data = iris, subset = train) Prior probabilities of groups: setosa versicolor virginica 0.36 0.32 0.32 Group means: Sepal.Length Sepal.Width Petal.Length Petal.Width setosa 4.951852 3.385185 1.462963 0.2259259 versicolor 5.766667 2.762500 4.133333 1.2875000 virginica 6.629167 3.050000 5.616667 2.0666667 Coefficients of linear discriminants: LD1 LD2 Sepal.Length -0.03310001 -0.2888114 Sepal.Width 2.77688758 -2.6437916 Petal.Length -2.11727618 1.0919196 Petal.Width -3.48689106 -2.8981641 Proportion of trace: LD1 LD2 0.99 0.01 Look at the lda object you have saved. A call to lda() returns the prior probability of each class, which is just based on the number of classes (the 3 difference species in this case), and the number of observations in each class. We start out very close to an even probability of an observation falling in any class (about 1/3). We also have the class-specific means for each covariate. So observations of species I.setosa have a mean Sepal.Length of 4.95. Finally we have the fitted model, which consists of the the linear combination of predictor variables that are used to form the LDA decision rule. With 3 classes we have at most two linear discriminants, and similar to PCA we can see parameters that relate the various characteristics such as Sepal.Length to these axes (e.g., LD1 = -0.03 Sepal.Length + 2.78 Sepal.Width + -2.12 Petal.Length + -3.49 Petal.Width. The trace shows us how much variance is captured by each axis. In this case, almost all of our class division can be done on the first axis. There is also a prediction method implemented for lda objects. It returns the classification and the posterior probabilities of the new data. If you examine the prediction object, you will see it contains a matrix called posterior whose columns are the groups, rows are the individuals and values are the posterior probability that the corresponding observation belongs to each of the groups. Lets see how we do with prediction on the testing portion of the data (i.e., everything that is not the training data). plda = predict(irislda, newdata = iris[-train, ]) confusionMatrix(iris$Species[-train], plda$class) Confusion Matrix and Statistics Reference Prediction setosa versicolor virginica setosa 23 0 0 versicolor 0 23 3 virginica 0 0 26 Overall Statistics Accuracy : 0.96 95% CI : (0.8875, 0.9917) No Information Rate : 0.3867 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.9399 Mcnemar&#39;s Test P-Value : NA Statistics by Class: Class: setosa Class: versicolor Class: virginica Sensitivity 1.0000 1.0000 0.8966 Specificity 1.0000 0.9423 1.0000 Pos Pred Value 1.0000 0.8846 1.0000 Neg Pred Value 1.0000 1.0000 0.9388 Prevalence 0.3067 0.3067 0.3867 Detection Rate 0.3067 0.3067 0.3467 Detection Prevalence 0.3067 0.3467 0.3467 Balanced Accuracy 1.0000 0.9712 0.9483 We have very good accuracy. We can also visualize our prediction accuracy using a regular plot() function. plot(plda$x, col = as.numeric(plda$class) + 1, pch = as.numeric(plda$class) + 14, ylim = c(-4, 4)) legend(&quot;bottom&quot;, levels(iris[-train, &quot;Species&quot;]), bty = &quot;n&quot;, pch = as.numeric(unique(plda$class)) + 14, col = as.numeric(unique(plda$class)) + 1) Figure 5.5: Classification of the iris dataset using LDA Again, we see some misclassifications (Fig. 5.5), where the colour of the symbol indicates whether it has been correctly identified, but this classifier works pretty well. The ldahist() function shows us why this is so, the centers of our synthetic variables are pretty well-separated between predicted groups on the first axis (5.6). ldahist(plda$x[, 1], g = plda$class, col = 2) Figure 5.6: Histogram of the synthetic axis coordinates for each species The basic lda() function assumes that the predictors have linear relationships relationships with the classification. Another way to display the model is to plot out the decision boundary lines in the variable space. However, it is also possible to assume nonlinear relationships. The drawparti() function in the klaR package can display the results of linear or quadratic classifications 2 variables at a time. 5.3.0.1 Unbalanced classes We should note that most if not all classification methods will perform poorly when the training data has a large number of observations in some classes and very few in others. There are several methods to try and cope with this problem, which we dont have space to describe here. The simplest approach is, of course, to just subsample your data so that there are close to equal numbers of observations in each class (called undersampling). However, this method will waste some data, and you may not even have enough data for this method to be feasible. With oversampling, we randomly duplicate samples from the class with fewer instances, or we generate additional data, based on the data that we do have, so as to match the number of samples in each class. We avoid losing information with this approach, but we run the risk of overfitting our model, as some methods of oversampling will lead to having the same samples in both the training and test data. There are also hybrid methods that combine undersampling with the generation of additional data. Two of the most popular are ROSE (Lunardon et al. 2014) and SMOTE (Chawla et al. 2002). Both of which can be implemented direct in the caret package. You can read more about unbalanced data here ([Fernández et al 2018] (https://link.springer.com/content/pdf/10.1007/978-3-319-98074-4.pdf)). 5.3.1 Examples of discriminant analysis used for classification Examples of using discriminant analysis (linear or nonlinear) in the literature include classification of images, sounds and stable isotope data. Mahdianpari, M., Salehi, B., Mohammadimanesh, F., Brisco, B., Mahdavi, S., Amani, M., &amp; Granger, J. E. (2018). Fisher Linear Discriminant Analysis of coherency matrix for wetland classification using PolSAR imagery. Remote Sensing of Environment, 206, 300317. https://doi.org/10.1016/j.rse.2017.11.005 Bellisario, K. M., VanSchaik, J., Zhao, Z., Gasc, A., Omrani, H., &amp; Pijanowski, B. C. (2019). Contributions of MIR to Soundscape Ecology. Part 2: Spectral timbral analysis for discriminating soundscape components. Ecological Informatics, 51, 114. https://doi.org/10.1016/j.ecoinf.2019.01.008 Polito, M. J., Hinke, J. T., Hart, T., Santos, M., Houghton, L. A., &amp; Thorrold, S. R. (2017). Stable isotope analyses of feather amino acids identify penguin migration strategies at ocean basin scales. Biology Letters, 13(8), 20170241. https://doi.org/10.1098/rsbl.2017.0241 5.4 Tree-based methods for classification 5.4.1 Classification and regression trees (CARTs) Tree-based methods for classification involve dividing up regions defined by the predictor variables. For example, simple species identification trees use this approach. You are looking at a plant: does it have smooth or serrated leaves? The leaf characteristic predictor is used to divide up the classification possibilities (not without errors!). So we repeatedly split the response data into two groups that are as homogeneous as possible. The split is determined by the single predictor that best discriminates among the data. The binary splits continue to partition the data into smaller and smaller groups, or nodes, until the groups are no longer homogeneous. This effort produces a single tree where the binary splits form the branches and the final groups compose the terminal nodes, or leaves. If this type of method is applied with a continuous response variable instead it is called a regression tree, if the response is categorical, it is called a classification tree. Often we refer to both techniques at the same time as Classification And Regression Trees (CARTs). When used as a regression response predictor, this technique differs from standard regression approaches which are global models where the predictive formula is supposed to hold in the entire data space. Instead trees try to partition the data space into small enough parts where we can apply a simple different model on each part. Lets try a simple example on the iris data. Well first split the data into a training and testing set, and run our classification tree algorithm using the rpart package. library(rpart) alpha &lt;- 0.7 # percentage of training set inTrain &lt;- sample(1:nrow(iris), alpha * nrow(iris)) train.set &lt;- iris[inTrain, ] test.set &lt;- iris[-inTrain, ] mytree &lt;- rpart(Species ~ ., data = train.set, method = &quot;class&quot;) par(mar = c(5, 3, 3, 3)) plot(mytree) text(mytree, cex = 1.2) Figure 5.7: A simple plot of a tree classifier for the iris data We can see that petal length is used to distinguish species I.setosa from the other two species, and then petal width classifies into I. versicolor or I. virginia. The model of course includes more information than this regarding the number of observations aggregating to each branch of the tree etc. More detailed information can be obtained from summary(mytree), or just typing mytree. A nicer plot, with more details can also be obtained with rpart.plot library. library(rpart.plot) rpart.plot(mytree, box.palette = 0) Figure 5.8: A nicer plot of a tree classifier for the iris data made with the rpart.plot package This plot (Fig. 5.8), in addition to the factor that splits each branch, also tells us the percentage of the data in each class, and the percentage that travels down each branch in each class. Starting at the top, each species makes up roughly a third of the data, after the petal length branch, travelling down the petal length greater than or equal to 2.5, all I.setosa observations all on the other side of the split, and we are left with data divided evenly between the I.versicolor and I. viriginica. The petal length &lt; 4.8 branch separates out these two species, with some error in classification. The algorithm determines which variable to split based on impurity, or how similar points are within a group. If all data points are identical, then impurity is zero. Impurity increases as points become more dissimilar. Impurity is calculated differently for different kinds of trees. For classification trees: the Gini index, which reflects the proportion of responses in each level of a categorical variable is often used. The Gini index is calculated as: \\(Gini=1-\\sum p_i\\), where \\(p_i\\) is the proportion of observations in each class. The Gini index is small when many observations fall into a single category, so the split is made at the single variable which minimizes the Gini index. Some classifiers use the Shannon-Weiner index instead, which has similar properties. Using this tree classifier, we can make predictions for our testing data, and get a confusion matrix pred &lt;- predict(mytree, test.set, type = &quot;class&quot;) confusionMatrix(pred, test.set$Species) Confusion Matrix and Statistics Reference Prediction setosa versicolor virginica setosa 16 0 0 versicolor 0 13 1 virginica 0 1 14 Overall Statistics Accuracy : 0.9556 95% CI : (0.8485, 0.9946) No Information Rate : 0.3556 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.9332 Mcnemar&#39;s Test P-Value : NA Statistics by Class: Class: setosa Class: versicolor Class: virginica Sensitivity 1.0000 0.9286 0.9333 Specificity 1.0000 0.9677 0.9667 Pos Pred Value 1.0000 0.9286 0.9333 Neg Pred Value 1.0000 0.9677 0.9667 Prevalence 0.3556 0.3111 0.3333 Detection Rate 0.3556 0.2889 0.3111 Detection Prevalence 0.3556 0.3111 0.3333 Balanced Accuracy 1.0000 0.9482 0.9500 Our accuracy is pretty good for all species, and significantly greater than the no information rate. 5.4.1.1 Tree pruning So how does the model decide when to stop? Presumably you could continue to build out the tree until every single observation is a node. Another way to phrase this question is: how do you prevent the model from overfitting the data? The answer is: pruning (the best part about this classification method is the metaphorical gardening language!). Pruning is the act of overgrowing the tree and then cutting it back. Ultimately pruning should yield a tree that optimizes the trade-off between complexity and predictive ability. Pruning begins by creating a nested series of trees of increasing number of branches, from 0 (no splits) to however many can be reasonably obtained from the data. For each number of branches, an optimal tree can be recovered, i.e., one that minimizes the overall misclassification rate. To select the tree of optimal size we use cross-validation. For a given tree size, cross-validation divides the data into equal portions, removes one portion from the data, builds a tree using the remaining portion, and then calculates the error between the observed data and the predictions. This procedure is repeated for each of the remaining portions and then the overall error is summed across all subsets of the data. This is done for each of the nested trees. The tree of optimal size is then determined based on the smallest tree that is with in 1 standard error of the minimum error observed across all trees. 5.4.2 Random Forests Even with pruning, a single CART is likely to overfit the data, particularly when there are many, many predictors, and thus is not very good for prediction. One way to get around this is to build a bunch of different, non-nested, trees on subsets of the data, and then average across them. Because any given tree is constructed with only a portion of the data, the likelihood of overfitting is drastically reduced. Moreover, averaging across many trees reduces the impact of anomalous results from a single tree. This is the idea of ensemble learning, or combining many weak learners (individual trees) to produce one strong learner (the ensemble). One type of ensemble decision tree is a random forest. Random forests are a frequently used machine learning tool that can be used for both classification and regression. Random forests use a bootstrapped sample of the data, and only a portion of the predictors to construct each tree. This procedure ensures that each individual tree is independent from the others, making it a much more accurate method than some other ensemble learning techniques. As well, since both the data and the predictors are subsampled, these models can be fit to more predictors than there are observations. This seems a little counterintuitive, but can be a real benefit for ecological data which typically suffers from low replication. Lets try an ensemble decision tree on the iris data. We will use the randomForest package. Note that we do not have to split our data into training and testing sets now, the randomForest package is already doing this sort of thing for us. library(randomForest) RF.model = randomForest(Species ~ ., data = iris) RF.model Call: randomForest(formula = Species ~ ., data = iris) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 2 OOB estimate of error rate: 4% Confusion matrix: setosa versicolor virginica class.error setosa 50 0 0 0.00 versicolor 0 47 3 0.06 virginica 0 3 47 0.06 Our classification is pretty good with a misclassification rate on the Out Of Bag (OOB) data of only 4% (this is the equivalent to the test data from a cross-validation), and errors for individual species from 0 to 0.06%. We might like to look at what the model is doing, but unlike a single CART, random forests do not produce a single visual, since of course the predictions are averaged across many hundreds or thousands of trees. When building random forests, there are three tuning parameters of interest: node size, number of trees, and number of predictors sampled at each split. Careful tuning of these parameters can prevent extended computations with little gain in error reduction. For example, the plot below (Fig. 5.9) shows how the overall OOB error rate, and the error rate for each of the three species, changes with the size of the forest (the number of trees). Figure 5.9: Out of bag error, and individual classification errors for the three species classes in the random forest model. We see large fluctuations in error at the beginning and middle which flatline as more trees are added Obviously with fewer trees the error rate is higher, but as more trees are added you can see the error rate decrease and more or less flatten out. As we can see (Fig 5.9), we could easily reduce the number of trees down to 300 and experience relatively little loss in predictive ability. This is easy to do: update(RF.model, ntree = 300) Call: randomForest(formula = Species ~ ., data = iris, ntree = 300) Type of random forest: classification Number of trees: 300 No. of variables tried at each split: 2 OOB estimate of error rate: 4.67% Confusion matrix: setosa versicolor virginica class.error setosa 50 0 0 0.00 versicolor 0 47 3 0.06 virginica 0 4 46 0.08 So we see little change in our error rate. Despite not yielding a single visualizable tree, we can get information about the random forest model. One metric is the relative importance of the predictors. By ranking predictors based on how much they influence the response, random forests may be a useful tool for selecting predictors before trying another framework, such as CART. Importance can be obtained and plotted using the varImpPlotfunction(): varImpPlot(RF.model, pch = 16, cex = 1.2) Figure 5.10: Variable importance plot for our random forest model of the iris data. The most important predictor is Petal.Width, followed by Petal.Length Variable importance reports the mean decrease in the Gini Index for each predictor (Fig. 5.10). If you recall, the Gini index is a measure of impurity for categorical data. For each tree, each predictor in the OOB sample is randomly permuted (aka, shuffled around) and passed to the tree to obtain the error rate. The error rate from the unpermuted OOB is then subtracted from the error rate on the permuted OOB data, and averaged across all trees. When this value is large, it implies that a variable had a strong relationship with the response. That is, the model got much worse at predicting the data when that variable was permuted. As we already knew, Petal.Length and Petal.Width are the two most important variables. One other useful aspect of random forests is getting a sense of the partial effect of each predictor given the other predictors in the model. This has analogues to partial correlation plots in linear models. We can construct a partial effects response by holding each value of the predictor of interest constant (while allowing all other predictors to vary at their original values), passing it through the random forest, and predicting the responses. The average of the predicted responses are plotted against each value of the predictor of interest (the ones that were held constant) to see how the effect of that predictor changes based on its value. This exercise can be repeated for all other predictors to gain a sense of their partial effects. The function to calculate partial effects in the randomForest package is partialPlot(). Lets look at the effect of Petal.Length: partialPlot(RF.model, iris, Petal.Length, main = &quot;&quot;, ylab = &quot;log odds&quot;, cex = 1.2, cex.lab = 1.3, lwd = 2) Figure 5.11: Partial effect of petal length in the random forest model of the iris data The y-axis is a bit tricky to interpret (Fig. 5.11). Since we are dealing with classification trees, y on the logit scale, and is the probability of success. In this case, the partial plot has defaulted to the first class, which is I. setosa. The plot says that there is a high chance of successfully predicting this species from Petal.Length when Petal.Length is less than around 2.5 cm, after which point the chance of successful prediction drops off precipitously. This is actually quite reassuring as this is the first split identified way back in the very first CART (where the split was &lt; 2.45 cm). Missing data Its worth noting that the default behavior of randomForest is to refuse to fit trees with missing predictors. You can, however, specify a few alternative arguments: the first is na.action = na.omit, which removes the rows with missing values outright. Another option is to use na.action = na.roughfix, which replaces missing values with the median (for continuous variables) or the most frequent level (for categorical variables). Missing classifications are harder: you can either remove that row, or use the function rfImpute() to impute values. The imputed values are the average of the non-missing observations, weighted by their proximity to non-missing observations (based on how often they fall in terminal nodes with those observations). rfImpute tends to give optimistic estimates of the OOB error. Exercise 4 See if you can construct a random forest that predicts diabetes in patients based on other indicators using the Pima.tr data as your training set and Pima.te as your test set. Report on the performance of your classifier. 5.4.3 Examples of tree-based methods used for classification Bertsimas, D., Dunn, J., Steele, D. W., Trikalinos, T. A., &amp; Wang, Y. (2019). Comparison of Machine Learning Optimal Classification Trees With the Pediatric Emergency Care Applied Research Network Head Trauma Decision Rules. JAMA Pediatrics, 173(7), 648656. https://doi.org/10.1001/jamapediatrics.2019.1068 Ghiasi, M. M., &amp; Zendehboudi, S. (2021). Application of decision tree-based ensemble learning in the classification of breast cancer. Computers in Biology and Medicine, 128, 104089. https://doi.org/10.1016/j.compbiomed.2020.104089 Kruk, C., Devercelli, M., Huszar, V. L. M., Hernández, E., Beamud, G., Diaz, M., Silva, L. H. S., &amp; Segura, A. M. (2017). Classification of Reynolds phytoplankton functional groups using individual traits and machine learning techniques. Freshwater Biology, 62(10), 16811692. https://doi.org/10.1111/fwb.12968 5.4.4 R functions tree() (tree library) produces CARTs rpart() (rpart) CARTs randomforest() (randomforest) produces ensemble trees ranger() (ranger) is a faster implementation of the random forest algorithm 5.5 Artificial Neural Networks (ANN) Artificial neural networks (ANN) are another machine learning tool that can be used for both classification and regression. They can be viewed as analogous to human nervous system, in the sense that a neural network is made up of interconnected information processing units. Like tree-based methods, artificial neural networks learn classification models by processing previously categorized examples (supervised learning). An artificial neural network is made up of artificial neurons. Each neuron in the network takes inputs, processes them, passes the processed information through a nonlinear function that converts the prediction to the desired type of output and finally gives the result (Fig. 5.12). Figure 5.12: Diagram showing the inputs, and internal components, including weighting, summation and activation function, that make up an aritifcial neuron Some of the information will be more important for producing the correct output, the machine learning algorithm then weights this type of information more heavily. During the process of training, the ANN determines the error between the prediction and the target output. The network then adjusts its the weight it gives to each type of information. Successive adjustments are made using a learning rule and will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria (see below for more discussion about learning). Once an ANN model has been fitted, to predict classification of novel data it: Weights the input Multiplies all the inputs, x, by their weights, w Adds all the multiplied values to get a weighted sum, and then applies an activation function to the weighted input The activation functions are used to map the input between the required values. For example, between 0 and 1 for probability predictions. There are many functions used such as sigmoid, tan hyperbolic, linear etc. I havent shown it here, but a bias can also be applied to the data which allows you to shift the activation function curve up or down. Of course, like a human brain, there is more than on neuron in a network, so we have have a layer of neurons. In Fig 5.13 we have a feedforward neural network where the information moves only in one direction, forward from input to output. The ANN consists of 2 inputs, 1 hidden layer with 3 hidden neurons, and 1 final output. The layers between input and output are called hidden because you do not see what they are doing, even though this is where all the processing happens. We could have decided to add another intermediate hidden layer between the input and the final output exactly in the same way. Then we would have had a neural network with 2 hidden layers. Or we could have chosen to stay with 1 hidden layer but to have more neurons in it (5 instead of 3, for example).The parallel processing of information by many neurons allows ANNs to deal with non-linearity easily. Not all the neural networks fit the template described above. There are different kinds of architectures, but the feedforward neural network (also called Multi Layer Perceptron) is really the first basic architecture to understand. Figure 5.13: Diagram of an artificial neural network. A box labelled input contains two circles that represent different data, which are connected to three neurons represented as circles in the box labelled hidden layer. The neurons point to the output 5.5.1 ANN learning Once defined, the model still need to be fitted (i.e., the weights should be adjusted, based on the data, to minimize some error function) and that is a really difficult optimization task to complete. There are many learning rules that are used with neural networks such as: least mean square error, gradient descent, newtons rule, conjugate gradient and others. So during the model fitting process the algorithm sets the initial weights of the network randomly (given some well chosen distribution), and then tries to adjust the weights (and other model parameters such as bias and intercepts) to minimize error. A common optimization method is backward error propagation using gradient descent. In this method, the ANN applies a gradient descent over all these parameters to iteratively improve (reduce) the error metric. As its name suggests, gradient descent involves calculating the gradient of the target function, which is a vector of partial derivatives with respect to input variables. You may recall from calculus that the first-order derivative of a function gives the slope of a function at a given point. The gradient descent algorithm uses this calculation to select new weights for each input variable that result in a lower error. The step size, sometimes called the learning rate, is used to control how much to change each input variable with respect to the gradient. This process is repeated until the minimum of the target function is located, a maximum number of candidate solutions are evaluated, or some other stop condition. The backpropagation algorithm consists in using the layered structure of the ANN to make the computation of derivatives for gradient descent more efficient. The backpropagation algorithm was originally introduced in the 1970s, but its importance wasnt fully appreciated until Rumelhart et al. (1986) described how backpropagation works far faster than earlier approaches to learning, making it possible to use neural nets to solve problems which had previously been insoluble. Backpropagation will give us an expression for the partial derivative of the error with respect to any weight w (or bias b) in the network. The expression tells us how quickly the error changes when we change the weights and biases. Im not going to give the mathematical details here, but briefly, to implement this method, the algorithm first completes a forward pass through the network, which is just a fancy way of saying the model makes a prediction. The error between the prediction and the expected value is calculated. Then, going backwards, through the network, the algorithm finds the partial derivative of the error with respect to the weights from each neuron in the hidden layer. It changes the weights a little bit in a way that makes the prediction slightly closer to the true value (through a gradient descent approach). It then repeat this process as long as it can make little changes to the weights that improve the result. To read more about neural net learning, and backwards propagation in particular, take a look at http://neuralnetworksanddeeplearning.com/. 5.5.2 Our first ANN Well get started trying to use this machine learning technique using our old friend the iris data and the package neuralnets. We divide the data into training and test sets, and then run the neural net algorithm from the neuralnet package. Lets use two inputs, three neurons and standard backward error propagation just like in Fig. 5.13. Well also need to set the learning rate to some small value. Our output will be the classification of one of the three species. When you are done, take a look at the output produced by the algorithm, and plot the ANN using the plot function. What is the meaning of the numbers on the diagram? # sample the data to divide into training and test train &lt;- sample(x = nrow(iris), size = nrow(iris) * 0.5) iristrain &lt;- iris[train, ] iristest &lt;- iris[-train, ] # Run the multiclass neural net to produce a classifier library(neuralnet) nniris &lt;- neuralnet(Species ~ Petal.Width + Petal.Length, iristrain, hidden = 3, linear.output = FALSE, algorith = &quot;backprop&quot;, learningrate = 0.01) # rounding the weight values so the plot is tidier nniris$weights[[1]] &lt;- lapply(nniris$weights[[1]], function(x) round(x, 1)) # plotting the neural net (but not the intercepts or error # rates) plot(nniris, rep = &quot;best&quot;, information = FALSE, intercept = FALSE) Figure 5.14: A neural network selected by our algorithm to classify the iris data. Two data inputs of Petal.Width and Petal.Length are connected to three neurons, which together work to classify the data. The numbers on connecting arrows represent the weight attributed to that connection Finally, well generate a confusion matrix for our neural net to see how well it did. The compute() function gives us the probability of class membership, and well just select the species with the highest probability. setosa versicolor virginica setosa 25 0 0 versicolor 0 22 0 virginica 0 7 21 Overall, not too bad. Pretty comparable to the other classifiers we have tried out. 5.5.3 Examples of ANNs used for classification Bewes, J., Low, A., Morphett, A., Pate, F. D., &amp; Henneberg, M. (2019). Artificial intelligence for sex determination of skeletal remains: Application of a deep learning artificial neural network to human skulls. Journal of Forensic and Legal Medicine, 62, 4043. https://doi.org/10.1016/j.jflm.2019.01.004 Krtolica, I., Cvijanovi, D., Obradovi, ., Novkovi, M., Miloevi, D., Savi, D., Vojinovi-Miloradov, M., &amp; Radulovi, S. (2021). Water quality and macrophytes in the Danube River: Artificial neural network modelling. Ecological Indicators, 121, 107076. https://doi.org/10.1016/j.ecolind.2020.107076 5.5.4 R functions nnet() (nnet package) is good for a simple neural network with just a single hidden layer neuralnet() (neuralnet) has a faster version of backwards propagation Packages for deep learning applications that use complex neural networks include MXNet, darch, deepnet, and h2o plotnet() (NeuralNetTools) implements tools for visualization and understanding as described by Olden &amp; Jackson (2002) 5.5.5 What else? Weve just provided a small sampler of classification methods here that will get you started. But other methods k-nearest neighbors, and Support Vector Machines (SVM) are also useful for building classification models. 5.5.6 Feedback We value your input! Please take the time to let us know how we might improve these materials. Survey 5.6 References Chawla, N. V., Bowyer, K. W., Hall, L. O., &amp; Kegelmeyer, W. P. (2002). SMOTE: Synthetic Minority Over-sampling Technique. Journal of Artificial Intelligence Research, 16, 321357. https://doi.org/10.1613/jair.953 Fernández, A., García, S., Galar, M., Prati, R. C., Krawczyk, B., &amp; Herrera, F. (2018). Learning from imbalanced data sets (Vol. 10). Springer. https://link.springer.com/content/pdf/10.1007/978-3-319-98074-4.pdf Lunardon, N., Menardi, G., &amp; Torelli, N. (2014). ROSE: A Package for Binary Imbalanced Learning. The R Journal, 6(1), 79. https://doi.org/10.32614/RJ-2014-008 Olden, J. D., &amp; Jackson, D. A. (2002). Illuminating the black box: A randomization approach for understanding variable contributions in artificial neural networks. Ecological Modelling, 154(1), 135150. https://doi.org/10.1016/S0304-3800(02)00064-9 Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323(6088), 533536. https://doi.org/10.1038/323533a0 5.7 Answer Key Exercise 1. Once you have fitted your model, you can use the code p18 = round(1/(1 + exp(-(logcoefs[1, 1] + logcoefs[2, 1] * 18))), 2) to get the probability of occupancy at locations with a mean annual temperature of 18°C of approximately 0.91. Your value will vary slightly depending on the stochasticity from the random routines used to generate our simulated observations. Exercise 2. We first need to change the threshold for our classification, and then recalculate sensitivity and specificity for our results # determine model predictions based on a new threshold # value logoccnew &lt;- ifelse(glm.probs &gt; 0.45, 1, 0) # Calculate a new confusion matrix ctabnew = table(logoccnew, y) dimnames(ctabnew) &lt;- list(Actual = c(0, 1), Predicted = c(0, 1)) ctabnew Predicted Actual 0 1 0 124 28 1 21 27 errN = (ctabnew[1, 2] + ctabnew[2, 1])/sum(ctabnew) sensN = ctabnew[2, 2]/(ctabnew[2, 1] + ctabnew[2, 2]) specN = 1 - ctabnew[1, 2]/(ctabnew[1, 1] + ctabnew[1, 2]) Again, there will be a bit of variation in your exact result. We obtain a sensitivity of 0.56 and specificity of 0.82 Exercise 3. To construct a logistic regression to determine if a patient has diabetes or not, first read in the the training dataset Pima.tr from the MASS package library(MASS) data(Pima.tr) str(Pima.tr) &#39;data.frame&#39;: 200 obs. of 8 variables: $ npreg: int 5 7 5 0 0 5 3 1 3 2 ... $ glu : int 86 195 77 165 107 97 83 193 142 128 ... $ bp : int 68 70 82 76 60 76 58 50 80 78 ... $ skin : int 28 33 41 43 25 27 31 16 15 37 ... $ bmi : num 30.2 25.1 35.8 47.9 26.4 35.6 34.3 25.9 32.4 43.3 ... $ ped : num 0.364 0.163 0.156 0.259 0.133 ... $ age : int 24 55 35 26 23 52 25 24 63 31 ... $ type : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 1 1 2 ... Next construct a logistic regression on the training data # run logistic regression Pima.log &lt;- glm(type ~ ., family = binomial, data = Pima.tr) Then get the predictions of the model for the testing data. Weve just used a threshold of 50%. testPima &lt;- predict(Pima.log, newdata = Pima.te, type = &quot;response&quot;) testPima = as.factor(ifelse(testPima &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;)) Finally create a confusion matrix # Calculate a confusion matrix atab = table(Pima.te$type, testPima) dimnames(atab) &lt;- list(Actual = levels(Pima.te$type), Predicted = levels(Pima.te$type)) atab Predicted Actual No Yes No 200 23 Yes 43 66 You should notice there there have been some misclassifications at 50%, and that the accuracy is only about 0.8. See if another decision boundary (e.g., 75%) does any better, or use the ROCit library, or other R packages to calculate an optimal threshold or an overall performance across thresholds. Exercise 4. Weve used the caret package to run our random forest to classify the Pima diabetes data using a k-fold cross-validation procedure, given that the dataset is fairly small. Weve even divided the data into training and testing data (i.e., this is a bit overkill, considering that random forest will calculate out of bag error all by itself). We use the fitted model to predict the test data, and then generate a confusion matrix to take a look at how well it does. We also examine which predictors are most important. library(randomForest) library(caret) # cross-validation with 5 folds train_control &lt;- trainControl(method = &quot;cv&quot;, number = 5) rf_Pima &lt;- train(type ~ ., data = Pima.tr, trControl = train_control, method = &quot;rf&quot;) rf_Pima Random Forest 200 samples 7 predictor 2 classes: &#39;No&#39;, &#39;Yes&#39; No pre-processing Resampling: Cross-Validated (5 fold) Summary of sample sizes: 161, 159, 161, 160, 159 Resampling results across tuning parameters: mtry Accuracy Kappa 2 0.7093621 0.3148905 4 0.6997405 0.3057397 7 0.7094903 0.3271013 Accuracy was used to select the optimal model using the largest value. The final value used for the model was mtry = 7. rf_Pima$finalModel Call: randomForest(x = x, y = y, mtry = min(param$mtry, ncol(x))) Type of random forest: classification Number of trees: 500 No. of variables tried at each split: 7 OOB estimate of error rate: 28.5% Confusion matrix: No Yes class.error No 107 25 0.1893939 Yes 32 36 0.4705882 rf_Pima_pred = predict(rf_Pima, Pima.te) confusionMatrix(rf_Pima_pred, Pima.te$type) Confusion Matrix and Statistics Reference Prediction No Yes No 185 43 Yes 38 66 Accuracy : 0.756 95% CI : (0.7062, 0.8013) No Information Rate : 0.6717 P-Value [Acc &gt; NIR] : 0.0005027 Kappa : 0.4403 Mcnemar&#39;s Test P-Value : 0.6567213 Sensitivity : 0.8296 Specificity : 0.6055 Pos Pred Value : 0.8114 Neg Pred Value : 0.6346 Prevalence : 0.6717 Detection Rate : 0.5572 Detection Prevalence : 0.6867 Balanced Accuracy : 0.7176 &#39;Positive&#39; Class : No varImpPlot(rf_Pima$finalModel, main = &quot;final random forest model from caret&quot;, cex = 1.2, pch = 16) Figure 5.15: Variable importance plot from the final random forest model selected by caret for the Pima.tr dataset. The variable labelled glu, which stands for glucose, is most important "],["optimization.html", "6 Optimization 6.1 Introduction 6.2 Fundamentals of Optimization 6.3 Regression 6.4 Iterative Optimization Algorithms 6.5 Calibration of Dynamic Models 6.6 Uncertainty Analysis and Bayesian Calibration 6.7 References 6.8 Answer Key", " 6 Optimization 6.1 Introduction To improve is to make better; to optimize is to make best. Optimization is the act of identifying the extreme (cheapest, tallest, fastest) over a collection of possibilities. Optimization over design space (also called decision space) is a critical feature of many engineering tasks, and has a role in most areas of applied science, including biology. Examples include optimal manipulation of biological systems (e.g. optimal harvesting or optimal drug dosing) or optimal design of biological systems (e.g. robust synthetic genetic circuits). A complementary task is optimal experimental design, which aims to identify the best experiment from a collection of possibilities. Model calibration, to be discussed further below, provides another example; here we seek the best fit version of a model from a collection of possible options. Optimization is used to investigate natural systems (i.e. in pure science) in cases where nature presents us with an optimal version of a given phenomenon. For example, the principle of energy minimization justifies a wide variety of phenomena, from the shapes of soap bubbles to the configuration of proteins. Darwinian evolution provides another example. Assuming evolution has arrived at optimal designs, we can apply optimization to understand a variety of biological phenomena, from metabolic flux distribution, to brain wiring, to foraging strategies. This module introduces optimization methods in R. Although simple optimization tasks, such as those addressed by introductory calculus, can be accomplished with paper-and-pencil calculations, most optimization tasks of interest in biology demand computational techniques. 6.2 Fundamentals of Optimization Figure 1 illustrates some basics terminology associated with optimization. The graph of a function \\(f\\) of a single variable \\(x\\) is shown, defined over a domain \\([a,b]\\). In the context of optimization, we can think of each \\(x\\)-value in the interval \\([a,b]\\) as one possible scenario (e.g. enzyme activity, foraging rate, etc.). The function \\(f\\) maps those scenarios to some objective (e.g. metabolic flux, fitness) to be optimized (either maximized or minimized). The global extrema (i.e. maximum and minimum) represent the goals of optimization. Figure 6.1: Extreme Values More generally, we define local extrema (maxima and minima) as cases that appear to be optimal if we restrict attention only to nearby possibilities (i.e. \\(x\\)-values). Most of the theory of optimization methods is dedicated to identifying local optima. These approaches cannot directly identify a global optimum  instead they identify candidate global optima, which then can be compared to one another to identify the best result. (A convenient special case occurs for problems where every local optimum is a global optimum; these are called convex optimization problems. Unfortunately, they typically occur only as special cases when addressing biological phenomena.) 6.2.1 Fermats Theorem To illustrate these fundamentals, consider the following two academic examples that rely on basic calculus, specifically on Fermats Theorem, which states that local extrema occur at points where the tangent line to a functions graph is horizontal, i.e. at points where the derivative (the slope of the tangent) is zero (Figure 2). Figure 6.2: Fermats Theorem: local extrema occur at points where the tangent line is horizontal or at endpoints of the domain. Some local extrema are also global extrema. 6.2.1.1 Example 1 Task: Identify the value of \\(x\\) for which the function \\(f(x) = x^2+3x-2\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 2x+3\\). To determine the point(s) where the derivative is zero, we solve: \\[f&#39;(x) = 0 \\ \\ \\Leftrightarrow \\ \\ 2x+3 = 0 \\ \\ \\Leftrightarrow \\ \\ x = -\\frac{3}{2} = -1.5\\] As shown in Figure 3, the single local minimum is the global miminum in this case, so \\(x=-1.5\\) is the desired solution Figure 6.3: Graph of \\(f(x) = x^2+3x-2\\) (Example 1) 6.2.1.2 Example 2 Task: Identify the value of \\(x\\) for which the function \\(f(x) = 3x^4-4x^3-54x^2+108x\\) is minimized. Solution: Taking the derivative, we find \\(f&#39;(x) = 12x^3-12x^2-108x+108 = 12(x-3)(x-1)(x+3)\\). To determine the point(s) where the derivative is zero, we solve: \\[f&#39;(x) = 0 \\ \\ \\Leftrightarrow \\ \\ x = 3, 1, -3\\] As shown in Figure 4, two of these points are local minima and one is a local maximum. Of the two local minima, \\(x=-3\\) is where the global minimum occurs. Figure 6.4: Graph of \\(f(x) = 3x^4-4x^3-54x^2+108x\\) (Example 2) 6.3 Regression 6.3.1 Linear Regression As mentioned above, finding the best fit from a family of models is a common optimization task in science. The simplest example is linear regression: the task of determining a line of best fit through a given set of points. To illustrate, consider the dataset of \\((x,y)\\) pairs shown in Figure 5 below, which we can label as \\((x_1, y_1)\\), \\((x_2, y_2)\\), \\(\\ldots\\), \\((x_N, y_N)\\). Several lines are displayed. The optimization task is to identify the best line: the one that best captures the trend in the data. Figure 6.5: Finding a line of best fit To specify this task mathematically, we need to decide on a measure of quality of fit. We start by recognizing that the line will (typically) fail to pass through most of the points in the dataset. Thus, at each point \\(x_i\\) we can define an error, which is the difference between the observed \\(y\\)-value and the model prediction, i.e. the \\(y\\)-value on the line. If we specify the line as \\(y=mx+b\\), then the error at \\(x_i\\) can be defined as \\(e_i = y_i-(mx_i+b)\\). We then need to combine these errors into a single quality-of-fit measure. This is typically done by squaring the errors and adding them together. (Squaring ensures that both under- and over-estimations contribute equally.) We define the sum of squared errors (SSE) as : \\[\\mbox{SSE:} \\ \\ \\ e_1^2+e_2^2+ \\cdots e_N^2 \\ \\ = \\ \\ (y_1-(mx_1+b))^2 + (y_2-(mx_2+b))^2 + \\cdots +(y_N-(mx_N+b))^2. \\] We can now pose the model-fitting task as an optimization problem. For each line (that is, each assignment of numerical values to \\(m\\) and \\(b\\)), we associate a corresponding SSE. We seek the values of \\(m\\) and \\(b\\) for which the SSE is a global minimum. 6.3.1.1 Example 3 Consider a simplified version of linear regression, in which we know that our model (line) should pass through the origin (0,0). That is, instead of lines \\(y=mx+b\\), we will consider only lines of the form \\(y=mx\\). We thus have a single parameter value to determine: the slope \\(m\\). To keep the algebra simple well take a tiny dataset consisting of just two points: \\((x_1, y_1) = (2,3)\\) and \\((x_2, y_2)= (5,4)\\), as indicated in Figure 6, below. The line passes through points \\((2,2m)\\) and \\((5, 5m)\\). Figure 6.6: Finding a line of best fit through the origin (Example 3) In this simple case, the sum of squared errors is: \\[\\begin{equation*} \\mbox{SSE} = \\mbox{SSE}(m)= e_1^2+e_2^2 = (3-2m)^2+(4-5m)^2 \\end{equation*}\\] To apply Fermats theorem we take the derivative and identify any values of \\(m\\) for which the derivative is zero: \\[\\begin{eqnarray*} \\frac{d}{dm} \\mbox{SSE}(m) &amp;=&amp; 2(3-2m)(-2)+2(4-5m)(-5) \\\\ &amp;=&amp; -4(3-2m) - 10(4-5m) = -12 +8m+-40+50m = -52+58m. \\end{eqnarray*}\\] The only local extremum (and hence the only candidate for global minimum) is \\(m=52/58 = 26/29\\). We conclude that the best fit line is \\(y=\\frac{26}{29}x\\). The analysis in Example 3 can be extended to determine the general solution of the linear regression task. For details, see, e.g. (Fairway, 2002). The solution formula is as follows: Linear regression formula The best fit line \\(y=mx+b\\) to the dataset \\((x_1, y_1)\\), \\((x_2, y_2)\\), \\(\\ldots\\), \\((x_n, y_n)\\) is given by \\[\\begin{eqnarray*} m&amp;=&amp; \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_{i=1}^n (x_i-\\bar{x})^2} \\\\ b&amp;=&amp; \\bar{y}-m\\bar{x}, \\\\ \\end{eqnarray*}\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are the averages \\[\\begin{eqnarray*} \\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\bar{y}=\\frac{1}{n} \\sum_{i=1}^n y_i \\end{eqnarray*}\\] This formula is somewhat unwieldy, but is straightforward to implement. In R, the command lm implements this formula, as the following example illustrates. 6.3.1.2 Example 4 Well work with a dataset called iris that is included in R, shown below (via the plot command). plot(iris$Sepal.Length ~ iris$Petal.Width, xlab = &quot;Petal Width&quot;, ylab = &quot;Sepal Length&quot;) Figure 6.7: Sepal Length vs Petal Width from the iris dataset The lm command applies the formula described above to arrive at the line of best fit (i.e. the minimizer of the sum of squared errors). The command below generates the parameters (\\(m\\) and \\(b\\)) that specify the line \\(y=mx+b\\). lm(iris$Sepal.Length ~ iris$Petal.Width) Call: lm(formula = iris$Sepal.Length ~ iris$Petal.Width) Coefficients: (Intercept) iris$Petal.Width 4.7776 0.8886 The best-fit line has intercept \\(b=4.7776\\) and slope is \\(m=0.8886\\). We can plot the data together with the line of best fit with the abline command: plot(iris$Sepal.Length ~ iris$Petal.Width, xlab = &quot;Petal Width&quot;, ylab = &quot;Sepal Length&quot;) abline(lm(iris$Sepal.Length ~ iris$Petal.Width)) Figure 6.8: Sepal Length vs Petal Width from the iris dataset, with best-fit line 6.3.2 Nonlinear Regression Nonlinear regression is the task of fitting a nonlinear model (e.g. a curve) to a dataset. The setup for this task is identical to linear regression. We begin by selecting a parameterized family of models (i.e. curves), and aim to identify the model (i.e. curve) that minimizes the sum of squared errors when compared to the data. The choice of model family is typically based on the mechanism that produced the data. For example, if we are investigating the rate law for a single-substrate enzyme-catalysed reaction, we might choose the family of curves specified by Michaelis-Menten kinetics, which relate reaction velocity \\(V\\) to substrate concentration \\(S\\): \\[\\begin{eqnarray*} V = \\frac{V_{\\mbox{max}} S}{K_M+S} \\end{eqnarray*}\\] Our goal would then be to identify values for the parameters \\(V_{\\mbox{max}}\\) and \\(K_M\\) that minimize the sum of squared errors when comparing with observed data. Regression via linearizing transformation: In several important cases, the nonlinear regression task can be transformed into a linear regression task. In the case of Michaelis-Menten kinetics, several linearizing transformations have been proposed, e.g. EadieHofstee and LineweaverBurk (Cho and Lim, 2018). Another common example is fitting exponential curves (to describe e.g. population growth or drug clearance). In those cases, application of a logarithm transforms the data so that a linear trend is captured. For example, the relationship \\(y = e^{rt}\\) becomes, after applying a logarithm, \\(Y = \\ln(y) = \\ln (e^{rt})= rt\\). Linear regression on the transformed data \\((t_i, Y_i)\\) then provides an estimate of the value of \\(r\\). Unfortunately, linearizing transformations are only available in a handful of special cases. Moreover, they often introduce biases that can make interpretation of the resulting model difficult. In general, the nonlinear regression task must be addressed directly. An example of the general procedure in R follows. 6.3.2.1 Example 5 We begin by defining a dataset against which we will fit a Michaelis-Menten curve. # substrate concentrations S &lt;- c(4.32, 2.16, 1.08, 0.54, 0.27, 0.135, 3.6, 1.8, 0.9, 0.45, 0.225, 0.1125, 2.88, 1.44, 0.72, 0.36, 0.18, 0.9, 0) # reaction velocities V &lt;- c(0.48, 0.42, 0.36, 0.26, 0.17, 0.11, 0.44, 0.47, 0.39, 0.29, 0.18, 0.12, 0.5, 0.45, 0.37, 0.28, 0.19, 0.31, 0) MM_data = cbind(S, V) plot(MM_data) Figure 6.9: Reaction velocity against substrate concentration dataset The nls command can be used for nonlinear regression in R. Like the lm command, the nls function takes as inputs the dataset and the model. In addition, nls requires that the user provide guesses for the values of the parameters to be estimated. In this case, we can roughly estimate \\(V_{\\mbox{max}} \\approx 0.5\\) as the maximal \\(V\\)-value that can be achieved, and \\(K_M \\approx 0.3\\) as the \\(S\\)-value at which \\(V\\) reaches half of its maximal value. MMmodel.nls &lt;- nls(V ~ Vmax * S/(Km + S), start = list(Km = 0.3, Vmax = 0.5)) #perform the nonlinear regression analysis params &lt;- summary(MMmodel.nls)$coeff[, 1] #extract the parameter estimates params Km Vmax 0.4187090 0.5331688 We thus arrive at parameter estimates that characterize the best-fit model: \\(K_M = 0.4187090\\) and \\(V_{\\mbox{max}} = 0.5331688\\)). Further details on using the nls command can be found in (Ritz and Streibig, 2008). We can now plot the best fit model along with the dataset: plot(MM_data) curve((params[2] * x)/(params[1] + x), from = 0, to = 4.5, add = TRUE, col = &quot;firebrick&quot;) Figure 6.10: Reaction velocity against substrate concentration dataset, with best-fit line From the implementation of nls, the reader might have the mistaken impression that nonlinear regression and linear regression are very similar tasks. Although the problem set-up is similar in both cases (chose model type, then minimize SSE), the strategy for optimization is very different. (The first hint of this is the need to provide a set of guesses to nls.) As we saw above, the solution to the linear regression task can be stated as a general formula. For nonlinear regression, no such formula exists. Worse, as well discuss in the next section there is no procedure (algorithm) that is guaranteed to find the solution! 6.4 Iterative Optimization Algorithms The best techniques for addressing the general nonlinear regression task are iterative global optimization routines. As well discuss below, these algorithms start with a guess for the parameter values and then take steps through parameter space to improve the quality of that estimate. In the exercise above, the nls command executed this kind of algorithm, which is why it requires that the user supply an initial guess. 6.4.1 Gradient Descent A simple iterative optimization algorithm is gradient descent, which can be understood intuitively via a thought experiment. Imagine finding your way to a valley bottom in a thick fog. The fog obscures your vision so that you can only discern changes in elevation in your immediate vicinity. To make your way to the valley bottom, it would be reasonable to take each step of your journey in the direction of steepest decline. This strategy is guaranteed to lead to a local minimum, but cannot guarantee arrival at the lowest point: the global minimum. That is, the search may lead to a local minimum at which there is no direction of local descent, and so the algorithm gets `stuck. Mathematically, the local change in elevation is determined by evaluating the objective function at points near the current estimate to determine the direction of steepest descent. (Technically, this involves a linearization of the function at the current position, or equivalently a calculation of the gradient vector). A step is then taken in this direction, and the process is repeated from this updated estimate. To implement this algorithm, a number of details have to be specified: how long should each step be? How many steps should be taken? Or should there be some other termination condition that will trigger the end of the journey? (Each of these decisions involve a trade-off between precision and computation time. For instance, taking very small steps will guarantee a smooth path down the steepest route, but might take a great many step to complete the journey.) Termination conditions are often specified in terms of the local topography: the algorithm stops when the current estimate is at a sufficiently flat position (no downhill direction detected). To illustrate the gradient descent approach, consider the following algorithm, which incorporates two termination conditions: a maximum number of allowed iterations and a threshold for shallowness. library(numDeriv) # contains functions for calculating gradient # define function that implements gradient descent. Inputs # are the objective function f, the initial parameter # estimate x0, the size of each step, the maximum number of # iterations to be applied, and a threshold gradient below # which the landscape is considered flat (at which # iterations are terminated) grad.descent = function(f, x0, step.size = 0.05, max.iter = 200, stopping.deriv = 0.01, ...) { n = length(x0) # record the number of parameters to be estimated (i.e. the dimension of the parameter space) xmat = matrix(0, nrow = n, ncol = max.iter) # initialize a matrix to record the sequence of estimates xmat[, 1] = x0 # the first row of matrix xmat is the initial &#39;guess&#39; for (k in 2:max.iter) { # loop over the allowed number of steps Calculate # the gradient (a vector indicating steepness and # direct of greatest ascent) grad.current = grad(f, xmat[, k - 1], ...) # Check termination condition: has a flat valley # bottom been reached? if (all(abs(grad.current) &lt; stopping.deriv)) { k = k - 1 break } # Step in the opposite direction of the gradient xmat[, k] = xmat[, k - 1] - step.size * grad.current } xmat = xmat[, 1:k] # Trim any unused columns from xmat return(list(x = xmat[, k], xmat = xmat, k = k)) } 6.4.1.1 Example 6 Well first demonstrating the performance of this algorithm on a simple function with a single local minimum. (Here and below, code for generating plots is suppressed to avoid clutter. All code can be accessed in the .rmd file posted at the associated github repository.) Paraboloid = function(x) { return((x[1] - 3)^2 + 1/3 * (x[2])^2 + 2) } Figure 6.11: Surface plot of a paraboloid In this case, we expect that the minimum (valley bottom) will be reached from any initial guess. Starting from four distinct points, the algorithm follows the paths shown below. # define a set of initial guess values x0 = c(-1.9, -1.9) x1 = c(-3, 1.1) x2 = c(5, -5) x3 = c(5, 7) # run the gradient descent algorithm from each gd = grad.descent(simpleFun, x0, step.size = 1/3) gd1 = grad.descent(simpleFun, x1, step.size = 1/3) gd2 = grad.descent(simpleFun, x2, step.size = 1/3) gd3 = grad.descent(simpleFun, x3, step.size = 1/3) The table below shows the estimates and the corresponding objective values reached by gradient descent starting from those four points. Each run has arrived at essentially the same point: \\((3, 0)\\), where the function reaches its mimimum value of 2. x1 x2 objective value x0_out 3 -0.01246994 2.000052 x1_out 3 0.01193417 2.000047 x2_out 3 -0.01200889 2.000048 x3_out 3 0.01307634 2.000057 6.4.1.2 Example 7 Next, lets consider a function that has multiple local minima. The second plot below is interactive, allowing you to rotate the surface. Example_7_function = function(x) { return((1/2 * x[1]^2 + 1/4 * x[2]^2 + 3) + cos(2 * x[1] + 1 - exp(x[2]))) } Figure 6.12: Surface plot with multiple minima Figure 6.13: Interactive surface plot with multiple minima Again, well apply gradient descent from a set of initial guess positions: # define a set of initial guess values x0 = c(0.5, 0.5) x1 = c(-0.1, -1.3) x2 = c(-1.5, 1.3) x3 = c(-1.2, -1.4) x4 = c(-0.5, -0.5) # run the gradient descent algorithm from each gd0 = grad.descent(Example_7_function, x0, step.size = 0.01, max.iter = 1000) gd1 = grad.descent(Example_7_function, x1, step.size = 0.01, max.iter = 1000) gd2 = grad.descent(Example_7_function, x2, step.size = 0.01, max.iter = 1000) gd3 = grad.descent(Example_7_function, x3, step.size = 0.01, max.iter = 1000) gd4 = grad.descent(Example_7_function, x4, step.size = 0.01, max.iter = 1000) From some initial guess values, the algorithm successfully reaches the global minimum. From others, it gets stuck at a local minimum. The plot below shows the paths followed by the algorithm, leading to two separate valley bottoms. Figure 6.14: Surface plot with multiple minima and search paths The table below shows the final estimate and the corresponding objective value reached by gradient descent starting from those five starting points. x1 x2 objective value x0_out 1.0697802 -0.5784429 2.810101 x1_out 1.0633307 -0.6030409 2.810100 x2_out -0.3775909 1.1636290 2.426837 x3_out -0.3776268 1.1636047 2.426838 x4_out -0.3776816 1.1635677 2.426839 6.4.1.3 Exercise 1 Consider the following dataset, which corresponds to measurements of drug concentration in the blood over time. try fitting the data to an exponential \\(C(t) = a e^{-r t}\\). Youll find that the best-fit model is not satisfactory. Next, try fitting a biexponential \\(C(t) = a_1 e^{-r_1 t} + a_2 e^{-r_2 t}\\). Youll find a more suitable agreement. For fitting, you can either use the nls command or the gradient descent function above (along with a sum of squared errors function). For nls, you may find the algorithm is very sensitive to your choice of initial guess (and will fail if the initial guess is not fairly accurate). For gradient descent, youll need to use a small stepsize, e.g. \\(10^{-5}\\), and a large number of iterations. t &lt;- c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100) C &lt;- c(5.585, 4.392, 3.2564, 2.9971, 2.6493, 2.3863, 2.0838, 2.0862, 1.7009, 1.7339, 1.4162) BE_data = cbind(t, C) plot(BE_data) Figure 6.15: Drug concentration data for exercise 1 6.4.2 Global Optimization The default optimization algorithm used by nls is the Gauss-Newton method, which is a generalization of Newtons method. (You may recall using Newtons method to solve nonlinear equations in an introductory calculus course.) Gauss-Newton is a refinement of gradient descent in which the local curvature of the function is used to predict the position of the bottom of the local valley. Both gradient descent and the Gauss-Newton method are designed to reach the bottom of the valley in which the initial guess lies. If thats not the global minimum, then the algorithm will not be successful. So, how does one select a good initial guess? Unfortunately, theres no general answer to this question. In some cases, one can use previous knowledge of the system to identify a solid initial guess. If no such previous knowledge is available, sometimes a wild guess is all we have. In those cases, we may have little confidence that the algorithm will arrive at a global minimum. The simplest way to improve the chance of achieving a global minimum is the multi-start strategy: choose many initial guesses, and run the algorithm from each (as in Example 7 above). This can be computationally expensive, but if the initial guesses are spread widely over the parameters space, one can expect that the global minimum will be achieved. A number of methods have been developed to complement the multi-start approach. These are known as global optimization routines. They are also known as heuristic methods, because their performance cannot be guaranteed in general: there are no guarantees that theyll find the global minimum, nor are there solid estimates of how many iterations will be required for them to carry out a satisfactory search of the parameter space. We will consider two commonly used heuristic methods: simulated annealing and genetic algorithms. 6.4.2.1 Simulated Annealing Simulated annealing is motivated by the process of annealing, which is a heat treatment by which the physical properties of metals can be altered. Simulated annealing is an iterative algorithm; the algorithm starts at an initial guess, and then steps through the search space. In contrast to gradient descent, the steps taken in simulated annealing are partially random. Consequently, the path followed from a particular initial condition wont be repeated if the algorithm is run again from the same point. (Algorithms that incorporate randomness are often referred to as Monte Carlo methods, in reference to the European gambling centre.) At each step, the algorithm begins by identifying a candidate next position. (This point could be selected by a variant of gradient descent, or some other method). The value of the objective at this candidate point is then compared with the objective value at the current point. If the objective is lower at the candidate point (i.e. this step takes the algorithm downhill), then the step is taken and a new iteration begins. If the value of the objective is larger at the candidate point (i.e. the step makes things worse), the step can still be taken, but only with a small probability. Both the size of the steps and the probability of accepting wrong (uphill) steps are tied to a cooling schedule: a decreasing temperature profile. At high temperatures, large steps are considered and wrong steps are taken frequently; as the temperature drops, only smaller steps are considered, and fewer wrong steps are allowed. By analogy, imagine a ping-pong ball resting on a curved landscape. One strategy to move the ball to the lowest valley bottom is to shake the table. To following a strategy inspired by simulated annealing, we could begin by applying violent shakes (high temperature) which would result in the ball bouncing across much of the landscape. By slowly reducing the severity of the shaking, we could ensure the ball would settle into a local minimum at a valley bottom. The hope is that if the cooling schedule is well-chosen, the ball would have sampled many valleys, and would end up at the bottom of the lowest valley. Simulated annealing can be combined with a multi-start strategy to further ensure broad sampling of the search space. 6.4.2.2 Example 8 To illustrate, well apply simulated annealing to the optimization task in Example 7 above, using the same initial guess points. Well use the GenSA library to implement the algorithm (described in detail here). Calls to GenSA require that we specify (i) the objective function; (ii) an initial guess; and (iii) upper and lower bounds for the search values for each parameter. (Optional input parameters allow the user to modify internal features of the algorithm such as the cooling schedule and stepping protocol.) To begin, we apply the algorithm at the first initial guess: library(GenSA) out0 &lt;- GenSA(par = x0, lower = c(-2, -2), upper = c(2, 2), fn = Example_7_function) out0[c(&quot;value&quot;, &quot;par&quot;)] $value [1] 2.426731 $par [1] -0.3629442 1.1734789 The result of this function call is recorded in the variable out0 which indicates the minimal value of the objective achieved (2.426731) and the parameter values at which this minimum occurs \\((x,y)= (-0.3629442, 1.1734789)\\) (matching the solution founds above). Next, well call GenSA starting from each of the initial guesses that were provided to the gradient descent algorithm in Example 7. $value [1] 2.426731 $par [1] -0.3629442 1.1734789 $value [1] 2.426731 $par [1] -0.3629442 1.1734789 $value [1] 2.426731 $par [1] -0.3629442 1.1734789 $value [1] 2.426731 $par [1] -0.3629442 1.1734789 $value [1] 2.426731 $par [1] -0.3629442 1.1734789 We see that in each case, simulated annealing has avoided getting stuck in the local minima. It has achieved the same optimized value from every initial guess. The plot below provides some insight into how the simulated annealing runs proceed. Iterations (steps) are shown along the horizontal axis. The vertical axis shows values of the objective function. The blue points represents the function value at the current position, while the red shows the minimum achieved so far. The minimum is found rather quickly, but the algorithm continues to explore the search space. 6.4.2.3 Example 9 To give the simulated annealing algorithm a more challenging task, consider the following function, which has many local minima. (Near the origin, the graph resembles an egg carton). \\[f(x,y) = (y+47)\\sin\\left (\\sqrt{|y+(x/2)+47|)}-x\\sin(\\sqrt{|x-(y+47)|} + \\frac{1}{1000}\\left(x^2+y^2\\right)\\right)\\] f.egg &lt;- function(x) { -(x[, 2] + 47) * sin(sqrt(abs(x[, 2] + (x[, 1]/2) + 47))) - x[, 1] * sin(sqrt(abs(x[, 1] - (x[, 2] + 47)))) + 0.001 * x[, 1]^2 + 0.001 * x[, 2]^2 } Figure 6.16: Surface plot with many minima, resembling an egg carton We will use simulated annealing to search for the minimum value. Well start the algorithm from two different initial guesses. Both runs result in the same solution: # define a set of initial guess values # initial guesses x1 &lt;- c(100, 100) x2 &lt;- c(0, 100) # specify bounds lower &lt;- rep(-200, 2) upper &lt;- rep(200, 2) egg.out1 &lt;- GenSA(par = x1, lower = lower, upper = upper, fn = f.egg) egg.out2 &lt;- GenSA(par = x2, lower = lower, upper = upper, fn = f.egg) egg.out1[c(&quot;value&quot;, &quot;par&quot;)] $value [1] -266.8175 $par [1] -161.38281 95.54245 egg.out2[c(&quot;value&quot;, &quot;par&quot;)] $value [1] -266.8175 $par [1] -161.38281 95.54245 From the plots below we see that the algorithm occasionally gets stuck at local minima, but in both cases has visited the lowest point. Figure 6.17: Progress of simulated annealing algorithm Figure 6.18: Progress of simulated annealing algorithm We next consider a heuristic algorithm in which multiple paths through the search space are followed simultaneously. 6.4.2.4 Genetic Algorithms Genetic algorithms are inspired by Darwinian evolution. The algorithm begins with the specification of a population of initial guesses. At each iteration of the algorithm, this population evolves toward improved estimates of the global minimum. This evolution step involves three substeps: selection, mutation, and cross-over. In the selection step, the population is pruned by removing a fraction that are not sufficiently fit. (Here fitness corresponds to the value of the objective function being minimized.) Then, mutations are introduced into the remaining population by adding small random perturbations to their position in the search space. Finally, a new generation is produced by crossing members of the current population. This can be done in several ways; the simplest is to generate crosses as averages of the numerical values of the two parents. Through several generations, this process leads to a population with high fitness (minimal objective) after a thorough exploration of the search space. Genetic algorithms are a subset of the more general class of evolutionary algorithms all of which involve simultaneous exploration of the search space through multiple paths. 6.4.2.5 Example 10 To implement a genetic algorithm, well make use of the ga function, from Library(GA), described here. The call to ga requires that we specify the objective function, lower and upper bounds to define the search space, and the number of iterations to execute. The initial population is generated automatically. The ga routine maximizes the objective function, so we enter our objective with a negative sign to achieve minimization. We consdier again the function from Example 7. Example_7_fun = function(x, y) { return((1/2 * x^2 + 1/4 * y^2 + 3) + cos(2 * x + 1 - exp(y))) } library(GA) ga &lt;- ga(type = &quot;real-valued&quot;, fitness = function(x) -Example_7_fun(x[1], x[2]), lower = c(-2, -2), upper = c(2, 2), maxiter = 30) GA | iter = 1 | Mean = -4.041041 | Best = -2.873942 GA | iter = 2 | Mean = -3.552414 | Best = -2.480774 GA | iter = 3 | Mean = -3.385857 | Best = -2.480774 GA | iter = 4 | Mean = -3.316562 | Best = -2.475568 GA | iter = 5 | Mean = -3.233971 | Best = -2.453920 GA | iter = 6 | Mean = -2.942318 | Best = -2.429493 GA | iter = 7 | Mean = -3.057510 | Best = -2.429493 GA | iter = 8 | Mean = -2.929332 | Best = -2.427140 GA | iter = 9 | Mean = -2.862539 | Best = -2.427125 GA | iter = 10 | Mean = -2.611589 | Best = -2.427125 GA | iter = 11 | Mean = -2.665873 | Best = -2.426933 GA | iter = 12 | Mean = -2.649276 | Best = -2.426795 GA | iter = 13 | Mean = -2.574107 | Best = -2.426795 GA | iter = 14 | Mean = -2.667271 | Best = -2.426775 GA | iter = 15 | Mean = -2.516660 | Best = -2.426756 GA | iter = 16 | Mean = -2.705621 | Best = -2.426756 GA | iter = 17 | Mean = -2.598244 | Best = -2.426756 GA | iter = 18 | Mean = -2.580032 | Best = -2.426756 GA | iter = 19 | Mean = -2.584246 | Best = -2.426745 GA | iter = 20 | Mean = -2.654025 | Best = -2.426745 GA | iter = 21 | Mean = -2.551965 | Best = -2.426745 GA | iter = 22 | Mean = -2.612507 | Best = -2.426745 GA | iter = 23 | Mean = -2.675908 | Best = -2.426745 GA | iter = 24 | Mean = -2.628224 | Best = -2.426745 GA | iter = 25 | Mean = -2.664589 | Best = -2.426745 GA | iter = 26 | Mean = -2.631207 | Best = -2.426745 GA | iter = 27 | Mean = -2.598906 | Best = -2.426745 GA | iter = 28 | Mean = -2.553829 | Best = -2.426745 GA | iter = 29 | Mean = -2.591655 | Best = -2.426745 GA | iter = 30 | Mean = -2.482629 | Best = -2.426745 Plotting the results of the genetic algorithm search, we see improvement in the overall behaviour from generation to generation. plot(ga) The summary displayed below shows that the solutions reached by the genetic algorithm agrees with the solution found by simulated annealing. summary(ga) -- Genetic Algorithm ------------------- GA settings: Type = real-valued Population size = 50 Number of generations = 30 Elitism = 2 Crossover probability = 0.8 Mutation probability = 0.1 Search domain = x1 x2 lower -2 -2 upper 2 2 GA results: Iterations = 30 Fitness function value = -2.426745 Solution = x1 x2 [1,] -0.3675713 1.169812 6.4.2.6 Exercise 2 Apply the genetic algorithm with ga to confirm the minimum of the egg-carton function in Example 9. 6.5 Calibration of Dynamic Models The principles of nonlinear regression carry over directly to calibration of more complex models. In many domains of biology, dynamic models are used to describe the time-varying behaviour of systems (from, e.g., biomolecular networks to physiology to ecology). Dynamic models take many forms; a commonly used formulation is based on ordinary differential equations (i.e. rate equations). These models are deterministic (i.e., they do not incorporate random effects) and assume that the dynamics occur in a spatially homogeneous (well-mixed) environment. Despite these limitations, these models can describe a wide variety of dynamic behaviours, and are useful for investigations across a range of biological domains. Differential equation models used in biology often take the form \\[\\begin{equation*} \\frac{d}{dt} {\\bf x}(t) = {\\bf f}({\\bf x}(t), {\\bf p}) \\end{equation*}\\] where components of the time-varying vector \\({\\bf x}(t)\\) are the states of the system (e.g. population sizes, molecular concentrations), the components of vector \\({\\bf p}\\) are the model parameters: numerical values that represent fixed features of the system and its environment (e.g. interaction strengths, temperature, nutrient availability), and the vector-valued function \\({\\bf f}\\) describes the rate of change of the state variables. As a concrete example, consider the Lotka-Volterra equations, a classical model describing interacting predator and prey populations: \\[\\begin{eqnarray*} \\frac{d}{dt} x_1(t) &amp;=&amp; \\alpha x_1(t) - \\beta x_1(t) x_2(t)\\\\ \\frac{d}{dt} x_2(t) &amp;=&amp; \\gamma x_1(t) x_2(t) - \\delta x_2(t)\\\\ \\end{eqnarray*}\\] Here \\(x_1\\) is the size of the prey population; \\(x_2\\) is the size of the predator population. The prey are presumed to have access to resources that support exponential growth in the absence of predation: growth at rate \\(\\alpha x_1(t)\\). Interactions between prey and predators, assumed to occur at rate \\(x_1(t) x_2(t)\\), lead to decrease of the prey population and increase of the predator population (characterized by parameters \\(\\beta\\) and \\(\\gamma\\) respectively). Finally, the prey suffer an exponential decline in population size in the absence of prey: decay at rate \\(\\delta x_2(t)\\). A simulation of the model, shown below, demonstrates the a boom-bust cycle of oscillations in both populations. Simulation of the model requires specification of numerical values for each of the four model parameters, \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), \\(\\delta\\), and the two initial population sizes \\(x_1(0)\\) and \\(x_2(0)\\). We generate the simulation by a call to the ode function from the deSolve library described here. library(deSolve) # define the dynamic model LotVmod &lt;- function(Time, State, Pars) { with(as.list(c(State, Pars)), { dx1 = alpha * x1 - beta * x1 * x2 dx2 = delta * x1 * x2 - gamma * x2 return(list(c(dx1, dx2))) }) } # specify the model parameters, the initial populations, # and the time course Pars &lt;- c(alpha = 30, beta = 5, gamma = 2, delta = 6) State &lt;- c(x1 = 0.008792889, x2 = 1.595545) Time &lt;- seq(0, 6, by = 0.01) # simulate the model out &lt;- as.data.frame(ode(func = LotVmod, y = State, parms = Pars, times = Time)) # plot the output plot(out$x2 ~ out$time, type = &quot;l&quot;, xlab = &quot;Time&quot;, ylab = &quot;Population Density&quot;, main = &quot;Predatory-Prey Model Simulation&quot;, col = &quot;red&quot;, ylim = c(0, 20)) points(out$x1 ~ out$time, type = &quot;l&quot;, add = TRUE) legend(3.8, 20, legend = c(&quot;Prey&quot;, &quot;Predator&quot;), col = c(&quot;red&quot;, &quot;black&quot;), pch = 20) Figure 6.19: Simulation of Lotka-Volterra system. The dataset shown below corresponds to observations of an oscillatory predator-prey system. To calibrate the Lotka-Volterra model to this data we seek values of the model parameters for which simulation of the model provides the best fit. As in the regression tasks described previously, the standard measure for quality of fit is the sum of squared errors. We thus proceed with a minimization task: for each choice of numerical values for the model parameters, we compare model simulation with the data and determine the sum of squared errors. We aim to minimize this measure of fit over the space of model parameters. Figure 6.20: Data for Lotka-Volterra system. 6.5.0.1 Example 11 To illustrate, well consider the case that two of the model parameter values have been established independently: \\(\\alpha= 30\\) and \\(\\delta=6\\). Well then calibrate the model by estimating values for parameters \\(\\beta\\) and \\(\\gamma\\). We begin by building a sum-of-squared errors function that takes values of \\(\\beta\\) and \\(\\gamma\\) as arguments. We will use the observed population values at time zero as initial condition for the simulations. # define SSE function library(deSolve) determine_sse &lt;- function(p) { # inputs are the two unknown model parameters collected # into a vector p=[beta, gamma] newPars &lt;- c(p[1], p[2]) # initial populations are the observed values at time # zero for x and y, respectively newState &lt;- c(x1 = 0.008792889, x2 = 1.595545) # define the time-grid (same as above) Time &lt;- seq(0, 6, by = 0.01) t_obs &lt;- c(0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5, 5.2, 5.4, 5.6, 5.8, 6) # dynamics as before newLotVmod &lt;- function(Time, State, newPars) { with(as.list(c(State, newPars)), { dx1 = x1 * (30 - p[1] * x2) dx2 = -x2 * (p[2] - 6 * x1) return(list(c(dx1, dx2))) }) } # run the simulation with the user-specified values for # beta and gamma new_out &lt;- as.data.frame(ode(func = newLotVmod, y = newState, parms = newPars, times = Time)) # generate vector of predictions to align with vector # of observations x1_pred &lt;- new_out$x1[seq(1, length(new_out$x1), 20)] x2_pred &lt;- new_out$x2[seq(1, length(new_out$x2), 20)] sse &lt;- sum((x1_obs_data - x1_pred)^2) + sum((x2_obs_data - x2_pred)^2) return(sse) } We now call an optimization routine to search the space of \\(\\beta\\) and \\(\\gamma\\) values to minimize this SSE function. We apply the simulated annealing function GenSA as described above, with initial guess of \\(\\beta=1\\), \\(\\gamma=1\\): library(GenSA) p1 = c(1, 1) lower &lt;- c(0.1, 0.1) upper &lt;- c(10, 10) out0 &lt;- GenSA(par = p1, lower = lower, upper = upper, fn = determine_sse, control = list(maxit = 4)) out0[c(&quot;value&quot;, &quot;par&quot;)] $value [1] 45.34987 $par [1] 4.915015 2.018205 The search algorithm has identified values of \\(\\beta=4.915\\) and \\(\\gamma=2.018\\) as minimizer, with the minimal sum of squares error value of 45.58991. The best fit model simulation is shown along with the data below: Figure 6.21: Data for Lotka-Volterra system and best-fit simulation. A comprehensive discussion of calibration and uncertainty analysis of dynamic biological models can be found in (Ashyraliyev et al. 2009). 6.5.0.2 Exercise 3 Following the process in Example 11, calibrate all four model parameters \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) to the following dataset, with initial states \\(x_1(0)=0.01\\) and \\(x_2(0)=1.0\\). Use either simulated annealing or a genetic algorithm. You can generate a simulation with the predictions at the time-points corresponding to the observations by setting Time &lt;- seq(0, 8, by = .2) in the simulation script above. If you have trouble finding a suitable initial guess, try \\(\\alpha=10\\), \\(\\beta=1\\), \\(\\gamma=1\\), \\(\\delta=1\\). Figure 6.22: Data for Lotka-Volterra system for exercise 3 t_obs_ex &lt;- c(0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5, 5.2, 5.4, 5.6, 5.8, 6, 6.2, 6.4, 6.6, 6.8, 7, 7.2, 7.4, 7.6, 7.8, 8) x1_obs_ex &lt;- c(0.001, 0.043, 0.18, 0.86, 3.6, 3.24, 0.13, 0.0075, 0.00095, 0.0079, 0.00012, 0.00012, 0.00015, 0.00026, 0.00057, 0.0016, 0.0052, 0.021, 0.084, 0.39, 1.8, 5.3, 0.72, 0.028, 0.0024, 0.01, 0.00019, 0.00013, 0.00013, 0.00019, 0.00037, 0.00095, 0.0062, 0.01, 0.04, 0.18, 0.84, 3.6, 3.3, 0.13, 0.0091) x2_obs_ex &lt;- c(0.67, 1.2, 5.1, 0.78, 0.13, 5.7, 8.2, 7.5, 5.6, 4.4, 3, 5.3, 2.2, 1.9, 1.4, 1.4, 1.2, 1.1, 2.3, 1.6, 0.25, 3.8, 8.4, 8.3, 6.4, 5, 4.5, 1.9, 2.8, 2.9, 1.6, 2, 1.7, 4.2, 1.8, 2.7, 1.3, 2, 6.8, 8.7, 7.4) 6.6 Uncertainty Analysis and Bayesian Calibration Regression is usually followed by uncertainty analysis, which provides some measure of confidence in those parameter value estimates. For instance, in the case of linear regression, 95% confidence intervals on the parameter estimates and corresponding confidence intervals on the model predictions are supported by extensive statistical theory, and can be readily generated in R, as follows. Consider again the linear regression fit from Example 4 above. library(ggplot2) data(&quot;iris&quot;) iris.lm &lt;- lm(Sepal.Length ~ Petal.Width, data = iris) new.dat &lt;- seq(min(iris$Petal.Width), max(iris$Petal.Width), by = 0.05) pred_interval &lt;- predict(iris.lm, newdata = data.frame(Petal.Width = new.dat), interval = &quot;prediction&quot;, level = 0.95) In example 4, we found best fit model parameters of \\(b=4.7776\\) and \\(m=0.8886\\). The plot below shows 95% confidence intervals on the model predictions. plot(iris$Petal.Width, iris$Sepal.Length, xlab = &quot;Sepal Length&quot;, ylab = &quot;Petal Width&quot;, main = &quot;Regression&quot;, pch = 16) abline(iris.lm, col = &quot;firebrick&quot;, lwd = 2) lines(new.dat, pred_interval[, 2], col = &quot;steelblue&quot;, lty = 2, lwd = 2) lines(new.dat, pred_interval[, 3], col = &quot;steelblue&quot;, lty = 2, lwd = 2) legend(4, 105, legend = c(&quot;Best-fit Line&quot;, &quot;Prediction interval&quot;), col = c(&quot;firebrick&quot;, &quot;steelblue&quot;), lty = 1:2, cex = 0.8) Figure 6.23: Prediction confidence intervals of linear regression fit. The confint function determines 95% confidence intervals on the estimates of the parameters (the intercept \\(b\\) and the slope \\(m\\)). 2.5 % 97.5 % (Intercept) 4.6335014 4.9217574 Petal.Width 0.7870598 0.9901007 For nonlinear regression (including calibration of dynamic models), the theory is less helpful, but estimates of uncertainty intervals can be achieved. Recall the nonlinear regression fit from Example 5 above, where the best-fit values were founds as \\(K_M =0.4187090\\) and \\(V_{\\mbox{max}}=0.5331688\\). # substrate concentrations S &lt;- c(4.32, 2.16, 1.08, 0.54, 0.27, 0.135, 3.6, 1.8, 0.9, 0.45, 0.225, 0.1125, 2.88, 1.44, 0.72, 0.36, 0.18, 0.9, 0) # reaction velocities V &lt;- c(0.48, 0.42, 0.36, 0.26, 0.17, 0.11, 0.44, 0.47, 0.39, 0.29, 0.18, 0.12, 0.5, 0.45, 0.37, 0.28, 0.19, 0.31, 0) MM_data = cbind(S, V) MMmodel.nls &lt;- nls(V ~ Vmax * S/(Km + S), start = list(Km = 0.3, Vmax = 0.5)) #perform the nonlinear regression analysis params &lt;- summary(MMmodel.nls)$coeff[, 1] #extract the parameter estimates plot(MM_data) curve((params[2] * x)/(params[1] + x), from = 0, to = 4.5, add = TRUE, col = &quot;firebrick&quot;) Figure 6.24: Nonlinear regression fit from example 5 Applying the confit function generates confidence intervals for the parameter estimates confint(MMmodel.nls) Waiting for profiling to be done... 2.5% 97.5% Km 0.3143329 0.5528199 Vmax 0.4899962 0.5816347 Bayesian methods address the regression task by combining calibration and uncertainty in a single process. The basic idea behind Bayesian analysis (founded on Bayes Theorem, which may be familiar from elementary probability), is to start with some knowledge about the parameter estimates (analogous to the initial guess supplied to nonlinear regression) and then use the available data to refine that knowledge. In the Bayesian context, instead of the initial guess and refined estimate being single numerical values, they are distributions. In Bayesian terminology, we being with a prior distribution, which may be based on previously established expert knowledge. A commonly used prior is a normal distribution centered at a good initial estimate. In other cases the prior may be a uniform distribution over a wide range of possible values, indicating minimal previously established knowledge about the parameter values. Application of a Bayesian calibration scheme uses the available data to generate an improved distribution of the parameter values, which is called the posterior distribution. A successful Bayesian calibration could take a `wild guess uniform prior and return a tightly-centered posterior. Uncertainty can then be gleaned directly from the posterior distribution. 6.6.0.1 Example 12 Here well consider a straightforward implementation of a Bayesian approach: Approximate Bayesian Computation (ABC). This approach is based on a simple idea: the rejection method, in which we sample repeatedly from the prior distribution and reject all samples that do not satisfy a pre-specified tolerance for quality of fit. (This is reminiscent of the selection step in genetic algorithms: culling unfit members of a population.) The samples that are not rejected form the posterior distribution To illustrate, well revisit the Michaelis-Menten data from Example 5 above. We select uniform priors for both parameters: \\(K_M\\) and \\(V_{\\mbox{max}}\\) both uniform on \\([0,1]\\). # recall the dataset S &lt;- c(4.32, 2.16, 1.08, 0.54, 0.27, 0.135, 3.6, 1.8, 0.9, 0.45, 0.225, 0.1125, 2.88, 1.44, 0.72, 0.36, 0.18, 0.9, 0) #substrate concentrations V &lt;- c(0.48, 0.42, 0.36, 0.26, 0.17, 0.11, 0.44, 0.47, 0.39, 0.29, 0.18, 0.12, 0.5, 0.45, 0.37, 0.28, 0.19, 0.31, 0) #reaction velocities # define functions to draw values for Km and Vmax from the # prior distributions draw_Km &lt;- function() { return(runif(1, min = 0, max = 1)) } draw_Vmax &lt;- function() { return(runif(1, min = 0, max = 1)) } # Michaelis-Menten function simulate_data &lt;- function(S, Km, Vmax) { return(Vmax * S/(Km + S)) } # function to determine SSE between data and predictions compare_with_squared_distance &lt;- function(true, simulated) { distance = sqrt(sum(mapply(function(x, y) (x - y)^2, true, simulated))) return(distance) } # sampling algorithm: returns the sampled Km and Vmax # values and whether this pair is accepted sample_by_rejection &lt;- function(true_data, n_iterations, acceptance_threshold, accept_or_reject_function) { number_of_data_points = length(true_data) #record length of data vector accepted &lt;- vector(length = n_iterations) #declare vector to store acceptance information sampled_Km &lt;- vector(length = n_iterations, mode = &quot;numeric&quot;) #declare vector to store samples of Km sampled_Vmax &lt;- vector(length = n_iterations, mode = &quot;numeric&quot;) #declare vector to store samples of Vmax for (i in 1:n_iterations) { # loop over number of iterations Km &lt;- draw_Km() # sample a Km value from the prior for Km Vmax &lt;- draw_Vmax() # sample a Vmax value from the prior for Vmax simulated_data &lt;- simulate_data(S, Km, Vmax) #generate model predictions distance &lt;- compare_with_squared_distance(true_data, simulated_data) #determine SSE if (distance &lt; acceptance_threshold) { accepted[i] &lt;- 1 #accept if SSE is below threshold } else { accepted[i] &lt;- 0 #otherwise reject } sampled_Km[i] = Km #store Km value sampled_Vmax[i] = Vmax #store Vmax value } return(data.frame(cbind(accepted = accepted, sampled_Kms = sampled_Km, sampled_Vmaxs = sampled_Vmax))) } In the call below, we sample from the prior distributions of \\(K_M\\) and \\(V_{\\mbox{max}}\\) 200000 times. We set the acceptance threshold as 0.15. That is, parameter pairs that give rise to SSE values below 0.15 are accepted; others are rejected. The histograms below show the uniform priors along with the posteriors. The algorithm has successfully tightened the distributions about the best-fit parameter estimates established above (\\(K_M =0.4187090\\) and \\(V_{\\mbox{max}}=0.5331688\\)). [1] 2225 We see that only 2225 of 200 000 samples were accepted, yielding an acceptance rate of about 1%. The acceptance rate can be tuned by choice of the rejection threshold. A low acceptance rate gives rise to a computationally expensive algorithm, while a high acceptance rate can lead to poor estimation. Figure 6.25: Posterior histograms for Michaelis-Menten fit. Figure 6.26: Posterior histograms for Michaelis-Menten fit. Typically, approximate Bayesian computation is implemented as a sequential method, in which a sequence of rejection steps is applied, with the distribution being refined at each step (generating, in essence, a sequence of posterior distributions, the last of which is considered to be the best description of the desired parameter estimates). The EasyABC package can be used to implement sequential ABC. (More details on this package can be found in (Beaumont, 2019) and here.) 6.6.0.2 Exercise 4 Consider the task of finding the minimum of the function \\(f(x,y)=x^4 + y^2\\), plotted below. Clearly, the minimum is zero, and numerical optimization routines will have no trouble estimating that solution. However, because the curvature at the minimum is much different along the \\(x\\)- and \\(y\\)-directions, most numerical algorithms will do a better job estimating one parameter compared to the other. This can be well-illustrated by applying the rejection method to this problem and noting the relative spread in the posterior distributions. Implement the rejection algorithm provided above with uniform prior distributions on [-1, 1] for both the \\(x\\) and \\(y\\) values. Choose a rejection threshold that provides an acceptance rate of about 1%. Which estimate is provided with more confidence (i.e. which posterior is more tightly distributed? How can you relate that back to the curvature of the function at the minimum?. Figure 6.27: Parabloid of \\(f(x,y)=x^4+y^2\\) 6.6.0.3 Exercise 5 Repeat example 11 by applying the rejection algorithm to estimate the values of \\(\\beta\\) and \\(\\gamma\\) by fitting the Lotka-Volterra model to the data provided below. Use initial conditions \\(x = 0.008792889\\), \\(y = 1.595545\\) as in the example. Use uniform prior distributions of [2,30] for \\(\\beta\\) and [1,4] for \\(\\gamma\\), and choose a rejection threshold that provides an acceptance rate of about 1%. Do you find that one parameter is more confidently estimated (tighter distribution) than the other? Use the data shown below. To avoid errors in the simulation, you can add the options method = \"radau\", atol = 1e-6, rtol = 1e-6 to the call to ode. t_obs_ex5 &lt;- c(0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5, 5.2, 5.4, 5.6, 5.8, 6, 6.2, 6.4, 6.6, 6.8, 7, 7.2, 7.4, 7.6, 7.8, 8) x_obs_ex5 &lt;- c(0.0091, 0.043, 0.2, 0.82, 3.4, 2.9, 0.12, 0.0077, 0.00093, 0.00026, 0.00014, 0.00013, 0.00015, 0.00025, 0.00054, 0.0015, 0.0051, 0.02, 0.087, 0.36, 1.7, 5.6, 0.66, 0.029, 0.0024, 0.00048, 0.00019, 0.00012, 0.00013, 0.00019, 0.00035, 0.001, 0.003, 0.01, 0.041, 0.19, 0.88, 3.8, 3.2, 0.13, 0.0078) y_obs_ex5 &lt;- c(0.82, 1.1, 0.98, 0.65, 1.1, 5.1, 7.1, 7.9, 4.3, 3.5, 3.8, 3.9, 2.5, 2.1, 1.6, 1.4, 1.2, 1, 0.92, 0.57, 0.73, 3.6, 6.7, 9.1, 5.5, 4.7, 4.5, 3, 2.9, 2.7, 1.7, 2, 1.7, 1.3, 0.98, 0.98, 0.84, 1.6, 6.5, 8.2, 7.7) 6.6.1 Feedback We value your input! Please take the time to let us know how we might improve these materials. Survey 6.7 References Ashyraliyev, M., FomekongNanfack, Y., Kaandorp, J. A., &amp; Blom, J. G. (2009). Systems biology: parameter estimation for biochemical models. The FEBS journal, 276(4), 886-902. Beaumont, Mark A. Approximate bayesian computation. Annual review of statistics and its application 6 (2019): 379-403. Cho, Yong-Soon, and Hyeong-Seok Lim. Comparison of various estimation methods for the parameters of Michaelis-Menten equation based on in vitro elimination kinetic simulation data. Translational and clinical pharmacology 26.1 (2018): 39-47. Fairway, Julien, (2002) Practical Regression and Anova using R. https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf Ritz, C and Streibig, J. C., Nonlinear Regression with R, Springer, 2008. 6.8 Answer Key 6.8.1 Exercise 1 Consider the following dataset, which corresponds to measurements of drug concentration in the blood over time. try fitting the data to an exponential \\(C(t) = a e^{-r t}\\). Youll find that the best-fit model is not satisfactory. Next, try fitting a biexponential \\(C(t) = a_1 e^{-r_1 t} + a_2 e^{-r_2 t}\\). Youll find a more suitable agreement. For fitting, you can either use the nls command or the gradient descent function above (along with a sum of squared errors function). For nls, you may find the algorithm is very sensitive to your choice of initial guess (and will fail if the initial guess is not fairly accurate). For gradient descent, youll need to use a small stepsize, e.g. \\(10^{-5}\\), and a large number of iterations. t &lt;- c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100) C &lt;- c(5.58, 4.39, 3.26, 3, 2.65, 2.39, 2.08, 2.09, 1.7, 1.73, 1.42) BE_data = cbind(t, C) plot(BE_data) Figure 6.28: Exercise 1 showing decreasing drug concentration 6.8.2 Solution To use nls, we need to select an initial guess. With initial guess \\(a=6\\), \\(r=-1\\) we find a r 5.0657795 -0.0143575 Figure 6.29: Exercise 1 data decreasing drug concentration with expoential fit. The best fit (\\(a=5.07\\), \\(r=-0.014\\)) is not satisfactory. For the biexponential, we need initial guesses for \\(a_1\\) and \\(a_2\\), \\(r_1\\) and \\(r_2\\). We first took \\(a_1\\) and \\(a_2\\) equal to half of the fitted \\(a\\) value above, and \\(r_1=r_2=-0.014\\), again, from the fit above. This starting point fails to return a value. We then decreased \\(r_2\\) to \\(-0.05\\) to find: a_1 a_2 r_1 r_2 3.534782830 2.076126883 -0.008545831 -0.073312476 Figure 6.30: Exercise 1 data decreasing drug concentration with biexponetial fit. These values: \\(a_1=3.53\\), \\(a_2=2.08\\), \\(r_1=-0.0085\\), \\(r_2=-0.073\\), which provide a good fit. Alternatively, we try applying gradient descent. First we declare the gradient descent function as in Module 5: library(numDeriv) # contains functions for calculating gradient # define function that implement gradient descent. Inputs # are the objective function f, the initial parameter # estimate x0, the size of each step, the maximum number of # iterations to be applied, and a threshold gradient below # which the landscape is considered flat (and so iterations # are terminated) grad.descent = function(f, x0, step.size = 0.05, max.iter = 200, stopping.deriv = 0.01, ...) { n = length(x0) # record the number of parameters to be estimated (i.e. the dimension of the parameter space) xmat = matrix(0, nrow = n, ncol = max.iter) # initialize a matrix to record the sequence of estimates xmat[, 1] = x0 # the first row of matrix xmat is the initial &#39;guess&#39; for (k in 2:max.iter) { # loop over the allowed number of steps Calculate # the gradient (a vector indicating steepness and # direct of greatest ascent) grad.current = grad(f, xmat[, k - 1], ...) # Check termination condition: has a flat valley # bottom been reached? if (all(abs(grad.current) &lt; stopping.deriv)) { k = k - 1 break } # Step in the opposite direction of the gradient xmat[, k] = xmat[, k - 1] - step.size * grad.current } xmat = xmat[, 1:k] # Trim any unused columns from xmat return(list(x = xmat[, k], xmat = xmat, k = k)) } Next, we define a sum-of-squared-errors (SSE) function for the exponential, as follows: # define SSE determine_sse_exp &lt;- function(x) { pred &lt;- (x[1] * exp(x[2] * t)) sse &lt;- sum((C - pred)^2) return(sse) } We now call the gradient descent function. The function as written is very sensitive to the choice of step-size, so a small step-size is required to achieve convergence. This demands that a large number of steps are allowed, even from an initial guess that is close to the best-fit. x0 = c(5.5, -0.01) gd = grad.descent(determine_sse_exp, x0, step.size = 2e-05, max.iter = 4e+05, stopping.deriv = 0.01) # x0_out &lt;- c(gd$x, determine_sse_exp(gd$x)) x0_out &lt;- gd$x x0_out [1] 5.82416193 -0.06119189 plot(BE_data) curve(x0_out[1] * exp(x0_out[2] * x), from = 0, to = 100, add = TRUE, col = &quot;firebrick&quot;) Figure 6.31: Exercise 1 data decreasing drug concentration with exponetial fit gradient descent The best-fit is of \\(a=5.50947400\\), \\(r= -0.04928845\\) is unsatisfactory We next define a sum-of-squared-errors (SSE) function for the biexponential: # define SSE determine_sse &lt;- function(x) { pred &lt;- (x[1] * exp(x[2] * t) + x[3] * exp(x[4] * t)) sse &lt;- sum((C - pred)^2) return(sse) } and call the gradient descent function with the same initial guesses as above: x0 = c(3, -0.03, 3, -0.06) gd = grad.descent(determine_sse, x0, step.size = 1e-05, max.iter = 1e+05, stopping.deriv = 0.01) x0_out &lt;- gd$x x0_out [1] 2.690791165 -0.004104963 2.802982189 -0.041738755 plot(BE_data) curve(x0_out[1] * exp(x0_out[2] * x) + x0_out[3] * exp(x0_out[4] * x), from = 0, to = 100, add = TRUE, col = &quot;firebrick&quot;) Figure 6.32: Exercise 1 data decreasing drug concentration with biexponetial fit gradient descent This fit: \\(a_1=2.690791165\\), \\(r_1=-0.004104963\\), \\(a_2=2.802982189\\), \\(r_2=-0.041738755\\) is decent. Final note: the data were generated from the biexponential function with \\(a_1=2\\), \\(a_2=4\\), \\(r_1=-0.1\\), and \\(r_2=-0.001\\), altered by adding some small noise terms. 6.8.3 Exercise 2 Apply the genetic algorithm with ga to confirm the minimum of the egg-carton function in Example 9. 6.8.4 Solution First we declare the egg-carton function: f.egg &lt;- function(x, y) { -(y + 47) * sin(sqrt(abs(y + (x/2) + 47))) - x * sin(sqrt(abs(x - (y + 47)))) + 0.001 * x^2 + 0.001 * y^2 } Next we call the genetic algorithm, with wide search bounds (-200 to 200) for each parameter: library(GA) ga1 &lt;- ga(type = &quot;real-valued&quot;, fitness = function(x) -f.egg(x[1], x[2]), lower = c(-200, -200), upper = c(200, 200), maxiter = 30) GA | iter = 1 | Mean = 5.589807 | Best = 243.453722 GA | iter = 2 | Mean = 37.65332 | Best = 243.45372 GA | iter = 3 | Mean = 34.55112 | Best = 243.52693 GA | iter = 4 | Mean = 44.66904 | Best = 265.27007 GA | iter = 5 | Mean = 57.8448 | Best = 266.4818 GA | iter = 6 | Mean = 26.21427 | Best = 266.48180 GA | iter = 7 | Mean = 30.6800 | Best = 266.4818 GA | iter = 8 | Mean = 27.64206 | Best = 266.48180 GA | iter = 9 | Mean = 54.96501 | Best = 266.48180 GA | iter = 10 | Mean = 91.44821 | Best = 266.48180 GA | iter = 11 | Mean = 192.4639 | Best = 266.4818 GA | iter = 12 | Mean = 189.6778 | Best = 266.5143 GA | iter = 13 | Mean = 210.2262 | Best = 266.6820 GA | iter = 14 | Mean = 223.351 | Best = 266.682 GA | iter = 15 | Mean = 176.5155 | Best = 266.6820 GA | iter = 16 | Mean = 163.2122 | Best = 266.6820 GA | iter = 17 | Mean = 157.0652 | Best = 266.6820 GA | iter = 18 | Mean = 182.6798 | Best = 266.6820 GA | iter = 19 | Mean = 198.1779 | Best = 266.6834 GA | iter = 20 | Mean = 215.6551 | Best = 266.6834 GA | iter = 21 | Mean = 206.1483 | Best = 266.6834 GA | iter = 22 | Mean = 225.1222 | Best = 266.6834 GA | iter = 23 | Mean = 201.7200 | Best = 266.6834 GA | iter = 24 | Mean = 203.1510 | Best = 266.6834 GA | iter = 25 | Mean = 220.9440 | Best = 266.6834 GA | iter = 26 | Mean = 214.7430 | Best = 266.6834 GA | iter = 27 | Mean = 224.0534 | Best = 266.6834 GA | iter = 28 | Mean = 178.4590 | Best = 266.6834 GA | iter = 29 | Mean = 199.3457 | Best = 266.6834 GA | iter = 30 | Mean = 200.0797 | Best = 266.6834 Next we plot the results of the genetic algorithm search as well as print summary of the results to confirm that the fitness function value obtained through this method is the same as the value obtained through simulated annealing. plot(ga1) Figure 6.33: Exercise 2 genetic algorithm progress summary(ga1) -- Genetic Algorithm ------------------- GA settings: Type = real-valued Population size = 50 Number of generations = 30 Elitism = 2 Crossover probability = 0.8 Mutation probability = 0.1 Search domain = x1 x2 lower -200 -200 upper 200 200 GA results: Iterations = 30 Fitness function value = 266.6834 Solution = x1 x2 [1,] -160.8756 94.89672 The result (-160.8756, 94.89672) is in close agreement with the result for Example 9. 6.8.5 Exercise 3 Extend Example 11 to calibrate all four model parameters \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) to the following dataset, with initial states \\(x=0.01\\) and \\(y=1.0\\). Use either simulated annealing or a genetic algorithm. You can generate a simulation with the predictions at the time-points corresponding to the observations by setting Time &lt;- seq(0, 8, by = .2) in the simulation script above. If you have trouble finding a suitable initial guess, try \\(\\alpha=10\\), \\(\\beta=1\\), \\(\\gamma=1\\), \\(\\delta=1\\). 6.8.6 Solution First, define the SSE function determine_sse_ex3 &lt;- function(p) { # input parameters are the kinetic parameters of the # model newPars &lt;- c(p[1], p[2], p[3], p[4]) # initial populations for x1 and x2, respectively newState &lt;- c(x1 = 0.01, x2 = 1) # time-grid as specified Time &lt;- seq(0, 8, by = 0.2) # model newLotVmod &lt;- function(Time, State, newPars) { with(as.list(c(State, newPars)), { dx1 = x1 * (p[1] - p[2] * x2) dx2 = -x2 * (p[3] - p[4] * x1) return(list(c(dx1, dx2))) }) } library(deSolve) # simulation new_out1 &lt;- as.data.frame(ode(func = newLotVmod, y = newState, parms = newPars, times = Time)) x1_obs_ex &lt;- c(0.001, 0.043, 0.18, 0.86, 3.6, 3.24, 0.13, 0.0075, 0.00095, 0.0079, 0.00012, 0.00012, 0.00015, 0.00026, 0.00057, 0.0016, 0.0052, 0.021, 0.084, 0.39, 1.8, 5.3, 0.72, 0.028, 0.0024, 0.01, 0.00019, 0.00013, 0.00013, 0.00019, 0.00037, 0.00095, 0.0062, 0.01, 0.04, 0.18, 0.84, 3.6, 3.3, 0.13, 0.0091) x2_obs_ex &lt;- c(0.67, 1.2, 5.1, 0.78, 0.13, 5.7, 8.2, 7.5, 5.6, 4.4, 3, 5.3, 2.2, 1.9, 1.4, 1.4, 1.2, 1.1, 2.3, 1.6, 0.25, 3.8, 8.4, 8.3, 6.4, 5, 4.5, 1.9, 2.8, 2.9, 1.6, 2, 1.7, 4.2, 1.8, 2.7, 1.3, 2, 6.8, 8.7, 7.4) # determine SSE sse &lt;- sum((x1_obs_ex - new_out1$x1)^2) + sum((x2_obs_ex - new_out1$x2)^2) return(sse) } Running simulated annealing, we try a range of bounds that ensure the simulation runs smoothly and arrive at a good fit: $value [1] 52.58729 $par [1] 9.688079 3.097912 1.035242 2.076327 Alternatively, running a genetic algorithm we can arrive at a similar result: library(GA) ga3 &lt;- ga(type = &quot;real-valued&quot;, fitness = function(x) -determine_sse_ex3(c(x[1], x[2], x[3], x[4])), lower = c(0.1, 0.1, 0.1, 0.1), upper = c(20, 10, 5, 5), maxiter = 25) GA | iter = 1 | Mean = -1.279049e+67 | Best = -4.693969e+02 GA | iter = 2 | Mean = -5296.9794 | Best = -469.3969 GA | iter = 3 | Mean = -1341.4363 | Best = -437.7628 GA | iter = 4 | Mean = -685.2232 | Best = -437.7628 GA | iter = 5 | Mean = -637.5083 | Best = -437.7628 GA | iter = 6 | Mean = -595.0994 | Best = -437.7628 GA | iter = 7 | Mean = -597.1370 | Best = -437.7628 GA | iter = 8 | Mean = -593.8506 | Best = -437.7628 GA | iter = 9 | Mean = -576.1049 | Best = -434.0252 GA | iter = 10 | Mean = -559.1329 | Best = -434.0252 GA | iter = 11 | Mean = -1529.7803 | Best = -387.7072 GA | iter = 12 | Mean = -597.7009 | Best = -387.7072 GA | iter = 13 | Mean = -521.3438 | Best = -387.7072 GA | iter = 14 | Mean = -585.0047 | Best = -378.8299 GA | iter = 15 | Mean = -534.7539 | Best = -378.8299 GA | iter = 16 | Mean = -511.1798 | Best = -354.9926 GA | iter = 17 | Mean = -493.9326 | Best = -334.8149 GA | iter = 18 | Mean = -804.1124 | Best = -277.2302 GA | iter = 19 | Mean = -476.4043 | Best = -277.2302 GA | iter = 20 | Mean = -493.5554 | Best = -277.2302 GA | iter = 21 | Mean = -626.6152 | Best = -277.2302 GA | iter = 22 | Mean = -503.3496 | Best = -277.2302 GA | iter = 23 | Mean = -478.7910 | Best = -277.2302 GA | iter = 24 | Mean = -452.3181 | Best = -277.2302 GA | iter = 25 | Mean = -440.9315 | Best = -130.1073 summary(ga3) -- Genetic Algorithm ------------------- GA settings: Type = real-valued Population size = 50 Number of generations = 25 Elitism = 2 Crossover probability = 0.8 Mutation probability = 0.1 Search domain = x1 x2 x3 x4 lower 0.1 0.1 0.1 0.1 upper 20.0 10.0 5.0 5.0 GA results: Iterations = 25 Fitness function value = -130.1073 Solution = x1 x2 x3 x4 [1,] 10.56185 2.892053 0.9918592 3.028824 6.8.7 Exercise 4 Consider the task of finding the minimum of the function \\(f(x,y)=x^4 + y^2\\), plotted below. Clearly, the minimum is zero, and numerical optimization routines will have no trouble estimating that solution. However, because the curvature at the minimum is much different along the \\(x\\)- and \\(y\\)-directions, most numerical algorithms will do a better job estimating one parameter compared to the other. This can be well-illustrated by applying the rejection method to this problem and noting the relative spread in the posterior distributions. Implement the rejection algorithm provided above with uniform prior distributions on [-1, 1] for both the \\(x\\) and \\(y\\) values. Choose a rejection threshold that provides an acceptance rate of about 1%. Which estimate is provided with more confidence (i.e. which posterior is more tightly distributed? How can you relate that back to the curvature of the function at the minimum?. 6.8.8 Solution We modify the rejection method code to apply to this function: # define functions to draw values from the prior # distributions draw_x &lt;- function() { return(runif(1, min = -1, max = 1)) } draw_y &lt;- function() { return(runif(1, min = -1, max = 1)) } # sampling algorithm: returns the sampled values and # whether this pair is accepted sample_by_rejection &lt;- function(n_iterations, acceptance_threshold) { accepted &lt;- vector(length = n_iterations) sampled_x &lt;- vector(length = n_iterations, mode = &quot;numeric&quot;) sampled_y &lt;- vector(length = n_iterations, mode = &quot;numeric&quot;) for (i in 1:n_iterations) { x &lt;- draw_x() y &lt;- draw_y() if (x^4 + y^2 &lt; acceptance_threshold) { accepted[i] &lt;- 1 } else { accepted[i] &lt;- 0 } # accepted_or_rejected[i] = # accept_or_reject_function(true_data, # simulated_data, acceptance_threshold) sampled_x[i] = x sampled_y[i] = y } return(data.frame(cbind(accepted = accepted, sampled_xs = sampled_x, sampled_ys = sampled_y))) } We then call the rejection algorithm and check the acceptance rate # set seed set.seed(132) # simulate 20000 times with a threshold of 0.15 sampled_parameter_values_squared_distances = sample_by_rejection(20000, 0.005) # report the number of accepted values among the 200000 # samples sum(sampled_parameter_values_squared_distances$accepted) [1] 324 Finally, we plot the posterior distributions. As expected, the posterior is much tighter in the x-direction, in which direction the objective surface is steep, as opposed to the \\(y\\)-direction, in which the objective is shallow. Figure 6.34: Exercise 4 posterior histograms Figure 6.35: Exercise 4 posterior histograms 6.8.9 Exercise 5 Repeat example 11 by applying the rejection algorithm to estimate the values of \\(\\beta\\) and \\(\\gamma\\) by fitting the Lotka-Volterra model to the data provided below. Use initial conditions \\(x = 0.008792889\\), \\(y = 1.595545\\) as in the example. Use uniform prior distributions of [2,30] for \\(\\beta\\) and [1,4] for \\(\\gamma\\), and choose a rejection threshold that provides an acceptance rate of about 1%. Do you find that one parameter is more confidently estimated (tighter distribution) than the other? Use the data shown below. To avoid errors in the simulation, you can add the options method = \"radau\", atol = 1e-6, rtol = 1e-6 to the call to ode. t_obs_ex5 &lt;- c(0, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2, 2.2, 2.4, 2.6, 2.8, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.4, 4.6, 4.8, 5, 5.2, 5.4, 5.6, 5.8, 6, 6.2, 6.4, 6.6, 6.8, 7, 7.2, 7.4, 7.6, 7.8, 8) x1_obs_ex5 &lt;- c(0.0091, 0.043, 0.2, 0.82, 3.4, 2.9, 0.12, 0.0077, 0.00093, 0.00026, 0.00014, 0.00013, 0.00015, 0.00025, 0.00054, 0.0015, 0.0051, 0.02, 0.087, 0.36, 1.7, 5.6, 0.66, 0.029, 0.0024, 0.00048, 0.00019, 0.00012, 0.00013, 0.00019, 0.00035, 0.001, 0.003, 0.01, 0.041, 0.19, 0.88, 3.8, 3.2, 0.13, 0.0078) x2_obs_ex5 &lt;- c(0.82, 1.1, 0.98, 0.65, 1.1, 5.1, 7.1, 7.9, 4.3, 3.5, 3.8, 3.9, 2.5, 2.1, 1.6, 1.4, 1.2, 1, 0.92, 0.57, 0.73, 3.6, 6.7, 9.1, 5.5, 4.7, 4.5, 3, 2.9, 2.7, 1.7, 2, 1.7, 1.3, 0.98, 0.98, 0.84, 1.6, 6.5, 8.2, 7.7) 6.8.10 Solution We begin by defining the SSE function: library(deSolve) determine_sse_ex5 &lt;- function(p) { # first four input parameters are the kinetic # parameters of the model newPars &lt;- c(p[1], p[2]) # last tw parameters are the initial populations for x # and y, respectively newState &lt;- c(x1 = 0.008792889, x2 = 1.595545) # time-grid is the same as before, no need to redefine Time_ex5 &lt;- seq(0, 8, by = 0.2) Time_ex5 # kinetics newLotVmod &lt;- function(Time, State, newPars) { with(as.list(c(State, newPars)), { dx1 = x1 * (30 - p[1] * x2) dx2 = -x2 * (p[2] - 6 * x1) return(list(c(dx1, dx2))) }) } new_out1 &lt;- as.data.frame(ode(func = newLotVmod, y = newState, parms = newPars, times = Time_ex5, method = &quot;radau&quot;, atol = 1e-06, rtol = 1e-06)) sse &lt;- sum((x1_obs_ex5 - new_out1$x1)^2) + sum((x2_obs_ex5 - new_out1$x2)^2) return(sse) } We then define the rejection method algorithm: # recall the dataset # define functions to draw values from the prior # distributions draw_beta &lt;- function() { return(runif(1, min = 2, max = 30)) } draw_gamma &lt;- function() { return(runif(1, min = 1, max = 4)) } # sampling algorithm: returns the sampled values and # whether this pair is accepted sample_by_rejection &lt;- function(n_iterations, acceptance_threshold) { accepted &lt;- vector(length = n_iterations) sampled_beta &lt;- vector(length = n_iterations, mode = &quot;numeric&quot;) sampled_gamma &lt;- vector(length = n_iterations, mode = &quot;numeric&quot;) for (i in 1:n_iterations) { beta &lt;- draw_beta() gamma &lt;- draw_gamma() if (determine_sse_ex5(c(beta, gamma)) &lt; acceptance_threshold) { accepted[i] &lt;- 1 } else { accepted[i] &lt;- 0 } # accepted_or_rejected[i] = # accept_or_reject_function(true_data, # simulated_data, acceptance_threshold) sampled_beta[i] = beta sampled_gamma[i] = gamma } return(data.frame(cbind(accepted = accepted, sampled_betas = sampled_beta, sampled_gammas = sampled_gamma))) } We then call the rejection method algorithm, and confirm the accepstance rate: [1] 75 Plotting the posterior distributions, we see mean values of about \\(\\beta \\approx 14\\), \\(\\gamma \\approx 1.3\\). The distribution for \\(\\beta\\) is tighter than that for \\(\\gamma\\), suggesting improved estimation of \\(\\beta\\). Figure 6.36: Posterior distributions for gamma and beta Figure 6.37: Posterior distributions for gamma and beta "]]
